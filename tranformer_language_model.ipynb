{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Language Model (based on Adrej Karpathy's nanoGPT tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the txt entire file into a single string\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of dataset in characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total length of dataset in characters: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character vocabulary:  ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# get vocabulary of characters\n",
    "vocab = sorted(set(list(text)))\n",
    "print(\"character vocabulary: \", vocab)\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text\n",
    "ctoi = {vocab[i]:i for i in range(vocab_size)}\n",
    "itoc = {i:vocab[i] for i in range(vocab_size)}\n",
    "encode = lambda s: [ctoi[c] for c in s]  # converts a string to integer token sequence\n",
    "decode = lambda s: [itoc[ix] for ix in s]  # converts an integer token sequence to string of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42, 2]\n",
      "['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!']\n"
     ]
    }
   ],
   "source": [
    "print(encode('Hello world!'))\n",
    "print(decode(encode('Hello world!')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "# tokenize the dataset into integer sequence, convert to torch tensor of type int64\n",
    "data = torch.tensor(encode(text), dtype=torch.long) \n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-validation splits (90-10)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split the data into chuncks of size block_size. For each chunk, we create (input,target) pairs for next character prediction, where the input is a context window containing all characters preceding the target character. Note that the context sizes range from 1 up to block size, i.e. there will be block_size number of (input,target) pairs per chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: tensor([18]) --> target: 47\n",
      "Context: tensor([18, 47]) --> target: 56\n",
      "Context: tensor([18, 47, 56]) --> target: 57\n",
      "Context: tensor([18, 47, 56, 57]) --> target: 58\n",
      "Context: tensor([18, 47, 56, 57, 58]) --> target: 1\n",
      "Context: tensor([18, 47, 56, 57, 58,  1]) --> target: 15\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15]) --> target: 47\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15, 47]) --> target: 58\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "\n",
    "# example showing the first chunk and all possible (input,target) pairs we can get from it\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"Context: {context} --> target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create a batch generator which creates a batch of randomly selected blocks/chunks from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input batch: \n",
      "torch.Size([4, 8])\n",
      "tensor([[17, 17, 26,  1, 17, 24, 21, 38],\n",
      "        [14, 17, 24, 24, 13, 10,  0, 28],\n",
      "        [63,  1, 47, 52,  1, 56, 43, 55],\n",
      "        [56, 59, 57, 58,  1, 59, 54, 53]], device='cuda:0')\n",
      "target batch: \n",
      "torch.Size([4, 8])\n",
      "tensor([[17, 26,  1, 17, 24, 21, 38, 13],\n",
      "        [17, 24, 24, 13, 10,  0, 28, 50],\n",
      "        [ 1, 47, 52,  1, 56, 43, 55, 59],\n",
      "        [59, 57, 58,  1, 59, 54, 53, 52]], device='cuda:0')\n",
      "\n",
      "A batch of 4 blocks:\n",
      "\n",
      "Block 0:\n",
      "Context: [17] --> target: 17\n",
      "Context: [17, 17] --> target: 26\n",
      "Context: [17, 17, 26] --> target: 1\n",
      "Context: [17, 17, 26, 1] --> target: 17\n",
      "Context: [17, 17, 26, 1, 17] --> target: 24\n",
      "Context: [17, 17, 26, 1, 17, 24] --> target: 21\n",
      "Context: [17, 17, 26, 1, 17, 24, 21] --> target: 38\n",
      "Context: [17, 17, 26, 1, 17, 24, 21, 38] --> target: 13\n",
      "\n",
      "\n",
      "Block 1:\n",
      "Context: [14] --> target: 17\n",
      "Context: [14, 17] --> target: 24\n",
      "Context: [14, 17, 24] --> target: 24\n",
      "Context: [14, 17, 24, 24] --> target: 13\n",
      "Context: [14, 17, 24, 24, 13] --> target: 10\n",
      "Context: [14, 17, 24, 24, 13, 10] --> target: 0\n",
      "Context: [14, 17, 24, 24, 13, 10, 0] --> target: 28\n",
      "Context: [14, 17, 24, 24, 13, 10, 0, 28] --> target: 50\n",
      "\n",
      "\n",
      "Block 2:\n",
      "Context: [63] --> target: 1\n",
      "Context: [63, 1] --> target: 47\n",
      "Context: [63, 1, 47] --> target: 52\n",
      "Context: [63, 1, 47, 52] --> target: 1\n",
      "Context: [63, 1, 47, 52, 1] --> target: 56\n",
      "Context: [63, 1, 47, 52, 1, 56] --> target: 43\n",
      "Context: [63, 1, 47, 52, 1, 56, 43] --> target: 55\n",
      "Context: [63, 1, 47, 52, 1, 56, 43, 55] --> target: 59\n",
      "\n",
      "\n",
      "Block 3:\n",
      "Context: [56] --> target: 59\n",
      "Context: [56, 59] --> target: 57\n",
      "Context: [56, 59, 57] --> target: 58\n",
      "Context: [56, 59, 57, 58] --> target: 1\n",
      "Context: [56, 59, 57, 58, 1] --> target: 59\n",
      "Context: [56, 59, 57, 58, 1, 59] --> target: 54\n",
      "Context: [56, 59, 57, 58, 1, 59, 54] --> target: 53\n",
      "Context: [56, 59, 57, 58, 1, 59, 54, 53] --> target: 52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1223)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "# data loader (generates a bvatch of randomly selected blocks)\n",
    "def get_batch(split='train'):\n",
    "    data = train_data if split=='train' else val_data\n",
    "\n",
    "    # sample positions from which to grab blocks\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])      \n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) # move tensors to gpu \n",
    "    return x,y \n",
    "\n",
    "xbatch, ybatch = get_batch('train')\n",
    "print(\"input batch: \")\n",
    "print(xbatch.shape)\n",
    "print(xbatch)\n",
    "print(\"target batch: \")\n",
    "print(ybatch.shape)     \n",
    "print(ybatch)     \n",
    "print(\"\")\n",
    "\n",
    "# context target pairs\n",
    "print(f\"A batch of {batch_size} blocks:\")\n",
    "for b in range(batch_size): # batch dimension\n",
    "    print(f\"\\nBlock {b}:\")\n",
    "    for t in range(block_size):  # time dimension\n",
    "        context = xbatch[b,:t+1]\n",
    "        target = ybatch[b,t]\n",
    "        print(f\"Context: {context.tolist()} --> target: {target}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's create a pytorch-ified Bi-gram language model (will serve as a baseline for comparing the transformer model later on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "batch_size = 64\n",
    "block_size = 8\n",
    "max_iters = 6000\n",
    "learning_rate = 1e-2\n",
    "eval_interval = 300\n",
    "eval_iters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        Define model parameters\n",
    "        '''\n",
    "        # lookup table for finding logits for the next token (i.e. log of counts for all possible next token given input token)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # shape: (C,C)\n",
    "\n",
    "\n",
    "    # forward pass takes in a batch of input token sequences of shape (B,T) and corresponding targets of shape (B,T)\n",
    "    def forward(self, idx, targets=None):\n",
    "        # get logits for every input token\n",
    "        logits = self.token_embedding_table(idx) # shape: (B,T,C)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B,T,C = logits.shape\n",
    "            # reshape the logits and targets such that batch of input sequences are flattened into a single big input sequence\n",
    "            # i.e. (B,T) --> (B*T)\n",
    "            logits = logits.view(B*T,C) # reshaped to (B*T,C)\n",
    "            targets = targets.view(B*T) # reshaped to (B*T)\n",
    "            # compute cross entropy loss (i.e. average negative log likelihood)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    # generates new sequences continuing from a given batch of context tokens\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # batch of contexts, idx has shape (B,T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get predictions\n",
    "            logits, _ = self(idx) # shape: (B,T,C)\n",
    "            # for each context sequence (in the batch), compute the probability of the next token using the logits of the last token in the context sequence\n",
    "            logits = logits[:,-1,:] # shape: (B,C)\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            # sample from the probability distribution to get next token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # shape: (B,1)\n",
    "            # append to the current context\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # shape: (B,T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.3812, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Generated sequence:\n",
      " \n",
      "zuPdOf$JQmmgsWCjS,yoc.obDjcewb:by\n",
      "ZzRAKGTx\n",
      "Xn3YnigR,\n",
      "T;t:'e$BszJiwljm?REKce'DuIN'-KY?fiwJpAS:bt-? M \n"
     ]
    }
   ],
   "source": [
    "# create a bigram language model and test it on the example batch\n",
    "model = BigramLanguageModel(vocab_size=vocab_size)\n",
    "# move model to device\n",
    "m = model.to(device)\n",
    "logits, loss = m(xbatch, ybatch)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# generate a single sequences using the model with start token 0\n",
    "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_seq = m.generate(idx, max_new_tokens=100)[0].tolist()\n",
    "# Decode integer tokens into characters\n",
    "generated_seq = decode(generated_seq)\n",
    "print(\"\\nGenerated sequence:\\n\",\"\".join(generated_seq))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated sequence looks like gibberish, because model is untrained. We now train the model using a graident based optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating training and validation losses averaged over lots of batches\n",
    "@torch.no_grad() # disable gradient tracking\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval() # swicth to inference mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split) \n",
    "            _, loss = m(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean() \n",
    "    model.train() # switch back to training mode\n",
    "    return out       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, training loss: 4.598198413848877, validation loss: 4.587023735046387\n",
      "epoch: 300, training loss: 2.735320806503296, validation loss: 2.7429261207580566\n",
      "epoch: 600, training loss: 2.524005174636841, validation loss: 2.54231858253479\n",
      "epoch: 900, training loss: 2.4882287979125977, validation loss: 2.499865770339966\n",
      "epoch: 1200, training loss: 2.4708216190338135, validation loss: 2.4979774951934814\n",
      "epoch: 1500, training loss: 2.4736366271972656, validation loss: 2.495673894882202\n",
      "epoch: 1800, training loss: 2.4659011363983154, validation loss: 2.486417770385742\n",
      "epoch: 2100, training loss: 2.4783377647399902, validation loss: 2.4822909832000732\n",
      "epoch: 2400, training loss: 2.4603683948516846, validation loss: 2.4931557178497314\n",
      "epoch: 2700, training loss: 2.4593422412872314, validation loss: 2.4919941425323486\n",
      "epoch: 3000, training loss: 2.4662582874298096, validation loss: 2.480480194091797\n",
      "epoch: 3300, training loss: 2.4537932872772217, validation loss: 2.4837450981140137\n",
      "epoch: 3600, training loss: 2.4631268978118896, validation loss: 2.490421772003174\n",
      "epoch: 3900, training loss: 2.4539620876312256, validation loss: 2.4880266189575195\n",
      "epoch: 4200, training loss: 2.4670491218566895, validation loss: 2.482585906982422\n",
      "epoch: 4500, training loss: 2.4545273780822754, validation loss: 2.491215229034424\n",
      "epoch: 4800, training loss: 2.461135149002075, validation loss: 2.4918999671936035\n",
      "epoch: 5100, training loss: 2.4698903560638428, validation loss: 2.479996919631958\n",
      "epoch: 5400, training loss: 2.454688787460327, validation loss: 2.488793134689331\n",
      "epoch: 5700, training loss: 2.4547009468078613, validation loss: 2.482509136199951\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_iters):\n",
    "    # sample a batch of trainin data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    _, loss = m(xb, yb)\n",
    "    # reset parameter gradients\n",
    "    optimizer.zero_grad(set_to_none=True) \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % eval_interval == 0:\n",
    "        losses = estimate_loss(m)\n",
    "        print(f\"epoch: {epoch}, training loss: {losses['train'].item()}, validation loss: {losses['val'].item()}\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try generating some text using the trained bigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sequence:\n",
      " \n",
      "\n",
      "They?\n",
      "Wha dre\n",
      "By'haty.\n",
      "MERThe may mem ad tees atomy acoucoowe,\n",
      "My?\n",
      "An went yollle\n",
      "Tunis ghest he\n",
      "\n",
      "NCory s wir theanoreimeca I Theist her peshainds bu? whamou larnd isthy t s, s dony atheeawioure s\n",
      "\n",
      "Mono at the, wicekeee'lowhe he it wipise bethan horomered.\n",
      "\n",
      "ARo, ovearannumime\n",
      "Boumet ho t ator tull \n"
     ]
    }
   ],
   "source": [
    "# generate a single sequences using the model with start token 0\n",
    "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_seq = m.generate(idx, max_new_tokens=300)[0].tolist()\n",
    "# Decode integer tokens into characters\n",
    "generated_seq = decode(generated_seq)\n",
    "print(\"\\nGenerated sequence:\\n\",\"\".join(generated_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better! It has similar syntactic structure as the training text and even has some correct words. The quality is still very bad because the context window is too small, only the previous character is used to predict the next character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-attention basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider a batch of 4 sequences, with 8 token embeddings in each sequence, with embedding dims=2\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for each sequence, we will create context windows randing from size 1 up to T. The context of size t is then computed as the average of the embeddings of all tokens up to position t. This simple averaging gives us a \"bag of words\" context which has no awareness of relative positions of the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # embedding vectors for all tokens in the context window of size t --> shape: (t,C)  \n",
    "        # compute bag of words context of size t for the bth sequence\n",
    "        xbow[b,t] = xprev.mean(dim=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more efficient way of computing these context vectors is using matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.]])\n",
      "\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 2.],\n",
      "        [2., 2.]])\n",
      "\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "tensor([[1.0000, 1.0000],\n",
      "        [0.5000, 1.0000],\n",
      "        [0.6667, 0.6667]])\n"
     ]
    }
   ],
   "source": [
    "# consider a single sequence of 3 tokens with embedding dims of 3 --> shape: (3,3)\n",
    "x = torch.tensor([[1,1], [0,1], [1,0]], dtype=torch.float32) # each row is an embedding vector\n",
    "print(x)\n",
    "print(\"\")\n",
    "# then to get the context of shape: (3,2), in which the the t-th row is the sum of the first t rows in x\n",
    "# we can simply multiply a lower triangular matrix in which all elements above the diagonal are zero and the rest are 1s\n",
    "W = torch.tril(torch.ones(3,3))\n",
    "print(W)   \n",
    "xbow = W @ x\n",
    "print(xbow)\n",
    "print(\"\")\n",
    "\n",
    "# however, we don't want the sum of embedding vectors, but the mean so instead do the following\n",
    "W = torch.tril(torch.ones(3,3))\n",
    "W = W / W.sum(dim=1, keepdims=True)\n",
    "print(W)\n",
    "xbow = W @ x\n",
    "print(xbow)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So each row in the weights matrix W gives us the weights for summing up the embedding vectors of all the token. For the ith row, all weights after the ith column are zero,\n",
    "which means the weighted sum for the ith context only includes tokens up to and including the ith position (in our example, we used uniform weights). So effectively, we're masking out all \"future\" tokens so that the context only depends on current and past tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then for a batch of sequences, we can do the following\n",
    "x = torch.randn(B,T,C)\n",
    "W = torch.tril(torch.ones(T,T))\n",
    "W = W / W.sum(dim=1, keepdims=True)\n",
    "xbow = W @ x # batch matrix multiplication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "# another way to generate the lower triangular matrix needed to compute the mean context vectors is as follows\n",
    "A = torch.tril(torch.ones(T,T))\n",
    "W = torch.zeros((T,T))\n",
    "W = W.masked_fill(A == 0, float('-inf')) # masked fill replaces every element in A which equals 0 with -infinity \n",
    "print(W)\n",
    "# then by taking the softmax, we get the desired matrix \n",
    "W = F.softmax(W, dim=-1)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In a self-attention head, these attention weights are not uniform, but are instead computed using the (key, query) vectors of each token in the sequence. Then the output of the attention-head is the weighted sum of value vectors of the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "un-normalized attention weights for first sequence in batch:\n",
      "\n",
      "tensor([[-3.4152e-01, -3.0470e-01,  8.8343e-02,  3.4636e-01,  8.2287e-04,\n",
      "          2.0130e-01, -4.5725e-01, -7.7955e-02],\n",
      "        [-2.9526e-01, -1.6414e-01,  3.1263e-02,  2.9052e-01,  1.4245e-02,\n",
      "          1.3007e-01, -2.6864e-01, -6.5693e-02],\n",
      "        [ 8.4051e-02,  2.9877e-02, -1.2441e-03, -8.1190e-02, -6.3516e-03,\n",
      "         -2.9566e-02,  5.4978e-02,  1.8412e-02],\n",
      "        [ 3.4551e-01,  2.9935e-01, -8.5324e-02, -3.4960e-01, -2.0478e-03,\n",
      "         -1.9971e-01,  4.5122e-01,  7.8712e-02],\n",
      "        [ 2.1104e-03,  1.5416e-02, -6.6950e-03, -3.3556e-03,  1.8396e-03,\n",
      "         -7.2365e-03,  2.0092e-02,  7.1362e-04],\n",
      "        [ 1.9712e-01,  1.3190e-01, -3.1014e-02, -1.9596e-01, -6.4676e-03,\n",
      "         -9.6720e-02,  2.0783e-01,  4.4241e-02],\n",
      "        [-4.4520e-01, -2.7053e-01,  5.7607e-02,  4.4013e-01,  1.8339e-02,\n",
      "          2.0632e-01, -4.3446e-01, -9.9450e-02],\n",
      "        [-7.7793e-02, -6.7705e-02,  1.9350e-02,  7.8742e-02,  4.1935e-04,\n",
      "          4.5100e-02, -1.0198e-01, -1.7728e-02]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "unnormalized masked attention weights:\n",
      "\n",
      "tensor([[-3.4152e-01,        -inf,        -inf,        -inf,        -inf,\n",
      "                -inf,        -inf,        -inf],\n",
      "        [-2.9526e-01, -1.6414e-01,        -inf,        -inf,        -inf,\n",
      "                -inf,        -inf,        -inf],\n",
      "        [ 8.4051e-02,  2.9877e-02, -1.2441e-03,        -inf,        -inf,\n",
      "                -inf,        -inf,        -inf],\n",
      "        [ 3.4551e-01,  2.9935e-01, -8.5324e-02, -3.4960e-01,        -inf,\n",
      "                -inf,        -inf,        -inf],\n",
      "        [ 2.1104e-03,  1.5416e-02, -6.6950e-03, -3.3556e-03,  1.8396e-03,\n",
      "                -inf,        -inf,        -inf],\n",
      "        [ 1.9712e-01,  1.3190e-01, -3.1014e-02, -1.9596e-01, -6.4676e-03,\n",
      "         -9.6720e-02,        -inf,        -inf],\n",
      "        [-4.4520e-01, -2.7053e-01,  5.7607e-02,  4.4013e-01,  1.8339e-02,\n",
      "          2.0632e-01, -4.3446e-01,        -inf],\n",
      "        [-7.7793e-02, -6.7705e-02,  1.9350e-02,  7.8742e-02,  4.1935e-04,\n",
      "          4.5100e-02, -1.0198e-01, -1.7728e-02]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "Normalized masked attention weights:\n",
      "\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4673, 0.5327, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3490, 0.3306, 0.3204, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3222, 0.3076, 0.2094, 0.1608, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2027, 0.1983, 0.1990, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2012, 0.1885, 0.1602, 0.1358, 0.1642, 0.1500, 0.0000, 0.0000],\n",
      "        [0.0927, 0.1104, 0.1533, 0.2247, 0.1474, 0.1779, 0.0937, 0.0000],\n",
      "        [0.1172, 0.1184, 0.1292, 0.1371, 0.1267, 0.1325, 0.1144, 0.1245]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "head_size = 16 # this is the dimensions of the key and query vectors\n",
    "\n",
    "# the key and query vector are ontained by a linear transform of the embeddings obtained by multiplying with (C, head_size) matrices of learnable weights\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# then given a batch of token sequence embeddings\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B,T,C) # (B,T,C)\n",
    "\n",
    "# we can compute the query, key and value vectors for each token as follows\n",
    "k = key(x) # (B,T,h) where h=16 is the head_size\n",
    "q = query(x) # (B,T,h)\n",
    "v = value(x) # (B,T,h)\n",
    "\n",
    "# then for a sequence of tokens, the (i,j)th attention weight is assigned to be the dot product of the query vector of ith token\n",
    "# with key vector of jth token, so for the entuire batch we have the following\n",
    "W = q @ k.transpose(-2,-1)  # we've transposed the key matrix: (B,T,h) --> (B,h,T), the shape of the matrix multiplication result is: (B,T,h) @ (B,h,T) = (B,T,T)\n",
    "\n",
    "# we also scale the unnormalized weights to have variance of roughly 1\n",
    "W = W * head_size**(-0.5)\n",
    "\n",
    "\n",
    "print(\"\\nun-normalized attention weights for first sequence in batch:\\n\")\n",
    "print(W[0]) \n",
    "\n",
    "# then we apply the \"temporal\" masking so that the attention weights of future tokens is zero and also normalize so that weights sum to one\n",
    "A = torch.tril(torch.ones(T,T))\n",
    "W = W.masked_fill(A == 0, float('-inf')) # masked fill replaces every element in A which equals 0 with -infinity \n",
    "print(\"\\nunnormalized masked attention weights:\\n\")\n",
    "print(W[0]) \n",
    "W = F.softmax(W, dim=-1)\n",
    "print(\"\\nNormalized masked attention weights:\\n\")\n",
    "print(W[0])\n",
    "\n",
    "# the output of the self-attention head is then the sums of the token embeddings weighted by the attention weights\n",
    "out = W @ v # (B,T,T) @ (B,T,h) = (B,T,h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now using the idea of self-attention, we will design a better language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we create a single self-attention head module\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_size = head_size\n",
    "\n",
    "        # define parameters\n",
    "        self.key = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "\n",
    "        # non-parameter tensor of lower triangular ones\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "   \n",
    "    # define forward pass, input shape: (B,T,C) where B=batch size, T=block_size, C=embedding_dim\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B,T,h) where h is the head_size\n",
    "        q = self.query(x) # (B,T,h)\n",
    "        v = self.value(x) # (B,T,h)\n",
    "        W = q @ k.transpose(-2,-1)  * self.head_size**(-0.5) # (B,T,T)\n",
    "        W = W.masked_fill(self.tril[:T, :T] == 0, float('-inf')) \n",
    "        W = F.softmax(W, dim=-1)\n",
    "        out = W @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "class ImprovedLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, embedding_dim, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_size = head_size\n",
    "\n",
    "        '''\n",
    "        Define model parameters\n",
    "        '''\n",
    "        # token embedding layer \n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim) # shape: (vocab_size,C)\n",
    "        # position embedding layer\n",
    "        self.pos_embedding = nn.Embedding(block_size, embedding_dim) # shape: (T,C)\n",
    "\n",
    "        # self-attention layer\n",
    "        self.sa_head = Head(block_size, embedding_dim, head_size) # shape: (T,C,h)\n",
    "        # output layer logits\n",
    "        self.lm_head = nn.Linear(head_size, vocab_size) # shape: (h,vocab_size)\n",
    "\n",
    "\n",
    "        # forward pass takes in a batch of input token sequences of shape (B,T) and corresponding targets of shape (B,T)\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T =idx.shape\n",
    "        # get token embeddings\n",
    "        token_embeds = self.token_embedding(idx) # (B,T,C)\n",
    "        # add positional encoding\n",
    "        pos_embeds = self.pos_embedding(torch.arange(T, device=device)) # (T,C) \n",
    "        x = token_embeds + pos_embeds # (B,T,C)\n",
    "        # apply self-attention\n",
    "        x = self.sa_head(x) # (B,T,h)\n",
    "        # compute output logits\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B,T,vocab_size = logits.shape\n",
    "            # reshape the logits and targets such that batch of input sequences are flattened into a single big input sequence\n",
    "            # i.e. (B,T) --> (B*T)\n",
    "            logits = logits.view(B*T,vocab_size) # reshaped to (B*T,vocab_size)\n",
    "            targets = targets.view(B*T) # reshaped to (B*T)\n",
    "            # compute cross entropy loss (i.e. average negative log likelihood)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "\n",
    "    # generates new sequences continuing from a given batch of context tokens\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # batch of contexts, idx has shape (B,T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # since we're using positional encoding, we need to crop idx if input sequence length exceeds block size (keep last block_size tokens)\n",
    "            idx_crop = idx[:,-block_size:] \n",
    "            # get predictions\n",
    "            logits, _ = self(idx_crop) # shape: (B,T,C)\n",
    "            # for each context sequence (in the batch), compute the probability of the next token using the logits of the last token in the context sequence\n",
    "            logits = logits[:,-1,:] # shape: (B,C)\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            # sample from the probability distribution to get next token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # shape: (B,1)\n",
    "            # append to the current context\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # shape: (B,T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train this improved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "block_size = 8\n",
    "embedding_dim = 32\n",
    "head_size = 32\n",
    "max_iters = 5000\n",
    "learning_rate = 1e-3\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "\n",
    "model = ImprovedLanguageModel(vocab_size=vocab_size, block_size=block_size, embedding_dim=embedding_dim, head_size=head_size)\n",
    "# move model to device\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, training loss: 4.14112663269043, validation loss: 4.141552925109863\n",
      "epoch: 500, training loss: 2.6918914318084717, validation loss: 2.709040880203247\n",
      "epoch: 1000, training loss: 2.5271646976470947, validation loss: 2.530012607574463\n",
      "epoch: 1500, training loss: 2.4721152782440186, validation loss: 2.4833765029907227\n",
      "epoch: 2000, training loss: 2.4424257278442383, validation loss: 2.458141326904297\n",
      "epoch: 2500, training loss: 2.426382303237915, validation loss: 2.4489858150482178\n",
      "epoch: 3000, training loss: 2.4147531986236572, validation loss: 2.4208338260650635\n",
      "epoch: 3500, training loss: 2.4083781242370605, validation loss: 2.4340643882751465\n",
      "epoch: 4000, training loss: 2.393080711364746, validation loss: 2.4226150512695312\n",
      "epoch: 4500, training loss: 2.387653350830078, validation loss: 2.413745164871216\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(max_iters):\n",
    "    # sample a batch of trainin data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    _, loss = m(xb, yb)\n",
    "    # reset parameter gradients\n",
    "    optimizer.zero_grad(set_to_none=True) \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % eval_interval == 0:\n",
    "        losses = estimate_loss(m)\n",
    "        print(f\"epoch: {epoch}, training loss: {losses['train'].item()}, validation loss: {losses['val'].item()}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sequence:\n",
      " \n",
      "Ma gi.\n",
      "SOF noy:\n",
      "LI I at by;\n",
      "Wun thir bathr I igrithangesarde thald sat therin\n",
      "Sho oms wizew cay my,\n",
      "Thaze.\n",
      "\n",
      "AULAfave cere ns tmy EOnd tpral the yoro picetr.\n",
      "\n",
      "An Isome, kutinif chan ouse, tche my houch yidnt whe, se whame gowamendene, leare\n",
      "GMa-tho we ther hasindem hin cmbigse!n?\n",
      "\n",
      "A'F:\n",
      "D, boural.\n",
      "Hy,\n"
     ]
    }
   ],
   "source": [
    "# generate a single sequences using the model with start token 0\n",
    "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_seq = m.generate(idx, max_new_tokens=300)[0].tolist()\n",
    "# Decode integer tokens into characters\n",
    "generated_seq = decode(generated_seq)\n",
    "print(\"\\nGenerated sequence:\\n\",\"\".join(generated_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the improved model acheives a slightly lower loss and generate slightly better sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To acheive even better performance, we can use multiple attention heads in parallel and concatenate their outputs. This is called \"multi-head attention\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, head_size, num_heads):\n",
    "        super().__init__()\n",
    "        assert head_size % num_heads == 0, \"head_size needs to be integer multiple of num_heads\"\n",
    "        self.heads = nn.ModuleList([Head(block_size, embedding_dim, head_size//num_heads) for _ in range(num_heads)])\n",
    "\n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improved language model with multi-head self attention\n",
    "class ImprovedLanguageModelMultiHead(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, embedding_dim, head_size, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_size = head_size\n",
    "        self.hum_heads = num_heads\n",
    "\n",
    "        '''\n",
    "        Define model parameters\n",
    "        '''\n",
    "        # token embedding layer \n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim) # shape: (vocab_size,C)\n",
    "        # position embedding layer\n",
    "        self.pos_embedding = nn.Embedding(block_size, embedding_dim) # shape: (T,C)\n",
    "\n",
    "        # self-attention layer\n",
    "        self.sa_heads = MultiHeadAttention(block_size, embedding_dim, head_size, num_heads) # shape: (T,C,h)\n",
    "        # output layer logits\n",
    "        self.lm_head = nn.Linear(head_size, vocab_size) # shape: (h,vocab_size)\n",
    "\n",
    "\n",
    "        # forward pass takes in a batch of input token sequences of shape (B,T) and corresponding targets of shape (B,T)\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T =idx.shape\n",
    "        # get token embeddings\n",
    "        token_embeds = self.token_embedding(idx) # (B,T,C)\n",
    "        # add positional encoding\n",
    "        pos_embeds = self.pos_embedding(torch.arange(T, device=device)) # (T,C) \n",
    "        x = token_embeds + pos_embeds # (B,T,C)\n",
    "        # apply self-attention\n",
    "        x = self.sa_heads(x) # (B,T,h)\n",
    "        # compute output logits\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B,T,vocab_size = logits.shape\n",
    "            # reshape the logits and targets such that batch of input sequences are flattened into a single big input sequence\n",
    "            # i.e. (B,T) --> (B*T)\n",
    "            logits = logits.view(B*T,vocab_size) # reshaped to (B*T,vocab_size)\n",
    "            targets = targets.view(B*T) # reshaped to (B*T)\n",
    "            # compute cross entropy loss (i.e. average negative log likelihood)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "\n",
    "    # generates new sequences continuing from a given batch of context tokens\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # batch of contexts, idx has shape (B,T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # since we're using positional encoding, we need to crop idx if input sequence length exceeds block size (keep last block_size tokens)\n",
    "            idx_crop = idx[:,-block_size:] \n",
    "            # get predictions\n",
    "            logits, _ = self(idx_crop) # shape: (B,T,C)\n",
    "            # for each context sequence (in the batch), compute the probability of the next token using the logits of the last token in the context sequence\n",
    "            logits = logits[:,-1,:] # shape: (B,C)\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            # sample from the probability distribution to get next token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # shape: (B,1)\n",
    "            # append to the current context\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # shape: (B,T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "block_size = 8\n",
    "embedding_dim = 32\n",
    "head_size = 32\n",
    "num_heads = 4\n",
    "max_iters = 5000\n",
    "learning_rate = 1e-3\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "\n",
    "model = ImprovedLanguageModelMultiHead(vocab_size=vocab_size, block_size=block_size, embedding_dim=embedding_dim, head_size=head_size, num_heads=num_heads)\n",
    "# move model to device\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, training loss: 4.205431938171387, validation loss: 4.203817844390869\n",
      "epoch: 500, training loss: 2.7132978439331055, validation loss: 2.7140204906463623\n",
      "epoch: 1000, training loss: 2.540811538696289, validation loss: 2.5597493648529053\n",
      "epoch: 1500, training loss: 2.448634386062622, validation loss: 2.465970993041992\n",
      "epoch: 2000, training loss: 2.4019908905029297, validation loss: 2.4068872928619385\n",
      "epoch: 2500, training loss: 2.3573694229125977, validation loss: 2.3764777183532715\n",
      "epoch: 3000, training loss: 2.317030429840088, validation loss: 2.3318843841552734\n",
      "epoch: 3500, training loss: 2.302166700363159, validation loss: 2.3186120986938477\n",
      "epoch: 4000, training loss: 2.2924306392669678, validation loss: 2.3139798641204834\n",
      "epoch: 4500, training loss: 2.263468027114868, validation loss: 2.293119430541992\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(max_iters):\n",
    "    # sample a batch of trainin data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    _, loss = m(xb, yb)\n",
    "    # reset parameter gradients\n",
    "    optimizer.zero_grad(set_to_none=True) \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % eval_interval == 0:\n",
    "        losses = estimate_loss(m)\n",
    "        print(f\"epoch: {epoch}, training loss: {losses['train'].item()}, validation loss: {losses['val'].item()}\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sequence:\n",
      " \n",
      "Whertent\n",
      "'t the rer tis ther maly Rovere.\n",
      "\n",
      "CSoo are prorow oul ibe comd ye hakre cod eend you, thilesfladew And thie sourlak forust\n",
      "Marce\n",
      "Anl all iqeay Os! soron ange, will ge? helll doleant gio your, I cove I aris the awithtellaverat heros hads ford ton ralexr ge sime Reck kliker tain\n",
      "Thes\n",
      "The xorr tha's?\n",
      "LURTAROLOD ELIO MAMIK:\n",
      "I Cyourr\n",
      "Aned soord\n",
      "Malfn thit yomd In he im:\n",
      "Wilirsk.\n",
      "\n",
      "Thavepo diestinhater shin Bin the\n",
      "The wis be but mallas, Ead his.\n",
      "fe lim:\n",
      "QUCferither?\n",
      "No hyoul ankimingfore, houth dis samos writ Couf co win not wighst, tall; wy In pis so pluson lxandie one my se livy!\n",
      "What awathim fe jo'lllouele hist lon,\n",
      "Hat,\n",
      "And He a in wiss out my the loue arop I shily you youldeis ter Bucy At dat\n",
      "lalat ofd.\n",
      "\n",
      "MOENBENRICHK The fay tour eise, misch presterin mow she old, a, ingsefer torse\n"
     ]
    }
   ],
   "source": [
    "# generate a single sequences using the model with start token 0\n",
    "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_seq = m.generate(idx, max_new_tokens=800)[0].tolist()\n",
    "# Decode integer tokens into characters\n",
    "generated_seq = decode(generated_seq)\n",
    "print(\"\\nGenerated sequence:\\n\",\"\".join(generated_seq))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instead of calculating the output logits directly from the attention head output, we can put a feed forward (multi-layer perceptron) layer in between the attention head and output layer. This allows us to pack in extra computations and extract more meaningful representations from the attention output. This brings us to the Transformer (Decoder) Block. Each transformer block consists of a multihead self-attemtion layer followed by a feed-forward layer. Transformer blocks are also designed to be stacked up (similar to stacked CNNs and stacked RNNs) and therefore also incorporate residual conections and layer normalization to ensure that the gradients can backpropagate without any difficulty as the stack of transformer blocks become deeper.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple mlp \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, head_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(head_size, head_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# a transformer block consisting of a multihead attention-layer followed by a feed-forward layer\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, head_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(block_size, embedding_dim, head_size, num_heads)\n",
    "        self.ff = FeedForward(head_size, num_heads)\n",
    "        \n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        x = self.sa(x)\n",
    "        x = self.ff(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language model with multiple transfop4mer blocks\n",
    "class ImprovedLanguageModelTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, embedding_dim, head_size, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_size = head_size\n",
    "        self.hum_heads = num_heads\n",
    "\n",
    "        '''\n",
    "        Define model parameters\n",
    "        '''\n",
    "        # token embedding layer \n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim) # shape: (vocab_size,C)\n",
    "        # position embedding layer\n",
    "        self.pos_embedding = nn.Embedding(block_size, embedding_dim) # shape: (T,C)\n",
    "\n",
    "        # stack of 3 transformer blocks\n",
    "        self.blocks = nn.Sequential(\n",
    "            TransformerBlock(block_size, embedding_dim, head_size, num_heads),\n",
    "            TransformerBlock(block_size, embedding_dim, head_size, num_heads),\n",
    "            TransformerBlock(block_size, embedding_dim, head_size, num_heads),\n",
    "        )\n",
    "\n",
    "        # output layer logits\n",
    "        self.lm_head = nn.Linear(head_size, vocab_size) # shape: (h,vocab_size)\n",
    "\n",
    "\n",
    "        # forward pass takes in a batch of input token sequences of shape (B,T) and corresponding targets of shape (B,T)\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T =idx.shape\n",
    "        # get token embeddings\n",
    "        token_embeds = self.token_embedding(idx) # (B,T,C)\n",
    "        # add positional encoding\n",
    "        pos_embeds = self.pos_embedding(torch.arange(T, device=device)) # (T,C) \n",
    "        x = token_embeds + pos_embeds # (B,T,C)\n",
    "        # pass through transformer blocks\n",
    "        x = self.blocks(x) # (B,T,h)\n",
    "        # compute output logits\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B,T,vocab_size = logits.shape\n",
    "            # reshape the logits and targets such that batch of input sequences are flattened into a single big input sequence\n",
    "            # i.e. (B,T) --> (B*T)\n",
    "            logits = logits.view(B*T,vocab_size) # reshaped to (B*T,vocab_size)\n",
    "            targets = targets.view(B*T) # reshaped to (B*T)\n",
    "            # compute cross entropy loss (i.e. average negative log likelihood)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "\n",
    "    # generates new sequences continuing from a given batch of context tokens\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # batch of contexts, idx has shape (B,T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # since we're using positional encoding, we need to crop idx if input sequence length exceeds block size (keep last block_size tokens)\n",
    "            idx_crop = idx[:,-block_size:] \n",
    "            # get predictions\n",
    "            logits, _ = self(idx_crop) # shape: (B,T,C)\n",
    "            # for each context sequence (in the batch), compute the probability of the next token using the logits of the last token in the context sequence\n",
    "            logits = logits[:,-1,:] # shape: (B,C)\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            # sample from the probability distribution to get next token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # shape: (B,1)\n",
    "            # append to the current context\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # shape: (B,T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "block_size = 8\n",
    "embedding_dim = 32\n",
    "head_size = 32\n",
    "num_heads = 4\n",
    "max_iters = 5000\n",
    "learning_rate = 1e-3\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "\n",
    "model = ImprovedLanguageModelTransformer(vocab_size=vocab_size, block_size=block_size, embedding_dim=embedding_dim, head_size=head_size, num_heads=num_heads)\n",
    "# move model to device\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, training loss: 4.143747806549072, validation loss: 4.144392013549805\n",
      "epoch: 500, training loss: 3.0773422718048096, validation loss: 3.0846164226531982\n",
      "epoch: 1000, training loss: 2.732046127319336, validation loss: 2.716034173965454\n",
      "epoch: 1500, training loss: 2.5512638092041016, validation loss: 2.5352306365966797\n",
      "epoch: 2000, training loss: 2.455603837966919, validation loss: 2.442411422729492\n",
      "epoch: 2500, training loss: 2.409034490585327, validation loss: 2.4058051109313965\n",
      "epoch: 3000, training loss: 2.3722264766693115, validation loss: 2.3759915828704834\n",
      "epoch: 3500, training loss: 2.341911554336548, validation loss: 2.34486722946167\n",
      "epoch: 4000, training loss: 2.3220529556274414, validation loss: 2.3295435905456543\n",
      "epoch: 4500, training loss: 2.298309326171875, validation loss: 2.3365747928619385\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(max_iters):\n",
    "    # sample a batch of trainin data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    _, loss = m(xb, yb)\n",
    "    # reset parameter gradients\n",
    "    optimizer.zero_grad(set_to_none=True) \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % eval_interval == 0:\n",
    "        losses = estimate_loss(m)\n",
    "        print(f\"epoch: {epoch}, training loss: {losses['train'].item()}, validation loss: {losses['val'].item()}\")     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that the validation loss has not decreased any further, in fact having the single multi-head attention layer seems to have been better. This because we need to add residual connections in the transformer block to improve gradient backpropagation and also include layer norms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, head_size, num_heads):\n",
    "        super().__init__()\n",
    "        assert head_size % num_heads == 0, \"head_size needs to be integer multiple of num_heads\"\n",
    "        self.heads = nn.ModuleList([Head(block_size, embedding_dim, head_size//num_heads) for _ in range(num_heads)])\n",
    "\n",
    "        # we also need to apply a linear projection to make the output residual the same dimension as the input\n",
    "        self.proj = nn.Linear(head_size, embedding_dim) \n",
    "\n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "# a simple mlp \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        # we add extra computations by growing out the feed-forward hidden size by a factor of 4\n",
    "        # we also add an extra linear layer at the end to project the residual back to same dimensions as input\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 4*embedding_dim),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*embedding_dim, embedding_dim), \n",
    "        )\n",
    "    \n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "# transformer block with residual connection \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, head_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(block_size, embedding_dim, head_size, num_heads)\n",
    "        self.ff = FeedForward(embedding_dim)\n",
    "        \n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        # residual connection between input and multi-head attention output\n",
    "        x = x + self.sa(x)\n",
    "        # residual connection between multi-head attention output and feed-forward output\n",
    "        x = x + self.ff(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, training loss: 4.703917026519775, validation loss: 4.697531700134277\n",
      "epoch: 500, training loss: 2.385143518447876, validation loss: 2.3879919052124023\n",
      "epoch: 1000, training loss: 2.242703914642334, validation loss: 2.270765781402588\n",
      "epoch: 1500, training loss: 2.184208393096924, validation loss: 2.206484317779541\n",
      "epoch: 2000, training loss: 2.1248888969421387, validation loss: 2.173213005065918\n",
      "epoch: 2500, training loss: 2.095299005508423, validation loss: 2.1506059169769287\n",
      "epoch: 3000, training loss: 2.064594030380249, validation loss: 2.1176111698150635\n",
      "epoch: 3500, training loss: 2.0370447635650635, validation loss: 2.1078944206237793\n",
      "epoch: 4000, training loss: 2.025120973587036, validation loss: 2.0856645107269287\n",
      "epoch: 4500, training loss: 2.007169723510742, validation loss: 2.0710246562957764\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "block_size = 8\n",
    "embedding_dim = 32\n",
    "head_size = 32\n",
    "num_heads = 4\n",
    "max_iters = 5000\n",
    "learning_rate = 1e-3\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "\n",
    "model = ImprovedLanguageModelTransformer(vocab_size=vocab_size, block_size=block_size, embedding_dim=embedding_dim, head_size=head_size, num_heads=num_heads)\n",
    "# move model to device\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(max_iters):\n",
    "    # sample a batch of trainin data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    _, loss = m(xb, yb)\n",
    "    # reset parameter gradients\n",
    "    optimizer.zero_grad(set_to_none=True) \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % eval_interval == 0:\n",
    "        losses = estimate_loss(m)\n",
    "        print(f\"epoch: {epoch}, training loss: {losses['train'].item()}, validation loss: {losses['val'].item()}\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sequence:\n",
      " \n",
      "\n",
      "DUCHENTBULERCENT:\n",
      "QUENEN IIZABHARD IV:\n",
      "Thereringates the want say.\n",
      "\n",
      "CORIOMENT:\n",
      "AgUS:\n",
      "The af headmen most hews, lokegise I with cartless sovon's and your, Weeith hath a it to should my look their to there him hald in Marnted ett\n",
      "gent and it real'd-woord.\n",
      "\n",
      "ICABELL:\n",
      "I wath to thy the criend fathery maid all ade nos I my lyond Yor Wow my and the the affal did fill af my earton that I'll mark you abther;\n",
      "Lecab the terefer and the frather knecce.\n",
      "\n",
      "RICHARETIO:\n",
      "Nrike Rotake a atoo.\n",
      "\n",
      "FICKINGSOUCESTES:\n",
      "Non I countlees from thane, and dess of excons, he dood 'the khes?\n",
      "For Rest mere.\n",
      "\n",
      "COMINay of sumpidiisbe mein, whell Ratioud I knot but you may let wilfnstren; if and me your death; no me of epere wor am'd thou dath with at?\n",
      "ClALAND:\n",
      "Hy, gunk most we them not weet Hech all Hore, funt my poor to twam\n"
     ]
    }
   ],
   "source": [
    "# generate a single sequences using the model with start token 0\n",
    "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_seq = m.generate(idx, max_new_tokens=800)[0].tolist()\n",
    "# Decode integer tokens into characters\n",
    "generated_seq = decode(generated_seq)\n",
    "print(\"\\nGenerated sequence:\\n\",\"\".join(generated_seq))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that adding residual conmnections inside the transformer block has significantly improved the performance. The quality of generated text has also improved dramatically.\n",
    "\n",
    "#### The final icing on the cake is Layer Normalization, which serves a similar purpose as batch normalization. Layer normalization forces the output neurons of a linear layer to all have zero mean and unit variance. This ensures that activations and gradients are more stable and well-behaved during training. We will use pre-layer norms, i.e. layer norm will be applied at the input of the multi-head attention layer and the input of the feed-forward layer. \n",
    "\n",
    "#### We can also add some dropout layers in front of the multi-head attention and feed forward layers and also to the attention weights. Then all together, we have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, head_size, dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_size = head_size\n",
    "\n",
    "        # define parameters\n",
    "        self.key = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "\n",
    "        # non-parameter tensor of lower triangular ones\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "   \n",
    "    # define forward pass, input shape: (B,T,C) where B=batch size, T=block_size, C=embedding_dim\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B,T,h) where h is the head_size\n",
    "        q = self.query(x) # (B,T,h)\n",
    "        v = self.value(x) # (B,T,h)\n",
    "        W = q @ k.transpose(-2,-1)  * self.head_size**(-0.5) # (B,T,T)\n",
    "        W = W.masked_fill(self.tril[:T, :T] == 0, float('-inf')) \n",
    "        W = F.softmax(W, dim=-1)\n",
    "        # apply dropout to attention weights\n",
    "        W = self.dropout(W)\n",
    "        out = W @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, head_size, num_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "        assert head_size % num_heads == 0, \"head_size needs to be integer multiple of num_heads\"\n",
    "        self.heads = nn.ModuleList([Head(block_size, embedding_dim, head_size//num_heads, dropout_rate) for _ in range(num_heads)])\n",
    "\n",
    "        # we also need to apply a linear projection to make the output residual the same dimension as the input\n",
    "        self.proj = nn.Linear(head_size, embedding_dim) \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        x = self.proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "# a simple mlp \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "        # we add extra computations by growing out the feed-forward hidden size by a factor of 4\n",
    "        # we also add an extra linear layer at the end to project the residual back to same dimensions as input\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 4*embedding_dim),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*embedding_dim, embedding_dim), \n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "    \n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "# transformer block with residual connection and layer norm\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, head_size, num_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(block_size, embedding_dim, head_size, num_heads, dropout_rate) # multi-head attention layer \n",
    "        self.ff = FeedForward(embedding_dim, dropout_rate)   # feed-forward layer\n",
    "        self.ln1 = nn.LayerNorm(embedding_dim) # layer norm at input of multi-head attention\n",
    "        self.ln2 = nn.LayerNorm(embedding_dim) # layer norm at input of feed-forward\n",
    "\n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        # residual connection between input and multi-head attention output\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        # residual connection between multi-head attention output and feed-forward output\n",
    "        x = x + self.ff(self.ln2(x)) \n",
    "        return x\n",
    "    \n",
    "\n",
    "# language model with multiple transformer blocks\n",
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, embedding_dim, head_size, num_heads, num_blocks, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_size = head_size\n",
    "        self.hum_heads = num_heads\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        '''\n",
    "        Define model parameters\n",
    "        '''\n",
    "        # token embedding layer \n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim) # shape: (vocab_size,C)\n",
    "        # position embedding layer\n",
    "        self.pos_embedding = nn.Embedding(block_size, embedding_dim) # shape: (T,C)\n",
    "\n",
    "        # stack of transformer blocks\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(block_size, embedding_dim, head_size, num_heads, dropout_rate) for _ in range(num_blocks)])\n",
    "\n",
    "        # we also add a layer norm before the final output layer\n",
    "        self.ln_f = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        # output layer logits\n",
    "        self.lm_head = nn.Linear(head_size, vocab_size) # shape: (h,vocab_size)\n",
    "\n",
    "\n",
    "        # forward pass takes in a batch of input token sequences of shape (B,T) and corresponding targets of shape (B,T)\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T =idx.shape\n",
    "        # get token embeddings\n",
    "        token_embeds = self.token_embedding(idx) # (B,T,C)\n",
    "        # add positional encoding\n",
    "        pos_embeds = self.pos_embedding(torch.arange(T, device=device)) # (T,C) \n",
    "        x = token_embeds + pos_embeds # (B,T,C)\n",
    "        # pass through transformer blocks\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        # apply layer norm\n",
    "        x = self.ln_f(x)  # (B,T,C)\n",
    "        # compute output logits \n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B,T,vocab_size = logits.shape\n",
    "            # reshape the logits and targets such that batch of input sequences are flattened into a single big input sequence\n",
    "            # i.e. (B,T) --> (B*T)\n",
    "            logits = logits.view(B*T,vocab_size) # reshaped to (B*T,vocab_size)\n",
    "            targets = targets.view(B*T) # reshaped to (B*T)\n",
    "            # compute cross entropy loss (i.e. average negative log likelihood)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "\n",
    "    # generates new sequences continuing from a given batch of context tokens\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # batch of contexts, idx has shape (B,T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # since we're using positional encoding, we need to crop idx if input sequence length exceeds block size (keep last block_size tokens)\n",
    "            idx_crop = idx[:,-block_size:] \n",
    "            # get predictions\n",
    "            logits, _ = self(idx_crop) # shape: (B,T,C)\n",
    "            # for each context sequence (in the batch), compute the probability of the next token using the logits of the last token in the context sequence\n",
    "            logits = logits[:,-1,:] # shape: (B,C)\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            # sample from the probability distribution to get next token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # shape: (B,1)\n",
    "            # append to the current context\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # shape: (B,T+1)\n",
    "        return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with the improved transformer block which has both residual connections and pre-layer norms and scaling up the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "block_size = 256\n",
    "embedding_dim = 384\n",
    "head_size = embedding_dim\n",
    "num_heads = 6\n",
    "num_blocks = 6\n",
    "dropout_rate = 0.2\n",
    "max_iters = 5000\n",
    "learning_rate = 5e-4\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "\n",
    "model = TransformerLanguageModel(vocab_size=vocab_size, block_size=block_size, embedding_dim=embedding_dim, head_size=head_size, num_heads=num_heads, num_blocks=num_blocks, dropout_rate=dropout_rate)\n",
    "# move model to device\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 10.788929 M\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in m.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5000, Train Loss: 0.984, Val Loss: 1.500: 100%|██████████| 5000/5000 [27:27<00:00,  3.76it/s]   \n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "train_loss = None\n",
    "val_loss = None\n",
    "\n",
    "pbar = tqdm(range(max_iters), desc=\"Epochs\")\n",
    "for epoch in pbar:\n",
    "    # sample a batch of trainin data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    _, loss = m(xb, yb)\n",
    "    # reset parameter gradients\n",
    "    optimizer.zero_grad(set_to_none=True) \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % eval_interval == 0:\n",
    "        losses = estimate_loss(m)\n",
    "        #print(f\"epoch: {epoch}, training loss: {losses['train'].item()}, validation loss: {losses['val'].item()}\")     \n",
    "        train_loss = losses['train'].item()\n",
    "        val_loss = losses['val'].item()\n",
    "\n",
    "    pbar.set_description(f\"Epoch {epoch + 1}, Train Loss: {train_loss:.3f}, Val Loss: {val_loss:.3f}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sequence:\n",
      " \n",
      "Confederers, you up your trancome: our widows wail enter were then\n",
      "you wear in person to the pattience of you,\n",
      "then all against these bloody house; but\n",
      "they have put'd sufferanch, stone, cry-like, tirst, hoo!\n",
      "\n",
      "MENENIUS:\n",
      "Good Paucincio, coming is their name he,\n",
      "to colour all hearts cannot below their times\n",
      "will be obeyed. Come home: so justice! Let\n",
      "your brother first and my swordship\n",
      "beause your target, wind how must I say\n",
      "years not before the bloody sun stars.\n",
      "\n",
      "MENENIUS:\n",
      "I will find out, why are you read?\n",
      "\n",
      "CORIOLANUS:\n",
      "How is't then?\n",
      "\n",
      "BRUTUS:\n",
      "At odds?\n",
      "\n",
      "BRUTUS:\n",
      "Saw you your friend?\n",
      "\n",
      "Citizens:\n",
      "You thalk me, if hearing that will.\n",
      "\n",
      "VIRCIOLIO:\n",
      "If he know\n",
      "Then report he is far honour with our soldiers, as\n",
      "any fair button there is a thing,--\n",
      "\n",
      "MERCIUS:\n",
      "He can set on word his power too?\n",
      "\n",
      "COMINIUS:\n",
      "Let me lay and plucker with that you\n",
      "ELBOW:\n",
      "I was this im, and so venture misable,\n",
      "To think it partly on his tongue.\n",
      "\n",
      "MENENIUS:\n",
      "He hath\n",
      "To so incondemn'd with him tonigation; holds\n",
      "He in the taper of sup, in some basenial either\n",
      "So, heavy idle.\n",
      "\n",
      "SIR LIAUS:\n",
      "I'll repent thee, good my lord, to prison, that though\n",
      "My brainest loved in Lendon can never tell:\n",
      "'Lood will me stick on the father's minutes,\n",
      "And few, and the grumpell blush, so true up:\n",
      "My blood, will I; if I live, desmiliant in\n",
      "One wretches he was every two in this frail.\n",
      "\n",
      "Nurse:\n",
      "What to thee against the Volscians and his:\n",
      "Alas, Romeo, cousin Blood, and the issue\n",
      "With Romans doth put for't, off the prince:\n",
      "If this is it so, but I saw'd up\n",
      "Those throne is a wooer, or unmajestable days,\n",
      "And from such corival of teny of law from hear.\n",
      "Poor Clarence is now well: we cheed buildings,\n",
      "Interible out of this duke? We, Camillia, Montague,\n",
      "Can there o' the bastard calm, and we are enough\n",
      "As but words.\n",
      "\n",
      "SICINIUS:\n",
      "Were these dislined our highness cliends.\n",
      "\n",
      "MENENIUS:\n",
      "If they shall go so?\n",
      "\n",
      "SICINIUS:\n",
      "They will have their to cut them.\n",
      "What would they are that yet?\n",
      "\n",
      "CORIOLANUS:\n",
      "Frie, sweet child!\n",
      "\n",
      "COMINIUS:\n",
      "Then they say.\n",
      "\n",
      "CORIOLANUS:\n",
      "The city'\n"
     ]
    }
   ],
   "source": [
    "# generate a single sequences using the model with start token 0\n",
    "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_seq = m.generate(idx, max_new_tokens=2000)[0].tolist()\n",
    "# Decode integer tokens into characters\n",
    "generated_seq = decode(generated_seq)\n",
    "print(\"\\nGenerated sequence:\\n\",\"\".join(generated_seq))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In our multihead attention module, we are currently storing all the individula attention heads inside a list and using a for loop to sequentially compute the outputs from all the heads before concatenating them. This implementation is very inefficient, instead we can compute the outputs from all heads in parallel because they are independent. So let's re-implement the multihead attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, total_head_size, num_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        assert total_head_size % num_heads == 0, \"head_size needs to be integer multiple of num_heads\"\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.total_head_size = total_head_size \n",
    "        self.head_size = total_head_size // num_heads \n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # define parameters\n",
    "        self.key = nn.Linear(embedding_dim, self.total_head_size, bias=False)\n",
    "        self.query = nn.Linear(embedding_dim, self.total_head_size, bias=False)\n",
    "        self.value = nn.Linear(embedding_dim, self.total_head_size, bias=False)\n",
    "        self.attn_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # non-parameter tensor of lower triangular ones\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        # we also need to apply a linear projection to make the output residual the same dimension as the input\n",
    "        self.proj = nn.Linear(total_head_size, embedding_dim) \n",
    "        self.output_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "\n",
    "    # define forward pass, input shape: (B,T,C) where B=batch size, T=block_size, C=embedding_dim\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B,T,H) where H is the total_head_size\n",
    "        q = self.query(x) # (B,T,H)\n",
    "        v = self.value(x) # (B,T,H)\n",
    "\n",
    "        # reshape (B,T,H) --> (B,T,n,h), where n=num_heads and h=head_size and H=n*h\n",
    "        k = k.view(B,T,self.num_heads,self.head_size) \n",
    "        q = q.view(B,T,self.num_heads,self.head_size) \n",
    "        v = v.view(B,T,self.num_heads,self.head_size) \n",
    "\n",
    "        # now we transpose so that the num_heads is the second dimension followed by T,h\n",
    "        # this allows us to batch matrix mutliply for all heads simulataneously to compute their attention weights\n",
    "        # (B,T,n,h) --> (B,n,T,h) \n",
    "        k = k.transpose(1,2) \n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "\n",
    "        W = q @ k.transpose(-2,-1)  / math.sqrt(self.head_size) # (B,n,T,T)\n",
    "        W = W.masked_fill(self.tril[:T, :T] == 0, float('-inf')) \n",
    "        W = F.softmax(W, dim=-1)\n",
    "        # apply dropout to attention weights\n",
    "        W = self.attn_dropout(W)\n",
    "        out = W @ v # (B,n,T,h)\n",
    "\n",
    "        # we can transpose the output from (B,n,T,h) --> (B,T,n,h)\n",
    "        # since the last two dimensions of the transposed tensor are non-contiguous, we apply \n",
    "        # contiguous() which return a contiguous tensor\n",
    "        out = out.transpose(1,2).contiguous()\n",
    "\n",
    "        # finally we collapse the last two dimensions to get the concatenated output, (B,T,n,h) --> (B,T,n*h) \n",
    "        out = out.view(B,T,self.total_head_size)\n",
    "\n",
    "        # now we project the concatenated output so that it has the same dimensions as the multihead attention layer input\n",
    "        # (we need to add it with the input because of the residual connection, so need to be same size) \n",
    "        out = self.proj(out) # (B,T,C) \n",
    "\n",
    "        # apply dropout\n",
    "        out = self.output_dropout(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "# a simple mlp \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "        # we add extra computations by growing out the feed-forward hidden size by a factor of 4\n",
    "        # we also add an extra linear layer at the end to project the residual back to same dimensions as input\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 4*embedding_dim),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*embedding_dim, embedding_dim), \n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "    \n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "# transformer block with residual connection and layer norm\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, head_size, num_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(block_size, embedding_dim, head_size, num_heads, dropout_rate) # multi-head attention layer \n",
    "        self.ff = FeedForward(embedding_dim, dropout_rate)   # feed-forward layer\n",
    "        self.ln1 = nn.LayerNorm(embedding_dim) # layer norm at input of multi-head attention\n",
    "        self.ln2 = nn.LayerNorm(embedding_dim) # layer norm at input of feed-forward\n",
    "\n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        # residual connection between input and multi-head attention output\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        # residual connection between multi-head attention output and feed-forward output\n",
    "        x = x + self.ff(self.ln2(x)) \n",
    "        return x\n",
    "    \n",
    "\n",
    "# language model with multiple transformer blocks\n",
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, embedding_dim, head_size, num_heads, num_blocks, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_size = head_size\n",
    "        self.hum_heads = num_heads\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        '''\n",
    "        Define model parameters\n",
    "        '''\n",
    "        # token embedding layer \n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim) # shape: (vocab_size,C)\n",
    "        # position embedding layer\n",
    "        self.pos_embedding = nn.Embedding(block_size, embedding_dim) # shape: (T,C)\n",
    "\n",
    "        # stack of transformer blocks\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(block_size, embedding_dim, head_size, num_heads, dropout_rate) for _ in range(num_blocks)])\n",
    "\n",
    "        # we also add a layer norm before the final output layer\n",
    "        self.ln_f = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        # output layer logits\n",
    "        self.lm_head = nn.Linear(head_size, vocab_size) # shape: (h,vocab_size)\n",
    "\n",
    "\n",
    "        # forward pass takes in a batch of input token sequences of shape (B,T) and corresponding targets of shape (B,T)\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T =idx.shape\n",
    "        # get token embeddings\n",
    "        token_embeds = self.token_embedding(idx) # (B,T,C)\n",
    "        # add positional encoding\n",
    "        pos_embeds = self.pos_embedding(torch.arange(T, device=device)) # (T,C) \n",
    "        x = token_embeds + pos_embeds # (B,T,C)\n",
    "        # pass through transformer blocks\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        # apply layer norm\n",
    "        x = self.ln_f(x)  # (B,T,C)\n",
    "        # compute output logits \n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B,T,vocab_size = logits.shape\n",
    "            # reshape the logits and targets such that batch of input sequences are flattened into a single big input sequence\n",
    "            # i.e. (B,T) --> (B*T)\n",
    "            logits = logits.view(B*T,vocab_size) # reshaped to (B*T,vocab_size)\n",
    "            targets = targets.view(B*T) # reshaped to (B*T)\n",
    "            # compute cross entropy loss (i.e. average negative log likelihood)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "\n",
    "    # generates new sequences continuing from a given batch of context tokens\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        model.eval() # swicth to inference mode\n",
    "        # batch of contexts, idx has shape (B,T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # since we're using positional encoding, we need to crop idx if input sequence length exceeds block size (keep last block_size tokens)\n",
    "            idx_crop = idx[:,-block_size:] \n",
    "            # get predictions\n",
    "            logits, _ = self(idx_crop) # shape: (B,T,C)\n",
    "            # for each context sequence (in the batch), compute the probability of the next token using the logits of the last token in the context sequence\n",
    "            logits = logits[:,-1,:] # shape: (B,C)\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            # sample from the probability distribution to get next token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # shape: (B,1)\n",
    "            # append to the current context\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # shape: (B,T+1)\n",
    "\n",
    "        model.train() # swicth to train mode\n",
    "\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 10.788929 M\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "block_size = 256\n",
    "embedding_dim = 384\n",
    "head_size = embedding_dim\n",
    "num_heads = 6\n",
    "num_blocks = 6\n",
    "dropout_rate = 0.2\n",
    "max_iters = 4000\n",
    "learning_rate = 5e-4\n",
    "eval_interval = 200\n",
    "eval_iters = 200\n",
    "\n",
    "model = TransformerLanguageModel(vocab_size=vocab_size, block_size=block_size, embedding_dim=embedding_dim, head_size=head_size, num_heads=num_heads, num_blocks=num_blocks, dropout_rate=dropout_rate)\n",
    "# move model to device\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "\n",
    "num_params = sum(p.numel() for p in m.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4000, Train Loss: 1.040, Val Loss: 1.501: 100%|██████████| 4000/4000 [27:45<00:00,  4.07it/s]  \n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "train_loss = None\n",
    "val_loss = None\n",
    "\n",
    "pbar = tqdm(range(max_iters), desc=\"Epochs\")\n",
    "for epoch in pbar:\n",
    "    # sample a batch of trainin data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    _, loss = m(xb, yb)\n",
    "    # reset parameter gradients\n",
    "    optimizer.zero_grad(set_to_none=True) \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % eval_interval == 0:\n",
    "        losses = estimate_loss(m)\n",
    "        #print(f\"epoch: {epoch}, training loss: {losses['train'].item()}, validation loss: {losses['val'].item()}\")     \n",
    "        train_loss = losses['train'].item()\n",
    "        val_loss = losses['val'].item()\n",
    "\n",
    "    pbar.set_description(f\"Epoch {epoch + 1}, Train Loss: {train_loss:.3f}, Val Loss: {val_loss:.3f}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sequence:\n",
      " \n",
      "GREGORY:\n",
      "Then you have said, come to that false your children,\n",
      "To be a large spake to Romeo?\n",
      "\n",
      "QUEEN MARGARET:\n",
      "Quick capable in our labour is continue;\n",
      "For that is noise before this fiend of yours,\n",
      "Seal in Pomfret's heir; for every writing arms,\n",
      "As we will take him dryak of this post-hunger widow:\n",
      "As if no worthily scape to the matter,\n",
      "Shall then the other of it, for thee very counsel,\n",
      "Which tides enough they make one report.\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "This time of these tribunes made me embrace them,\n",
      "Have you a stiff with posterous slaughter to bed\n",
      "Against this I have, goddess of this mind\n",
      "Whereon you will, say to you all make a too much a mine\n",
      "To appear them!\n",
      "\n",
      "Nurse:\n",
      "My lord!\n",
      "\n",
      "Nurse:\n",
      "O woful\n",
      "As wary that guilty lords to will is queen:\n",
      "Now do I will perceive you for them not welcome.\n",
      "\n",
      "JOHN OF GAUNT:\n",
      "And farewell, my drudges fiends, cousin, let me speak,\n",
      "When she they would give all shake the tongues?\n",
      "\n",
      "Nurse:\n",
      "I will not for a trespass of service:\n",
      "For this day should cried 'knee'er cried.'\n",
      "\n",
      "JOHN OF GAUNT:\n",
      "God for this king! what says thou sing?\n",
      "And when in thy behind oaks to bed with blood toe?\n",
      "Yet on the celebring, I was she might she should\n",
      "serve she has pitied to be a flatterous Claudio\n",
      "And have fought and by thee; thou dast it were\n",
      "Miles, and reverended aptied steel'd as thou art,\n",
      "All thou from bespite of this dispatch'd,\n",
      "To let me not speak it: therefore, reconcile\n",
      "Her embrace was quickly received: then she like ones,\n",
      "And does out to meet fast, dearly and and danger is\n",
      "That true not unluck like a nubsking shut!\n",
      "Were not to religion of our back of point,\n",
      "Hath silent been in carries and till my very\n",
      "Too much happiness done; life, brother of the Irill\n",
      "Were up the bridegreom's rotten law: I must\n",
      "Affect me this you to make their feather. Fear not\n",
      "Be hold of all them, and there at perish. Therefore perjured\n",
      "My shroud at the house of York, if we should\n",
      "Be but only Duke of Norfolk, for the County\n",
      "Hast scope at granded be her outward, rest rebels;\n",
      "And therefore I be a little little\n"
     ]
    }
   ],
   "source": [
    "# generate a single sequences using the model with start token 0\n",
    "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_seq = m.generate(idx, max_new_tokens=2000)[0].tolist()\n",
    "# Decode integer tokens into characters\n",
    "generated_seq = decode(generated_seq)\n",
    "print(\"\\nGenerated sequence:\\n\",\"\".join(generated_seq))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
