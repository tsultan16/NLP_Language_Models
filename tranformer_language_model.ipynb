{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Language Model (based on Adrej Karpathy's nanoGPT tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1234)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the txt entire file into a single string\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of dataset in characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total length of dataset in characters: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character vocabulary:  ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# get vocabulary of characters\n",
    "vocab = sorted(set(list(text)))\n",
    "print(\"character vocabulary: \", vocab)\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text\n",
    "ctoi = {vocab[i]:i for i in range(vocab_size)}\n",
    "itoc = {i:vocab[i] for i in range(vocab_size)}\n",
    "encode = lambda s: [ctoi[c] for c in s]  # converts a string to integer token sequence\n",
    "decode = lambda s: [itoc[ix] for ix in s]  # converts an integer token sequence to string of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42, 2]\n",
      "['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!']\n"
     ]
    }
   ],
   "source": [
    "print(encode('Hello world!'))\n",
    "print(decode(encode('Hello world!')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "# tokenize the dataset into integer sequence, convert to torch tensor of type int64\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long) \n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-validation splits (90-10)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split the data into chuncks of size block_size. For each chunk, we create (input,target) pairs for next character prediction, where the input is a context window containing all characters preceding the target character. Note that the context sizes range from 1 up to block size, i.e. there will be block_size number of (input,target) pairs per chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: tensor([18]) --> target: 47\n",
      "Context: tensor([18, 47]) --> target: 56\n",
      "Context: tensor([18, 47, 56]) --> target: 57\n",
      "Context: tensor([18, 47, 56, 57]) --> target: 58\n",
      "Context: tensor([18, 47, 56, 57, 58]) --> target: 1\n",
      "Context: tensor([18, 47, 56, 57, 58,  1]) --> target: 15\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15]) --> target: 47\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15, 47]) --> target: 58\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "\n",
    "# example showing the first chunk and all possible (input,target) pairs we can get from it\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"Context: {context} --> target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create a batch generator which creates a batch of randomly selected blocks/chunks from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input batch: \n",
      "torch.Size([4, 8])\n",
      "tensor([[17, 17, 26,  1, 17, 24, 21, 38],\n",
      "        [14, 17, 24, 24, 13, 10,  0, 28],\n",
      "        [63,  1, 47, 52,  1, 56, 43, 55],\n",
      "        [56, 59, 57, 58,  1, 59, 54, 53]], device='cuda:0')\n",
      "target batch: \n",
      "torch.Size([4, 8])\n",
      "tensor([[17, 26,  1, 17, 24, 21, 38, 13],\n",
      "        [17, 24, 24, 13, 10,  0, 28, 50],\n",
      "        [ 1, 47, 52,  1, 56, 43, 55, 59],\n",
      "        [59, 57, 58,  1, 59, 54, 53, 52]], device='cuda:0')\n",
      "\n",
      "A batch of 4 blocks:\n",
      "\n",
      "Block 0:\n",
      "Context: [17] --> target: 17\n",
      "Context: [17, 17] --> target: 26\n",
      "Context: [17, 17, 26] --> target: 1\n",
      "Context: [17, 17, 26, 1] --> target: 17\n",
      "Context: [17, 17, 26, 1, 17] --> target: 24\n",
      "Context: [17, 17, 26, 1, 17, 24] --> target: 21\n",
      "Context: [17, 17, 26, 1, 17, 24, 21] --> target: 38\n",
      "Context: [17, 17, 26, 1, 17, 24, 21, 38] --> target: 13\n",
      "\n",
      "\n",
      "Block 1:\n",
      "Context: [14] --> target: 17\n",
      "Context: [14, 17] --> target: 24\n",
      "Context: [14, 17, 24] --> target: 24\n",
      "Context: [14, 17, 24, 24] --> target: 13\n",
      "Context: [14, 17, 24, 24, 13] --> target: 10\n",
      "Context: [14, 17, 24, 24, 13, 10] --> target: 0\n",
      "Context: [14, 17, 24, 24, 13, 10, 0] --> target: 28\n",
      "Context: [14, 17, 24, 24, 13, 10, 0, 28] --> target: 50\n",
      "\n",
      "\n",
      "Block 2:\n",
      "Context: [63] --> target: 1\n",
      "Context: [63, 1] --> target: 47\n",
      "Context: [63, 1, 47] --> target: 52\n",
      "Context: [63, 1, 47, 52] --> target: 1\n",
      "Context: [63, 1, 47, 52, 1] --> target: 56\n",
      "Context: [63, 1, 47, 52, 1, 56] --> target: 43\n",
      "Context: [63, 1, 47, 52, 1, 56, 43] --> target: 55\n",
      "Context: [63, 1, 47, 52, 1, 56, 43, 55] --> target: 59\n",
      "\n",
      "\n",
      "Block 3:\n",
      "Context: [56] --> target: 59\n",
      "Context: [56, 59] --> target: 57\n",
      "Context: [56, 59, 57] --> target: 58\n",
      "Context: [56, 59, 57, 58] --> target: 1\n",
      "Context: [56, 59, 57, 58, 1] --> target: 59\n",
      "Context: [56, 59, 57, 58, 1, 59] --> target: 54\n",
      "Context: [56, 59, 57, 58, 1, 59, 54] --> target: 53\n",
      "Context: [56, 59, 57, 58, 1, 59, 54, 53] --> target: 52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1223)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "# data loader (generates a bvatch of randomly selected blocks)\n",
    "def get_batch(split='train'):\n",
    "    data = train_data if split=='train' else val_data\n",
    "\n",
    "    # sample positions from which to grab blocks\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])      \n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) # move tensors to gpu \n",
    "    return x,y \n",
    "\n",
    "xbatch, ybatch = get_batch('train')\n",
    "print(\"input batch: \")\n",
    "print(xbatch.shape)\n",
    "print(xbatch)\n",
    "print(\"target batch: \")\n",
    "print(ybatch.shape)     \n",
    "print(ybatch)     \n",
    "print(\"\")\n",
    "\n",
    "# context target pairs\n",
    "print(f\"A batch of {batch_size} blocks:\")\n",
    "for b in range(batch_size): # batch dimension\n",
    "    print(f\"\\nBlock {b}:\")\n",
    "    for t in range(block_size):  # time dimension\n",
    "        context = xbatch[b,:t+1]\n",
    "        target = ybatch[b,t]\n",
    "        print(f\"Context: {context.tolist()} --> target: {target}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's create a pytorch-ified Bi-gram language model (will serve as a baseline for comparing the transformer model later on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "batch_size = 64\n",
    "block_size = 8\n",
    "max_iters = 6000\n",
    "learning_rate = 1e-2\n",
    "eval_interval = 300\n",
    "eval_iters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        Define model parameters\n",
    "        '''\n",
    "        # lookup table for finding logits for the next token (i.e. log of counts for all possible next token given input token)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # shape: (C,C)\n",
    "\n",
    "\n",
    "    # forward pass takes in a batch of input token sequences of shape (B,T) and corresponding targets of shape (B,T)\n",
    "    def forward(self, idx, targets=None):\n",
    "        # get logits for every input token\n",
    "        logits = self.token_embedding_table(idx) # shape: (B,T,C)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B,T,C = logits.shape\n",
    "            # reshape the logits and targets such that batch of input sequences are flattened into a single big input sequence\n",
    "            # i.e. (B,T) --> (B*T)\n",
    "            logits = logits.view(B*T,C) # reshaped to (B*T,C)\n",
    "            targets = targets.view(B*T) # reshaped to (B*T)\n",
    "            # compute cross entropy loss (i.e. average negative log likelihood)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    # generates new sequences continuing from a given batch of context tokens\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # batch of contexts, idx has shape (B,T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get predictions\n",
    "            logits, _ = self(idx) # shape: (B,T,C)\n",
    "            # for each context sequence (in the batch), compute the probability of the next token using the logits of the last token in the context sequence\n",
    "            logits = logits[:,-1,:] # shape: (B,C)\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            # sample from the probability distribution to get next token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # shape: (B,1)\n",
    "            # append to the current context\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # shape: (B,T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.3812, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Generated sequence:\n",
      " \n",
      "zuPdOf$JQmmgsWCjS,yoc.obDjcewb:by\n",
      "ZzRAKGTx\n",
      "Xn3YnigR,\n",
      "T;t:'e$BszJiwljm?REKce'DuIN'-KY?fiwJpAS:bt-? M \n"
     ]
    }
   ],
   "source": [
    "# create a bigram language model and test it on the example batch\n",
    "model = BigramLanguageModel(vocab_size=vocab_size)\n",
    "# move model to device\n",
    "m = model.to(device)\n",
    "logits, loss = m(xbatch, ybatch)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# generate a single sequences using the model with start token 0\n",
    "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_seq = m.generate(idx, max_new_tokens=100)[0].tolist()\n",
    "# Decode integer tokens into characters\n",
    "generated_seq = decode(generated_seq)\n",
    "print(\"\\nGenerated sequence:\\n\",\"\".join(generated_seq))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated sequence looks like gibberish, because model is untrained. We now train the model using a graident based optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating training and validation losses averaged over lots of batches\n",
    "@torch.no_grad() # disable gradient tracking\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval() # swicth to inference mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split) \n",
    "            _, loss = m(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean() \n",
    "    model.train() # switch back to training mode\n",
    "    return out       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, training loss: 4.598198413848877, validation loss: 4.587023735046387\n",
      "epoch: 300, training loss: 2.735320806503296, validation loss: 2.7429261207580566\n",
      "epoch: 600, training loss: 2.524005174636841, validation loss: 2.54231858253479\n",
      "epoch: 900, training loss: 2.4882287979125977, validation loss: 2.499865770339966\n",
      "epoch: 1200, training loss: 2.4708216190338135, validation loss: 2.4979774951934814\n",
      "epoch: 1500, training loss: 2.4736366271972656, validation loss: 2.495673894882202\n",
      "epoch: 1800, training loss: 2.4659011363983154, validation loss: 2.486417770385742\n",
      "epoch: 2100, training loss: 2.4783377647399902, validation loss: 2.4822909832000732\n",
      "epoch: 2400, training loss: 2.4603683948516846, validation loss: 2.4931557178497314\n",
      "epoch: 2700, training loss: 2.4593422412872314, validation loss: 2.4919941425323486\n",
      "epoch: 3000, training loss: 2.4662582874298096, validation loss: 2.480480194091797\n",
      "epoch: 3300, training loss: 2.4537932872772217, validation loss: 2.4837450981140137\n",
      "epoch: 3600, training loss: 2.4631268978118896, validation loss: 2.490421772003174\n",
      "epoch: 3900, training loss: 2.4539620876312256, validation loss: 2.4880266189575195\n",
      "epoch: 4200, training loss: 2.4670491218566895, validation loss: 2.482585906982422\n",
      "epoch: 4500, training loss: 2.4545273780822754, validation loss: 2.491215229034424\n",
      "epoch: 4800, training loss: 2.461135149002075, validation loss: 2.4918999671936035\n",
      "epoch: 5100, training loss: 2.4698903560638428, validation loss: 2.479996919631958\n",
      "epoch: 5400, training loss: 2.454688787460327, validation loss: 2.488793134689331\n",
      "epoch: 5700, training loss: 2.4547009468078613, validation loss: 2.482509136199951\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_iters):\n",
    "    # sample a batch of trainin data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    _, loss = m(xb, yb)\n",
    "    # reset parameter gradients\n",
    "    optimizer.zero_grad(set_to_none=True) \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % eval_interval == 0:\n",
    "        losses = estimate_loss(m)\n",
    "        print(f\"epoch: {epoch}, training loss: {losses['train'].item()}, validation loss: {losses['val'].item()}\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try generating some text using the trained bigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sequence:\n",
      " \n",
      "\n",
      "They?\n",
      "Wha dre\n",
      "By'haty.\n",
      "MERThe may mem ad tees atomy acoucoowe,\n",
      "My?\n",
      "An went yollle\n",
      "Tunis ghest he\n",
      "\n",
      "NCory s wir theanoreimeca I Theist her peshainds bu? whamou larnd isthy t s, s dony atheeawioure s\n",
      "\n",
      "Mono at the, wicekeee'lowhe he it wipise bethan horomered.\n",
      "\n",
      "ARo, ovearannumime\n",
      "Boumet ho t ator tull \n"
     ]
    }
   ],
   "source": [
    "# generate a single sequences using the model with start token 0\n",
    "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_seq = m.generate(idx, max_new_tokens=300)[0].tolist()\n",
    "# Decode integer tokens into characters\n",
    "generated_seq = decode(generated_seq)\n",
    "print(\"\\nGenerated sequence:\\n\",\"\".join(generated_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better! It has similar syntactic structure as the training text and even has some correct words. The quality is still very bad because the context window is too small, only the previous character is used to predict the next character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-attention basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider a batch of 4 sequences, with 8 token embeddings in each sequence, with embedding dims=2\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for each sequence, we will create context windows randing from size 1 up to T. The context of size t is then computed as the average of the embeddings of all tokens up to position t. This simple averaging gives us a \"bag of words\" context which has no awareness of relative positions of the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # embedding vectors for all tokens in the context window of size t --> shape: (t,C)  \n",
    "        # compute bag of words context of size t for the bth sequence\n",
    "        xbow[b,t] = xprev.mean(dim=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more efficient way of computing these context vectors is using matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.]])\n",
      "\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 2.],\n",
      "        [2., 2.]])\n",
      "\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "tensor([[1.0000, 1.0000],\n",
      "        [0.5000, 1.0000],\n",
      "        [0.6667, 0.6667]])\n"
     ]
    }
   ],
   "source": [
    "# consider a single sequence of 3 tokens with embedding dims of 3 --> shape: (3,3)\n",
    "x = torch.tensor([[1,1], [0,1], [1,0]], dtype=torch.float32) # each row is an embedding vector\n",
    "print(x)\n",
    "print(\"\")\n",
    "# then to get the context of shape: (3,2), in which the the t-th row is the sum of the first t rows in x\n",
    "# we can simply multiply a lower triangular matrix in which all elements above the diagonal are zero and the rest are 1s\n",
    "W = torch.tril(torch.ones(3,3))\n",
    "print(W)   \n",
    "xbow = W @ x\n",
    "print(xbow)\n",
    "print(\"\")\n",
    "\n",
    "# however, we don't want the sum of embedding vectors, but the mean so instead do the following\n",
    "W = torch.tril(torch.ones(3,3))\n",
    "W = W / W.sum(dim=1, keepdims=True)\n",
    "print(W)\n",
    "xbow = W @ x\n",
    "print(xbow)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So each row in the weights matrix W gives us the weights for summing up the embedding vectors of all the token. For the ith row, all weights after the ith column are zero,\n",
    "which means the weighted sum for the ith context only includes tokens up to and including the ith position (in our example, we used uniform weights). So effectively, we're masking out all \"future\" tokens so that the context only depends on current and past tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then for a batch of sequences, we can do the following\n",
    "x = torch.randn(B,T,C)\n",
    "W = torch.tril(torch.ones(T,T))\n",
    "W = W / W.sum(dim=1, keepdims=True)\n",
    "xbow = W @ x # batch matrix multiplication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "# another way to generate the lower triangular matrix needed to compute the mean context vectors is as follows\n",
    "A = torch.tril(torch.ones(T,T))\n",
    "W = torch.zeros((T,T))\n",
    "W = W.masked_fill(A == 0, float('-inf')) # masked fill replaces every element in A which equals 0 with -infinity \n",
    "print(W)\n",
    "# then by taking the softmax, we get the desired matrix \n",
    "W = F.softmax(W, dim=-1)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In a self-attention head, these attention weights are not uniform, but are instead computed using the (key, query) vectors of each token in the sequence. Then the output of the attention-head is the weighted sum of value vectors of the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "un-normalized attention weights for first sequence in batch:\n",
      "\n",
      "tensor([[-0.3255, -0.1359,  0.7023,  0.7843,  0.9132, -0.0642,  0.7554, -0.7008],\n",
      "        [ 0.4533, -0.0343, -0.0129, -0.1061, -0.1898, -0.3820, -0.2144,  0.0173],\n",
      "        [-1.8406,  0.1965, -0.1939,  0.1792,  0.4945,  1.6717,  0.6568,  0.1746],\n",
      "        [-1.8140,  0.2287, -0.3421,  0.0223,  0.3180,  1.7213,  0.5163,  0.3221],\n",
      "        [-1.9375,  0.2729, -0.4890, -0.1024,  0.2012,  1.8989,  0.4442,  0.4667],\n",
      "        [ 1.1779,  0.0204, -0.5070, -0.7595, -1.0239, -0.7616, -0.9681,  0.5151],\n",
      "        [-1.4515,  0.2315, -0.4831, -0.1960,  0.0198,  1.4796,  0.2314,  0.4657],\n",
      "        [ 1.8251, -0.1966,  0.1995, -0.1704, -0.4822, -1.6611, -0.6450, -0.1803]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "unnormalized masked attention weights:\n",
      "\n",
      "tensor([[-0.3255,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.4533, -0.0343,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-1.8406,  0.1965, -0.1939,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-1.8140,  0.2287, -0.3421,  0.0223,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-1.9375,  0.2729, -0.4890, -0.1024,  0.2012,    -inf,    -inf,    -inf],\n",
      "        [ 1.1779,  0.0204, -0.5070, -0.7595, -1.0239, -0.7616,    -inf,    -inf],\n",
      "        [-1.4515,  0.2315, -0.4831, -0.1960,  0.0198,  1.4796,  0.2314,    -inf],\n",
      "        [ 1.8251, -0.1966,  0.1995, -0.1704, -0.4822, -1.6611, -0.6450, -0.1803]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "Normalized masked attention weights:\n",
      "\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6195, 0.3805, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0722, 0.5534, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0517, 0.3987, 0.2253, 0.3243, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0343, 0.3131, 0.1461, 0.2151, 0.2914, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5268, 0.1656, 0.0977, 0.0759, 0.0583, 0.0757, 0.0000, 0.0000],\n",
      "        [0.0244, 0.1312, 0.0642, 0.0856, 0.1062, 0.4572, 0.1312, 0.0000],\n",
      "        [0.5511, 0.0730, 0.1085, 0.0749, 0.0549, 0.0169, 0.0466, 0.0742]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "head_size = 16 # this is the dimensions of the key and query vectors\n",
    "\n",
    "# the key and query vector are ontained by a linear transform of the embeddings obtained by multiplying with (C, head_size) matrices of learnable weights\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# then given a batch of token sequence embeddings\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B,T,C) # (B,T,C)\n",
    "\n",
    "# we can compute the query, key and value vectors for each token as follows\n",
    "k = key(x) # (B,T,h) where h=16 is the head_size\n",
    "q = query(x) # (B,T,h)\n",
    "v = value(x) # (B,T,h)\n",
    "\n",
    "# then for a sequence of tokens, the (i,j)th attention weight is assigned to be the dot product of the query vector of ith token\n",
    "# with key vector of jth token, so for the entuire batch we have the following\n",
    "W = q @ k.transpose(-2,-1)  # we've transposed the key matrix: (B,T,h) --> (B,h,T), the shape of the matrix multiplication result is: (B,T,h) @ (B,h,T) = (B,T,T)\n",
    "\n",
    "# we also scale the unnormalized weights to have variance of roughly 1\n",
    "W = W * head_size**(-0.5)\n",
    "\n",
    "\n",
    "print(\"\\nun-normalized attention weights for first sequence in batch:\\n\")\n",
    "print(W[0]) \n",
    "\n",
    "# then we apply the \"temporal\" masking so that the attention weights of future tokens is zero and also normalize so that weights sum to one\n",
    "A = torch.tril(torch.ones(T,T))\n",
    "W = W.masked_fill(A == 0, float('-inf')) # masked fill replaces every element in A which equals 0 with -infinity \n",
    "print(\"\\nunnormalized masked attention weights:\\n\")\n",
    "print(W[0]) \n",
    "W = F.softmax(W, dim=-1)\n",
    "print(\"\\nNormalized masked attention weights:\\n\")\n",
    "print(W[0])\n",
    "\n",
    "# the output of the self-attention head is then the sums of the token embeddings weighted by the attention weights\n",
    "out = W @ v # (B,T,T) @ (B,T,h) = (B,T,h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now using the idea of self-attention, we will design a better language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we create a single self-attention head module\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_size = head_size\n",
    "\n",
    "        # define parameters\n",
    "        self.key = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "\n",
    "        # non-parameter tensor of lower triangular ones\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "   \n",
    "    # define forward pass, input shape: (B,T,C) where B=batch size, T=block_size, C=embedding_dim\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B,T,h) where h is the head_size\n",
    "        q = self.query(x) # (B,T,h)\n",
    "        v = self.value(x) # (B,T,h)\n",
    "        W = q @ k.transpose(-2,-1)  * self.head_size**(-0.5) # (B,T,T)\n",
    "        W = W.masked_fill(self.tril[:T, :T] == 0, float('-inf')) \n",
    "        W = F.softmax(W, dim=-1)\n",
    "        out = W @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "class ImprovedLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, embedding_dim, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_size = head_size\n",
    "\n",
    "        '''\n",
    "        Define model parameters\n",
    "        '''\n",
    "        # token embedding layer \n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim) # shape: (vocab_size,C)\n",
    "        # position embedding layer\n",
    "        self.pos_embedding = nn.Embedding(block_size, embedding_dim) # shape: (T,C)\n",
    "\n",
    "        # self-attention layer\n",
    "        self.sa_head = Head(block_size, embedding_dim, head_size) # shape: (T,C,h)\n",
    "        # output layer logits\n",
    "        self.lm_head = nn.Linear(head_size, vocab_size) # shape: (h,vocab_size)\n",
    "\n",
    "\n",
    "        # forward pass takes in a batch of input token sequences of shape (B,T) and corresponding targets of shape (B,T)\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T =idx.shape\n",
    "        # get token embeddings\n",
    "        token_embeds = self.token_embedding(idx) # (B,T,C)\n",
    "        # add positional encoding\n",
    "        pos_embeds = self.pos_embedding(torch.arange(T, device=device)) # (T,C) \n",
    "        x = token_embeds + pos_embeds # (B,T,C)\n",
    "        # apply self-attention\n",
    "        x = self.sa_head(x) # (B,T,h)\n",
    "        # compute output logits\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B,T,vocab_size = logits.shape\n",
    "            # reshape the logits and targets such that batch of input sequences are flattened into a single big input sequence\n",
    "            # i.e. (B,T) --> (B*T)\n",
    "            logits = logits.view(B*T,vocab_size) # reshaped to (B*T,vocab_size)\n",
    "            targets = targets.view(B*T) # reshaped to (B*T)\n",
    "            # compute cross entropy loss (i.e. average negative log likelihood)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "\n",
    "    # generates new sequences continuing from a given batch of context tokens\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # batch of contexts, idx has shape (B,T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # since we're using positional encoding, we need to crop idx if input sequence length exceeds block size (keep last block_size tokens)\n",
    "            idx_crop = idx[:,-block_size:] \n",
    "            # get predictions\n",
    "            logits, _ = self(idx_crop) # shape: (B,T,C)\n",
    "            # for each context sequence (in the batch), compute the probability of the next token using the logits of the last token in the context sequence\n",
    "            logits = logits[:,-1,:] # shape: (B,C)\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            # sample from the probability distribution to get next token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # shape: (B,1)\n",
    "            # append to the current context\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # shape: (B,T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train this improved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "block_size = 8\n",
    "embedding_dim = 32\n",
    "head_size = 32\n",
    "max_iters = 5000\n",
    "learning_rate = 1e-3\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "\n",
    "model = ImprovedLanguageModel(vocab_size=vocab_size, block_size=block_size, embedding_dim=embedding_dim, head_size=head_size)\n",
    "# move model to device\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, training loss: 4.1714324951171875, validation loss: 4.177326202392578\n",
      "epoch: 500, training loss: 2.7168636322021484, validation loss: 2.7289960384368896\n",
      "epoch: 1000, training loss: 2.5374414920806885, validation loss: 2.538370132446289\n",
      "epoch: 1500, training loss: 2.472914695739746, validation loss: 2.4781994819641113\n",
      "epoch: 2000, training loss: 2.4376065731048584, validation loss: 2.4514846801757812\n",
      "epoch: 2500, training loss: 2.4210972785949707, validation loss: 2.4393491744995117\n",
      "epoch: 3000, training loss: 2.409301996231079, validation loss: 2.4124605655670166\n",
      "epoch: 3500, training loss: 2.4062507152557373, validation loss: 2.4262123107910156\n",
      "epoch: 4000, training loss: 2.3829004764556885, validation loss: 2.4174094200134277\n",
      "epoch: 4500, training loss: 2.382000684738159, validation loss: 2.401224374771118\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(max_iters):\n",
    "    # sample a batch of trainin data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    _, loss = m(xb, yb)\n",
    "    # reset parameter gradients\n",
    "    optimizer.zero_grad(set_to_none=True) \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % eval_interval == 0:\n",
    "        losses = estimate_loss(m)\n",
    "        print(f\"epoch: {epoch}, training loss: {losses['train'].item()}, validation loss: {losses['val'].item()}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sequence:\n",
      " \n",
      "Ma gi.\n",
      "SOF noy:\n",
      "LI I at by;\n",
      "Wun thir bathr I igrithangesarde thald sat therin\n",
      "Sho oms wizew cay my,\n",
      "Thaze.\n",
      "\n",
      "AULAfave cere ns tmy EOnd tpral the yoro picetr.\n",
      "\n",
      "An Isome, kutinif chan ouse, tche my houch yidnt whe, se whame gowamendene, leare\n",
      "GMa-tho we ther hasindem hin cmbigse!n?\n",
      "\n",
      "A'F:\n",
      "D, boural.\n",
      "Hy,\n"
     ]
    }
   ],
   "source": [
    "# generate a single sequences using the model with start token 0\n",
    "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_seq = m.generate(idx, max_new_tokens=300)[0].tolist()\n",
    "# Decode integer tokens into characters\n",
    "generated_seq = decode(generated_seq)\n",
    "print(\"\\nGenerated sequence:\\n\",\"\".join(generated_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the improved model acheives a slightly lower loss and generate slightly better sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To acheive even better performance, we can use multiple attention heads in parallel and concatenate their outputs. This is called \"multi-head attention\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, head_size, num_heads):\n",
    "        super().__init__()\n",
    "        assert head_size % num_heads == 0, \"head_size needs to be integer multiple of num_heads\"\n",
    "        self.heads = nn.ModuleList([Head(block_size, embedding_dim, head_size//num_heads) for _ in range(num_heads)])\n",
    "\n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improved language model with multi-head self attention\n",
    "class ImprovedLanguageModelMultiHead(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, embedding_dim, head_size, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_size = head_size\n",
    "        self.hum_heads = num_heads\n",
    "\n",
    "        '''\n",
    "        Define model parameters\n",
    "        '''\n",
    "        # token embedding layer \n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim) # shape: (vocab_size,C)\n",
    "        # position embedding layer\n",
    "        self.pos_embedding = nn.Embedding(block_size, embedding_dim) # shape: (T,C)\n",
    "\n",
    "        # self-attention layer\n",
    "        self.sa_heads = MultiHeadAttention(block_size, embedding_dim, head_size, num_heads) # shape: (T,C,h)\n",
    "        # output layer logits\n",
    "        self.lm_head = nn.Linear(head_size, vocab_size) # shape: (h,vocab_size)\n",
    "\n",
    "\n",
    "        # forward pass takes in a batch of input token sequences of shape (B,T) and corresponding targets of shape (B,T)\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T =idx.shape\n",
    "        # get token embeddings\n",
    "        token_embeds = self.token_embedding(idx) # (B,T,C)\n",
    "        # add positional encoding\n",
    "        pos_embeds = self.pos_embedding(torch.arange(T, device=device)) # (T,C) \n",
    "        x = token_embeds + pos_embeds # (B,T,C)\n",
    "        # apply self-attention\n",
    "        x = self.sa_heads(x) # (B,T,h)\n",
    "        # compute output logits\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B,T,vocab_size = logits.shape\n",
    "            # reshape the logits and targets such that batch of input sequences are flattened into a single big input sequence\n",
    "            # i.e. (B,T) --> (B*T)\n",
    "            logits = logits.view(B*T,vocab_size) # reshaped to (B*T,vocab_size)\n",
    "            targets = targets.view(B*T) # reshaped to (B*T)\n",
    "            # compute cross entropy loss (i.e. average negative log likelihood)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "\n",
    "    # generates new sequences continuing from a given batch of context tokens\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # batch of contexts, idx has shape (B,T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # since we're using positional encoding, we need to crop idx if input sequence length exceeds block size (keep last block_size tokens)\n",
    "            idx_crop = idx[:,-block_size:] \n",
    "            # get predictions\n",
    "            logits, _ = self(idx_crop) # shape: (B,T,C)\n",
    "            # for each context sequence (in the batch), compute the probability of the next token using the logits of the last token in the context sequence\n",
    "            logits = logits[:,-1,:] # shape: (B,C)\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            # sample from the probability distribution to get next token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # shape: (B,1)\n",
    "            # append to the current context\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # shape: (B,T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "block_size = 8\n",
    "embedding_dim = 32\n",
    "head_size = 32\n",
    "num_heads = 4\n",
    "max_iters = 5000\n",
    "learning_rate = 1e-3\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "\n",
    "model = ImprovedLanguageModelMultiHead(vocab_size=vocab_size, block_size=block_size, embedding_dim=embedding_dim, head_size=head_size, num_heads=num_heads)\n",
    "# move model to device\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, training loss: 4.205431938171387, validation loss: 4.203817844390869\n",
      "epoch: 500, training loss: 2.7132978439331055, validation loss: 2.7140204906463623\n",
      "epoch: 1000, training loss: 2.540811538696289, validation loss: 2.5597493648529053\n",
      "epoch: 1500, training loss: 2.448634386062622, validation loss: 2.465970993041992\n",
      "epoch: 2000, training loss: 2.4019908905029297, validation loss: 2.4068872928619385\n",
      "epoch: 2500, training loss: 2.3573694229125977, validation loss: 2.3764777183532715\n",
      "epoch: 3000, training loss: 2.317030429840088, validation loss: 2.3318843841552734\n",
      "epoch: 3500, training loss: 2.302166700363159, validation loss: 2.3186120986938477\n",
      "epoch: 4000, training loss: 2.2924306392669678, validation loss: 2.3139798641204834\n",
      "epoch: 4500, training loss: 2.263468027114868, validation loss: 2.293119430541992\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(max_iters):\n",
    "    # sample a batch of trainin data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    _, loss = m(xb, yb)\n",
    "    # reset parameter gradients\n",
    "    optimizer.zero_grad(set_to_none=True) \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % eval_interval == 0:\n",
    "        losses = estimate_loss(m)\n",
    "        print(f\"epoch: {epoch}, training loss: {losses['train'].item()}, validation loss: {losses['val'].item()}\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sequence:\n",
      " \n",
      "Whertent\n",
      "'t the rer tis ther maly Rovere.\n",
      "\n",
      "CSoo are prorow oul ibe comd ye hakre cod eend you, thilesfladew And thie sourlak forust\n",
      "Marce\n",
      "Anl all iqeay Os! soron ange, will ge? helll doleant gio your, I cove I aris the awithtellaverat heros hads ford ton ralexr ge sime Reck kliker tain\n",
      "Thes\n",
      "The xorr tha's?\n",
      "LURTAROLOD ELIO MAMIK:\n",
      "I Cyourr\n",
      "Aned soord\n",
      "Malfn thit yomd In he im:\n",
      "Wilirsk.\n",
      "\n",
      "Thavepo diestinhater shin Bin the\n",
      "The wis be but mallas, Ead his.\n",
      "fe lim:\n",
      "QUCferither?\n",
      "No hyoul ankimingfore, houth dis samos writ Couf co win not wighst, tall; wy In pis so pluson lxandie one my se livy!\n",
      "What awathim fe jo'lllouele hist lon,\n",
      "Hat,\n",
      "And He a in wiss out my the loue arop I shily you youldeis ter Bucy At dat\n",
      "lalat ofd.\n",
      "\n",
      "MOENBENRICHK The fay tour eise, misch presterin mow she old, a, ingsefer torse\n"
     ]
    }
   ],
   "source": [
    "# generate a single sequences using the model with start token 0\n",
    "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_seq = m.generate(idx, max_new_tokens=800)[0].tolist()\n",
    "# Decode integer tokens into characters\n",
    "generated_seq = decode(generated_seq)\n",
    "print(\"\\nGenerated sequence:\\n\",\"\".join(generated_seq))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instead of calculating the output logits directly from the attention head output, we can put a feed forward (multi-layer perceptron) layer in between the attention head and output layer. This allows us to pack in extra computations and extract more meaningful representations from the attention output. This brings us to the Transformer (Decoder) Block. Each transformer block consists of a multihead self-attemtion layer followed by a feed-forward layer. Transformer blocks are also designed to be stacked up (similar to stacked CNNs and stacked RNNs) and therefore also incorporate residual conections and layer normalization to ensure that the gradients can backpropagate without any difficulty as the stack of transformer blocks become deeper.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple mlp \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, head_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(head_size, head_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# a transformer block consisting of a multihead attention-layer followed by a feed-forward layer\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, head_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(block_size, embedding_dim, head_size, num_heads)\n",
    "        self.ff = FeedForward(head_size, num_heads)\n",
    "        \n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        x = self.sa(x)\n",
    "        x = self.ff(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language model with multiple transfop4mer blocks\n",
    "class ImprovedLanguageModelTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, embedding_dim, head_size, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_size = head_size\n",
    "        self.hum_heads = num_heads\n",
    "\n",
    "        '''\n",
    "        Define model parameters\n",
    "        '''\n",
    "        # token embedding layer \n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim) # shape: (vocab_size,C)\n",
    "        # position embedding layer\n",
    "        self.pos_embedding = nn.Embedding(block_size, embedding_dim) # shape: (T,C)\n",
    "\n",
    "        # stack of 3 transformer blocks\n",
    "        self.blocks = nn.Sequential(\n",
    "            TransformerBlock(block_size, embedding_dim, head_size, num_heads),\n",
    "            TransformerBlock(block_size, embedding_dim, head_size, num_heads),\n",
    "            TransformerBlock(block_size, embedding_dim, head_size, num_heads),\n",
    "        )\n",
    "\n",
    "        # output layer logits\n",
    "        self.lm_head = nn.Linear(head_size, vocab_size) # shape: (h,vocab_size)\n",
    "\n",
    "\n",
    "        # forward pass takes in a batch of input token sequences of shape (B,T) and corresponding targets of shape (B,T)\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T =idx.shape\n",
    "        # get token embeddings\n",
    "        token_embeds = self.token_embedding(idx) # (B,T,C)\n",
    "        # add positional encoding\n",
    "        pos_embeds = self.pos_embedding(torch.arange(T, device=device)) # (T,C) \n",
    "        x = token_embeds + pos_embeds # (B,T,C)\n",
    "        # pass through transformer blocks\n",
    "        x = self.blocks(x) # (B,T,h)\n",
    "        # compute output logits\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B,T,vocab_size = logits.shape\n",
    "            # reshape the logits and targets such that batch of input sequences are flattened into a single big input sequence\n",
    "            # i.e. (B,T) --> (B*T)\n",
    "            logits = logits.view(B*T,vocab_size) # reshaped to (B*T,vocab_size)\n",
    "            targets = targets.view(B*T) # reshaped to (B*T)\n",
    "            # compute cross entropy loss (i.e. average negative log likelihood)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "\n",
    "    # generates new sequences continuing from a given batch of context tokens\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # batch of contexts, idx has shape (B,T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # since we're using positional encoding, we need to crop idx if input sequence length exceeds block size (keep last block_size tokens)\n",
    "            idx_crop = idx[:,-block_size:] \n",
    "            # get predictions\n",
    "            logits, _ = self(idx_crop) # shape: (B,T,C)\n",
    "            # for each context sequence (in the batch), compute the probability of the next token using the logits of the last token in the context sequence\n",
    "            logits = logits[:,-1,:] # shape: (B,C)\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            # sample from the probability distribution to get next token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # shape: (B,1)\n",
    "            # append to the current context\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # shape: (B,T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "block_size = 8\n",
    "embedding_dim = 32\n",
    "head_size = 32\n",
    "num_heads = 4\n",
    "max_iters = 5000\n",
    "learning_rate = 1e-3\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "\n",
    "model = ImprovedLanguageModelTransformer(vocab_size=vocab_size, block_size=block_size, embedding_dim=embedding_dim, head_size=head_size, num_heads=num_heads)\n",
    "# move model to device\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, training loss: 4.143747806549072, validation loss: 4.144392013549805\n",
      "epoch: 500, training loss: 3.0773422718048096, validation loss: 3.0846164226531982\n",
      "epoch: 1000, training loss: 2.732046127319336, validation loss: 2.716034173965454\n",
      "epoch: 1500, training loss: 2.5512638092041016, validation loss: 2.5352306365966797\n",
      "epoch: 2000, training loss: 2.455603837966919, validation loss: 2.442411422729492\n",
      "epoch: 2500, training loss: 2.409034490585327, validation loss: 2.4058051109313965\n",
      "epoch: 3000, training loss: 2.3722264766693115, validation loss: 2.3759915828704834\n",
      "epoch: 3500, training loss: 2.341911554336548, validation loss: 2.34486722946167\n",
      "epoch: 4000, training loss: 2.3220529556274414, validation loss: 2.3295435905456543\n",
      "epoch: 4500, training loss: 2.298309326171875, validation loss: 2.3365747928619385\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(max_iters):\n",
    "    # sample a batch of trainin data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    _, loss = m(xb, yb)\n",
    "    # reset parameter gradients\n",
    "    optimizer.zero_grad(set_to_none=True) \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % eval_interval == 0:\n",
    "        losses = estimate_loss(m)\n",
    "        print(f\"epoch: {epoch}, training loss: {losses['train'].item()}, validation loss: {losses['val'].item()}\")     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that the validation loss has not decreased any further, in fact having the single multi-head attention layer seems to have been better. This because we need to add residual connections in the transformer block to improve gradient backpropagation and also include layer norms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, head_size, num_heads):\n",
    "        super().__init__()\n",
    "        assert head_size % num_heads == 0, \"head_size needs to be integer multiple of num_heads\"\n",
    "        self.heads = nn.ModuleList([Head(block_size, embedding_dim, head_size//num_heads) for _ in range(num_heads)])\n",
    "\n",
    "        # we also need to apply a linear projection to make the output residual the same dimension as the input\n",
    "        self.proj = nn.Linear(head_size, embedding_dim) \n",
    "\n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "# a simple mlp \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        # we add extra computations by growing out the feed-forward hidden size by a factor of 4\n",
    "        # we also add an extra linear layer at the end to project the residual back to same dimensions as input\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 4*embedding_dim),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*embedding_dim, embedding_dim), \n",
    "        )\n",
    "    \n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "# transformer block with residual connection \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, head_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(block_size, embedding_dim, head_size, num_heads)\n",
    "        self.ff = FeedForward(embedding_dim)\n",
    "        \n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        # residual connection between input and multi-head attention output\n",
    "        x = x + self.sa(x)\n",
    "        # residual connection between multi-head attention output and feed-forward output\n",
    "        x = x + self.ff(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, training loss: 4.703917026519775, validation loss: 4.697531700134277\n",
      "epoch: 500, training loss: 2.385143518447876, validation loss: 2.3879919052124023\n",
      "epoch: 1000, training loss: 2.242703914642334, validation loss: 2.270765781402588\n",
      "epoch: 1500, training loss: 2.184208393096924, validation loss: 2.206484317779541\n",
      "epoch: 2000, training loss: 2.1248888969421387, validation loss: 2.173213005065918\n",
      "epoch: 2500, training loss: 2.095299005508423, validation loss: 2.1506059169769287\n",
      "epoch: 3000, training loss: 2.064594030380249, validation loss: 2.1176111698150635\n",
      "epoch: 3500, training loss: 2.0370447635650635, validation loss: 2.1078944206237793\n",
      "epoch: 4000, training loss: 2.025120973587036, validation loss: 2.0856645107269287\n",
      "epoch: 4500, training loss: 2.007169723510742, validation loss: 2.0710246562957764\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "block_size = 8\n",
    "embedding_dim = 32\n",
    "head_size = 32\n",
    "num_heads = 4\n",
    "max_iters = 5000\n",
    "learning_rate = 1e-3\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "\n",
    "model = ImprovedLanguageModelTransformer(vocab_size=vocab_size, block_size=block_size, embedding_dim=embedding_dim, head_size=head_size, num_heads=num_heads)\n",
    "# move model to device\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(max_iters):\n",
    "    # sample a batch of trainin data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    _, loss = m(xb, yb)\n",
    "    # reset parameter gradients\n",
    "    optimizer.zero_grad(set_to_none=True) \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % eval_interval == 0:\n",
    "        losses = estimate_loss(m)\n",
    "        print(f\"epoch: {epoch}, training loss: {losses['train'].item()}, validation loss: {losses['val'].item()}\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sequence:\n",
      " \n",
      "\n",
      "DUCHENTBULERCENT:\n",
      "QUENEN IIZABHARD IV:\n",
      "Thereringates the want say.\n",
      "\n",
      "CORIOMENT:\n",
      "AgUS:\n",
      "The af headmen most hews, lokegise I with cartless sovon's and your, Weeith hath a it to should my look their to there him hald in Marnted ett\n",
      "gent and it real'd-woord.\n",
      "\n",
      "ICABELL:\n",
      "I wath to thy the criend fathery maid all ade nos I my lyond Yor Wow my and the the affal did fill af my earton that I'll mark you abther;\n",
      "Lecab the terefer and the frather knecce.\n",
      "\n",
      "RICHARETIO:\n",
      "Nrike Rotake a atoo.\n",
      "\n",
      "FICKINGSOUCESTES:\n",
      "Non I countlees from thane, and dess of excons, he dood 'the khes?\n",
      "For Rest mere.\n",
      "\n",
      "COMINay of sumpidiisbe mein, whell Ratioud I knot but you may let wilfnstren; if and me your death; no me of epere wor am'd thou dath with at?\n",
      "ClALAND:\n",
      "Hy, gunk most we them not weet Hech all Hore, funt my poor to twam\n"
     ]
    }
   ],
   "source": [
    "# generate a single sequences using the model with start token 0\n",
    "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_seq = m.generate(idx, max_new_tokens=800)[0].tolist()\n",
    "# Decode integer tokens into characters\n",
    "generated_seq = decode(generated_seq)\n",
    "print(\"\\nGenerated sequence:\\n\",\"\".join(generated_seq))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that adding residual conmnections inside the transformer block has significantly improved the performance. The quality of generated text has also improved dramatically.\n",
    "\n",
    "#### The final icing on the cake is Layer Normalization, which serves a similar purpose as batch normalization. Layer normalization forces the output neurons of a linear layer to all have zero mean and unit variance. This ensures that activations and gradients are more stable and well-behaved during training. We will use pre-layer norms, i.e. layer norm will be applied at the input of the multi-head attention layer and the input of the feed-forward layer. \n",
    "\n",
    "#### We can also add some dropout layers in front of the multi-head attention and feed forward layers and also to the attention weights. Then all together, we have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, head_size, dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_size = head_size\n",
    "\n",
    "        # define parameters\n",
    "        self.key = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "\n",
    "        # non-parameter tensor of lower triangular ones\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "   \n",
    "    # define forward pass, input shape: (B,T,C) where B=batch size, T=block_size, C=embedding_dim\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B,T,h) where h is the head_size\n",
    "        q = self.query(x) # (B,T,h)\n",
    "        v = self.value(x) # (B,T,h)\n",
    "        W = q @ k.transpose(-2,-1)  * self.head_size**(-0.5) # (B,T,T)\n",
    "        W = W.masked_fill(self.tril[:T, :T] == 0, float('-inf')) \n",
    "        W = F.softmax(W, dim=-1)\n",
    "        # apply dropout to attention weights\n",
    "        W = self.dropout(W)\n",
    "        out = W @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, head_size, num_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "        assert head_size % num_heads == 0, \"head_size needs to be integer multiple of num_heads\"\n",
    "        self.heads = nn.ModuleList([Head(block_size, embedding_dim, head_size//num_heads, dropout_rate) for _ in range(num_heads)])\n",
    "\n",
    "        # we also need to apply a linear projection to make the output residual the same dimension as the input\n",
    "        self.proj = nn.Linear(head_size, embedding_dim) \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        x = self.proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "# a simple mlp \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "        # we add extra computations by growing out the feed-forward hidden size by a factor of 4\n",
    "        # we also add an extra linear layer at the end to project the residual back to same dimensions as input\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 4*embedding_dim),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*embedding_dim, embedding_dim), \n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "    \n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "# transformer block with residual connection and layer norm\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, head_size, num_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(block_size, embedding_dim, head_size, num_heads, dropout_rate) # multi-head attention layer \n",
    "        self.ff = FeedForward(embedding_dim, dropout_rate)   # feed-forward layer\n",
    "        self.ln1 = nn.LayerNorm(embedding_dim) # layer norm at input of multi-head attention\n",
    "        self.ln2 = nn.LayerNorm(embedding_dim) # layer norm at input of feed-forward\n",
    "\n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        # residual connection between input and multi-head attention output\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        # residual connection between multi-head attention output and feed-forward output\n",
    "        x = x + self.ff(self.ln2(x)) \n",
    "        return x\n",
    "    \n",
    "\n",
    "# language model with multiple transformer blocks\n",
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, embedding_dim, head_size, num_heads, num_blocks, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_size = head_size\n",
    "        self.hum_heads = num_heads\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        '''\n",
    "        Define model parameters\n",
    "        '''\n",
    "        # token embedding layer \n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim) # shape: (vocab_size,C)\n",
    "        # position embedding layer\n",
    "        self.pos_embedding = nn.Embedding(block_size, embedding_dim) # shape: (T,C)\n",
    "\n",
    "        # stack of transformer blocks\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(block_size, embedding_dim, head_size, num_heads, dropout_rate) for _ in range(num_blocks)])\n",
    "\n",
    "        # we also add a layer norm before the final output layer\n",
    "        self.ln_f = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        # output layer logits\n",
    "        self.lm_head = nn.Linear(head_size, vocab_size) # shape: (h,vocab_size)\n",
    "\n",
    "\n",
    "        # forward pass takes in a batch of input token sequences of shape (B,T) and corresponding targets of shape (B,T)\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T =idx.shape\n",
    "        # get token embeddings\n",
    "        token_embeds = self.token_embedding(idx) # (B,T,C)\n",
    "        # add positional encoding\n",
    "        pos_embeds = self.pos_embedding(torch.arange(T, device=device)) # (T,C) \n",
    "        x = token_embeds + pos_embeds # (B,T,C)\n",
    "        # pass through transformer blocks\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        # apply layer norm\n",
    "        x = self.ln_f(x)  # (B,T,C)\n",
    "        # compute output logits \n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B,T,vocab_size = logits.shape\n",
    "            # reshape the logits and targets such that batch of input sequences are flattened into a single big input sequence\n",
    "            # i.e. (B,T) --> (B*T)\n",
    "            logits = logits.view(B*T,vocab_size) # reshaped to (B*T,vocab_size)\n",
    "            targets = targets.view(B*T) # reshaped to (B*T)\n",
    "            # compute cross entropy loss (i.e. average negative log likelihood)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "\n",
    "    # generates new sequences continuing from a given batch of context tokens\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # batch of contexts, idx has shape (B,T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # since we're using positional encoding, we need to crop idx if input sequence length exceeds block size (keep last block_size tokens)\n",
    "            idx_crop = idx[:,-block_size:] \n",
    "            # get predictions\n",
    "            logits, _ = self(idx_crop) # shape: (B,T,C)\n",
    "            # for each context sequence (in the batch), compute the probability of the next token using the logits of the last token in the context sequence\n",
    "            logits = logits[:,-1,:] # shape: (B,C)\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            # sample from the probability distribution to get next token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # shape: (B,1)\n",
    "            # append to the current context\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # shape: (B,T+1)\n",
    "        return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with the improved transformer block which has both residual connections and pre-layer norms and scaling up the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 64\n",
    "block_size = 256\n",
    "embedding_dim = 384\n",
    "head_size = embedding_dim\n",
    "num_heads = 6\n",
    "num_blocks = 6\n",
    "dropout_rate = 0.2\n",
    "max_iters = 5000\n",
    "learning_rate = 5e-4\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "\n",
    "model = TransformerLanguageModel(vocab_size=vocab_size, block_size=block_size, embedding_dim=embedding_dim, head_size=head_size, num_heads=num_heads, num_blocks=num_blocks, dropout_rate=dropout_rate)\n",
    "# move model to device\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 10.788929 M\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in m.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5000, Train Loss: 0.984, Val Loss: 1.500: 100%|| 5000/5000 [27:27<00:00,  3.76it/s]   \n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "train_loss = None\n",
    "val_loss = None\n",
    "\n",
    "pbar = tqdm(range(max_iters), desc=\"Epochs\")\n",
    "for epoch in pbar:\n",
    "    # sample a batch of trainin data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    _, loss = m(xb, yb)\n",
    "    # reset parameter gradients\n",
    "    optimizer.zero_grad(set_to_none=True) \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % eval_interval == 0:\n",
    "        losses = estimate_loss(m)\n",
    "        #print(f\"epoch: {epoch}, training loss: {losses['train'].item()}, validation loss: {losses['val'].item()}\")     \n",
    "        train_loss = losses['train'].item()\n",
    "        val_loss = losses['val'].item()\n",
    "\n",
    "    pbar.set_description(\n",
    "        f\"Epoch {epoch + 1}, Train Loss: {train_loss:.3f}, Val Loss: {val_loss:.3f}\"\n",
    "    )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sequence:\n",
      " \n",
      "That have his act much tongue\n",
      "And hurlined upon their own desires,\n",
      "Are how my prince be sweeted, now strongly are\n",
      "As musters, and have met our attempted body,\n",
      "Been witness to thy husband's growthoods;\n",
      "But of their deaths, province should be plaing on agon,\n",
      "Put for cordserves would be consul to say.\n",
      "I'll afide you tell your true hearts' torms!\n",
      "I'll not swift.\n",
      "\n",
      "HASTINGS:\n",
      "I would advise the man to cheque,\n",
      "I to do that were sense thou suspectures?\n",
      "Like may be a sweet son in earth; devery wedding;\n",
      "For away, I sing, a potson! Your which that's next\n",
      "With immitted with musiciate be broke, a sea,\n",
      "And thousan Montague: before so quick to his holy\n",
      "Assoleme. A pluck on my look friend, and it\n",
      "Is good a tent, or as a whoo's entertain, I\n",
      "knowing both forget, which I show'd it.\n",
      "\n",
      "LEONTES:\n",
      "\n",
      "OLANTES:\n",
      "Ha! why shall I will have herefore the hour\n",
      "To wash here stands, whispering heaven stretch'd at the Lord Stanles\n",
      "To go but with me: alas! I pluck the love\n",
      "That man I am commanded. Provided, perhaps,\n",
      "Come I to starce and na farm of hate, was I can deserved.\n",
      "\n",
      "JULIET:\n",
      "But let me off the heart is bound, shake haste.\n",
      "\n",
      "DUKE OF BURSET:\n",
      "Poor prey, visite heart's death, while we bound.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "Clifford! that we give meets' me revenge,\n",
      "I am as crease, not for so much tith another,\n",
      "A horse! the bald meleiance may arm\n",
      "Stand out of our conveyancement.\n",
      "\n",
      "Boy:\n",
      "This man being a cotdarling care? Do hear my war\n",
      "Is no such issue. Take your uncle York; a wold;\n",
      "Good brother; for if the Vouchsan is alive,\n",
      "He mayst not, but that late have too do more\n",
      "Than by country's life and my dage, my dostresses,\n",
      "For whiles appell'd with the blood of your brother's heart;\n",
      "To make haste, to mistake ashorous time:\n",
      "And yet till use wise it escapasion, and scratch'd,\n",
      "To opalitia times mad, makes:\n",
      "'Will not brine it not, my life, I am arrigon\n",
      "Might still have me to keep. We'll yok on of a garment.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "O plain traitor Norfolk, Tyrrel, ho!\n",
      "Will it made to rest, Petruchio, thou take the vain,\n",
      "As now that with t\n"
     ]
    }
   ],
   "source": [
    "# generate a single sequences using the model with start token 0\n",
    "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_seq = m.generate(idx, max_new_tokens=2000)[0].tolist()\n",
    "# Decode integer tokens into characters\n",
    "generated_seq = decode(generated_seq)\n",
    "print(\"\\nGenerated sequence:\\n\",\"\".join(generated_seq))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
