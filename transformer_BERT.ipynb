{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "import psutil\n",
    "import sys\n",
    "from itertools import islice\n",
    "\n",
    "# use GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A simple/minimal implementation of the BERT model (https://arxiv.org/pdf/1810.04805v2.pdf). \n",
    "\n",
    "The transformer block and multihead attention layer implementations are based on the Andrej Karpathy GPT youtube tutorial. In this case, we use a transformer encoder block which uses bi-directional context, differing from the transformer decoder in GPT which is unidirectional (achieved via causal masking of attention weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, total_head_size, num_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        assert total_head_size % num_heads == 0, \"head_size needs to be integer multiple of num_heads\"\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.total_head_size = total_head_size \n",
    "        self.head_size = total_head_size // num_heads \n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # define parameters\n",
    "        self.key = nn.Linear(embedding_dim, self.total_head_size, bias=False)\n",
    "        self.query = nn.Linear(embedding_dim, self.total_head_size, bias=False)\n",
    "        self.value = nn.Linear(embedding_dim, self.total_head_size, bias=False)\n",
    "        self.attn_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # non-parameter tensor of lower triangular ones\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        # we also need to apply a linear projection to make the output residual the same dimension as the input\n",
    "        self.proj = nn.Linear(total_head_size, embedding_dim) \n",
    "        self.output_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "\n",
    "    # define forward pass, input shape: (B,T,C) where B=batch size, T=block_size, C=embedding_dim\n",
    "    # the attn_mask is a mask that can be used for masking out the attention weights for padding tokens \n",
    "    def forward(self, x, attn_mask):\n",
    "        B, T, C = x.shape\n",
    "        #print(f\"B = {B}, T={T}, C={C}\")\n",
    "        k = self.key(x) # (B,T,H) where H is the total_head_size\n",
    "        q = self.query(x) # (B,T,H)\n",
    "        v = self.value(x) # (B,T,H)\n",
    "\n",
    "        # reshape (B,T,H) --> (B,T,n,h), where n=num_heads and h=head_size and H=n*h\n",
    "        k = k.view(B,T,self.num_heads,self.head_size) \n",
    "        q = q.view(B,T,self.num_heads,self.head_size) \n",
    "        v = v.view(B,T,self.num_heads,self.head_size) \n",
    "\n",
    "        # now we transpose so that the num_heads is the second dimension followed by T,h\n",
    "        # this allows us to batch matrix mutliply for all heads simulataneously to compute their attention weights\n",
    "        # (B,T,n,h) --> (B,n,T,h) \n",
    "        k = k.transpose(1,2) \n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "\n",
    "        # compute attention scores manually (slower)\n",
    "        W = q @ k.transpose(-2,-1)  / math.sqrt(self.head_size) # (B,n,T,T)\n",
    "        attn_mask = attn_mask.unsqueeze(1).unsqueeze(1)        \n",
    "        #print(f\"W shape= {W.shape}, attn_mask shape = {attn_mask.shape}\")\n",
    "        W = W.masked_fill(attn_mask == 0, float('-inf')) \n",
    "        W = F.softmax(W, dim=-1)\n",
    "        # apply dropout to attention weights\n",
    "        W = self.attn_dropout(W)\n",
    "        out = W @ v # (B,n,T,h)\n",
    "        \n",
    "\n",
    "        # use pytorch built-in function for faster computation of attention scores (set the 'is_causal' parameter for applying causal masking)\n",
    "        #out = F.scaled_dot_product_attention(q,k,v,attn_mask=attn_mask.bool(),dropout_p=self.dropout_rate if self.training else 0,is_causal=False)\n",
    "\n",
    "        # we can transpose the output from (B,n,T,h) --> (B,T,n,h)\n",
    "        # since the last two dimensions of the transposed tensor are non-contiguous, we apply \n",
    "        # contiguous() which return a contiguous tensor\n",
    "        out = out.transpose(1,2).contiguous()\n",
    "\n",
    "        # finally we collapse the last two dimensions to get the concatenated output, (B,T,n,h) --> (B,T,n*h) \n",
    "        out = out.view(B,T,self.total_head_size)\n",
    "\n",
    "        # now we project the concatenated output so that it has the same dimensions as the multihead attention layer input\n",
    "        # (we need to add it with the input because of the residual connection, so need to be same size) \n",
    "        out = self.proj(out) # (B,T,C) \n",
    "\n",
    "        # apply dropout\n",
    "        out = self.output_dropout(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "# a simple mlp \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "        # we add extra computations by growing out the feed-forward hidden size by a factor of 4\n",
    "        # we also add an extra linear layer at the end to project the residual back to same dimensions as input\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 4*embedding_dim),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*embedding_dim, embedding_dim), \n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "    \n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "# transformer encoder block with residual connection and layer norm\n",
    "# Note: the original transformer uses post layer norms, here we use pre layer norms, i.e. layer norm is applied at the input\n",
    "# instead of the output, this typically leads to better results in terms of training convergence speed and gradient scaling \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, head_size, num_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(block_size, embedding_dim, head_size, num_heads, dropout_rate) # multi-head attention layer \n",
    "        self.ff = FeedForward(embedding_dim, dropout_rate)   # feed-forward layer\n",
    "        self.ln1 = nn.LayerNorm(embedding_dim) # layer norm at input of multi-head attention\n",
    "        self.ln2 = nn.LayerNorm(embedding_dim) # layer norm at input of feed-forward\n",
    "\n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x, attn_mask):\n",
    "        # residual connection between input and multi-head attention output (also note that we're doing a pre-layer norm, i.e. layer norm at the input of the multi-head attention)\n",
    "        x = x + self.sa(self.ln1(x), attn_mask)\n",
    "        # residual connection between multi-head attention output and feed-forward output (also note that we're doing a pre-layer norm, i.e. layer norm at the input of the feed-forward)\n",
    "        x = x + self.ff(self.ln2(x)) \n",
    "        return x\n",
    "    \n",
    "\n",
    "# BERT model with multiple transformer blocks \n",
    "class BERTModel(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, embedding_dim, head_size, num_heads, num_blocks, pad_token_id, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size        # block_size is just the input sequence length\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_size = head_size\n",
    "        self.hum_heads = num_heads\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        '''\n",
    "        Define model parameters\n",
    "        '''\n",
    "        # token embedding layer \n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_token_id) # shape: (vocab_size,C)\n",
    "        # position embedding layer\n",
    "        self.pos_embedding = nn.Embedding(block_size, embedding_dim) # shape: (T,C)\n",
    "        # segment embedding layer (disabled for now)\n",
    "        #self.segment_embedding = nn.Embedding(2, embedding_dim)\n",
    "\n",
    "        # stack of transformer blocks\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(block_size, embedding_dim, head_size, num_heads, dropout_rate) for _ in range(num_blocks)])\n",
    "\n",
    "        # pooling transformation of CLS token (for downstream tasks requiring full sentence hidden representation)\n",
    "        #self.pooling_linear = nn.Linear(embedding_dim, embedding_dim) # shape: (C,C)\n",
    "        #self.pooling_activation_fn = nn.Tanh()\n",
    "\n",
    "        # output layer\n",
    "        self.ln = nn.LayerNorm(embedding_dim)\n",
    "        self.output_linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "\n",
    "        # store position indices inside a buffer for fast access when computing position embeddings\n",
    "        position_idx = torch.arange(block_size, device=device).unsqueeze(0)\n",
    "        self.register_buffer('position_idx', position_idx)\n",
    "\n",
    "\n",
    "        # forward pass takes in a batch of input token sequences idx of shape (B,T) and corresponding targets of shape (B,T)\n",
    "    def forward(self, idx, attn_mask, targets=None, segment_idx=None):\n",
    "        B, T = idx.shape\n",
    "        # get token embeddings\n",
    "        token_embeds = self.token_embedding(idx) # (B,T,C)\n",
    "        # add positional encoding\n",
    "        pos_embeds = self.pos_embedding(self.position_idx[:,:T]) # (T,C) \n",
    "        \n",
    "        # add sentence segment embedding (disabled for now)\n",
    "        # segment_embeds = self.segment_embedding(segment_idx) # segment_idx is an integer tensor of shape (B,T) and has 0's at positions corresponding to \n",
    "        \n",
    "        # the first sentence and 1's at positions corresponding to the second sentence \n",
    "        x = token_embeds + pos_embeds # (B,T,C)\n",
    "        # pass through transformer blocks to get encoding\n",
    "        for block in self.blocks:\n",
    "            x = block(x, attn_mask) # (B,T,C)\n",
    "    \n",
    "        # get CLS token encoding and apply pooling transform\n",
    "        #cls_encoding = x[:,0] # (B,C)\n",
    "        #pooled_cls_encoding = self.pooling_activation_fn(self.pooling_linear(cls_encoding)) # (B,C)\n",
    "\n",
    "        # apply final layers norm\n",
    "        x = self.ln(x)\n",
    "\n",
    "        # compute output logits\n",
    "        logits = self.output_linear(self.dropout(x))\n",
    "\n",
    "        return logits \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use the WordPiece Tokenizer with the vocabulary used for pre-training the original BERT (https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['freaking',\n",
       " 'evenly',\n",
       " '##গ',\n",
       " 'internacional',\n",
       " 'inference',\n",
       " '##ious',\n",
       " 'yuan',\n",
       " 'atoll',\n",
       " 'compilation',\n",
       " 'cmll',\n",
       " 'ろ',\n",
       " '##morphic',\n",
       " 'trillion',\n",
       " 'squire',\n",
       " '##岡',\n",
       " 'irs',\n",
       " 'denote',\n",
       " '##rigues',\n",
       " 'comedian',\n",
       " '##shu']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will use the uncased wordpiece tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# show some words from the wordpiece vocabulary\n",
    "list(tokenizer.vocab.keys())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokenizer)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# load the books corpus dataset from file\n",
    "dataset = Dataset.from_file('book_corpus_dataset/archive/train/dataset.arrow')\n",
    "\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "# check how many sentences are in the dataset\n",
    "print(len(dataset))\n",
    "\n",
    "# show some sentences from the dataset\n",
    "print(dataset[:10]['text'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For simplicity, we will work with a small subset of the full corpus (the first 10M sentences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.05016326904297"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# get chunk of 10M sentences\n",
    "chunk = dataset[:10000000]['text']\n",
    "\n",
    "# size in Mb\n",
    "sys.getsizeof(chunk)/1024**2\n",
    "\n",
    "# save it to a txt file\n",
    "with open('bookcorpus_small.txt', 'w') as output:\n",
    "    for sent in chunk:\n",
    "        output.write(f\"{sent}\\n\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_first_n_lines(n):\n",
    "    with open(\"bookcorpus_small.txt\", 'r') as file:\n",
    "        return list(islice(file, n))\n",
    "    \n",
    "# read in 100000 sentences from the txt file\n",
    "text = read_first_n_lines(100000)\n",
    "text = [line.strip() for line in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in reduced corpus: 100000\n",
      "RAM used: 475.09 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of sentences in reduced corpus: {len(text)}\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordPiece Tokennizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# pretokenize the corpus into words and get unigram counts\n",
    "word_freqs = defaultdict(int)\n",
    "\n",
    "for s in text:\n",
    "    words = s.split()\n",
    "    for word in words:\n",
    "        word_freqs[word] += 1\n",
    "    break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'the': 2,\n",
       "             'half-ling': 1,\n",
       "             'book': 1,\n",
       "             'one': 1,\n",
       "             'in': 1,\n",
       "             'fall': 1,\n",
       "             'of': 1,\n",
       "             'igneeria': 1,\n",
       "             'series': 1,\n",
       "             'kaylee': 2,\n",
       "             'soderburg': 2,\n",
       "             'copyright': 1,\n",
       "             '2013': 1,\n",
       "             'all': 1,\n",
       "             'rights': 1,\n",
       "             'reserved': 1,\n",
       "             '.': 1})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we initialize the WordPiece vocabulary. This contains all the unique first letters of every word in the corpus and all other letters that appear in words prefixed by '##'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['##-', '##0', '##1', '##3', '##a', '##b', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##y', '.', '2', '[CLS]', '[MASK]', '[PAD]', '[SEP]', '[UNK]', 'a', 'b', 'c', 'f', 'h', 'i', 'k', 'o', 'r', 's', 't']\n"
     ]
    }
   ],
   "source": [
    "# initialize WordPiece vocabulary\n",
    "vocab = []\n",
    "for word in word_freqs.keys():\n",
    "    if word[0] not in vocab:\n",
    "        vocab.append(word[0])\n",
    "    for letter in word[1:]:\n",
    "        prefixed = '##' + letter\n",
    "        if prefixed not in vocab:\n",
    "            vocab.append(prefixed)\n",
    "\n",
    "# now add the special tokens\n",
    "vocab = vocab + [\"[PAD]\", \"[CLS]\", \"[UNK]\", \"[MASK]\", \"[SEP]\"]\n",
    "\n",
    "vocab = sorted(vocab)         \n",
    "print(vocab) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split every unique word in corpus into characters and prefix the characters which are not the first with '##'\\\n",
    "e.g. 'word' --> 'w', '##o', '##r', '##d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': ['t', '##h', '##e'],\n",
       " 'half-ling': ['h', '##a', '##l', '##f', '##-', '##l', '##i', '##n', '##g'],\n",
       " 'book': ['b', '##o', '##o', '##k'],\n",
       " 'one': ['o', '##n', '##e'],\n",
       " 'in': ['i', '##n'],\n",
       " 'fall': ['f', '##a', '##l', '##l'],\n",
       " 'of': ['o', '##f'],\n",
       " 'igneeria': ['i', '##g', '##n', '##e', '##e', '##r', '##i', '##a'],\n",
       " 'series': ['s', '##e', '##r', '##i', '##e', '##s'],\n",
       " 'kaylee': ['k', '##a', '##y', '##l', '##e', '##e'],\n",
       " 'soderburg': ['s', '##o', '##d', '##e', '##r', '##b', '##u', '##r', '##g'],\n",
       " 'copyright': ['c', '##o', '##p', '##y', '##r', '##i', '##g', '##h', '##t'],\n",
       " '2013': ['2', '##0', '##1', '##3'],\n",
       " 'all': ['a', '##l', '##l'],\n",
       " 'rights': ['r', '##i', '##g', '##h', '##t', '##s'],\n",
       " 'reserved': ['r', '##e', '##s', '##e', '##r', '##v', '##e', '##d'],\n",
       " '.': ['.']}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = {word: [c if i==0 else f\"##{c}\" for i,c in enumerate(word)] for word in word_freqs.keys()}\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will iteratively merge these splits into subword tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
