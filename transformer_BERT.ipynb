{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import psutil\n",
    "import sys\n",
    "from itertools import islice\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import unicodedata\n",
    "from multiprocess import Pool\n",
    "import random \n",
    "random.seed(1234)\n",
    "\n",
    "# use GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A simple/minimal implementation of the BERT model (https://arxiv.org/pdf/1810.04805v2.pdf) \n",
    "\n",
    "##### The transformer block and multihead attention layer implementations are based on the Andrej Karpathy GPT youtube tutorial. In this case, we use a transformer encoder block which uses bi-directional context, differing from the transformer decoder in GPT which is unidirectional (achieved via causal masking of attention weights).\n",
    "\n",
    "#### Pre-Training:\n",
    "\n",
    "##### We will train our BERT model on the masked language modeling (MLM) task. The MLM task involves masking out parts of the input sequence and having the model reconstruct those missing parts. By pre-training the model on this task using a large corpus, it learns a strong representation of language (e.g. it learns syntax structure, gains knowledge about the world and different entities, word semantics and sentiment) which can then be trasferred into many different downstream language tasks with some additional finetuning. The learning process is made even more robust by masking out randomly selected tokens from the input sequence which don't necessarily have to be in a contiguous chunk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, total_head_size, num_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        assert total_head_size % num_heads == 0, \"head_size needs to be integer multiple of num_heads\"\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.total_head_size = total_head_size \n",
    "        self.head_size = total_head_size // num_heads \n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # define parameters\n",
    "        self.key = nn.Linear(embedding_dim, self.total_head_size, bias=False)\n",
    "        self.query = nn.Linear(embedding_dim, self.total_head_size, bias=False)\n",
    "        self.value = nn.Linear(embedding_dim, self.total_head_size, bias=False)\n",
    "        self.attn_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # we also need to apply a linear projection to make the output residual the same dimension as the input\n",
    "        self.proj = nn.Linear(total_head_size, embedding_dim) \n",
    "        self.output_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "\n",
    "    # define forward pass, input shape: (B,T,C) where B=batch size, T=block_size, C=embedding_dim\n",
    "    # the attn_mask is a mask that can be used for masking out the attention weights for padding tokens \n",
    "    def forward(self, x, attn_mask):\n",
    "        B, T, C = x.shape\n",
    "        #print(f\"B = {B}, T={T}, C={C}\")\n",
    "        k = self.key(x) # (B,T,H) where H is the total_head_size\n",
    "        q = self.query(x) # (B,T,H)\n",
    "        v = self.value(x) # (B,T,H)\n",
    "\n",
    "        # reshape (B,T,H) --> (B,T,n,h), where n=num_heads and h=head_size and H=n*h\n",
    "        k = k.view(B,T,self.num_heads,self.head_size) \n",
    "        q = q.view(B,T,self.num_heads,self.head_size) \n",
    "        v = v.view(B,T,self.num_heads,self.head_size) \n",
    "\n",
    "        # now we transpose so that the num_heads is the second dimension followed by T,h\n",
    "        # this allows us to batch matrix mutliply for all heads simulataneously to compute their attention weights\n",
    "        # (B,T,n,h) --> (B,n,T,h) \n",
    "        k = k.transpose(1,2) \n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "\n",
    "        # compute attention scores manually (slower)\n",
    "        W = q @ k.transpose(-2,-1)  / math.sqrt(self.head_size) # (B,n,T,T)\n",
    "        attn_mask = attn_mask.view(B,1,1,T)        \n",
    "        #print(f\"W shape= {W.shape}, attn_mask shape = {attn_mask.shape}\")\n",
    "        W = W.masked_fill(attn_mask == 0, float('-inf')) \n",
    "        W = F.softmax(W, dim=-1)\n",
    "        # apply dropout to attention weights\n",
    "        W = self.attn_dropout(W)\n",
    "        out = W @ v # (B,n,T,h)\n",
    "        \n",
    "\n",
    "        # use pytorch built-in function for faster computation of attention scores (set the 'is_causal' parameter for applying causal masking)\n",
    "        #out = F.scaled_dot_product_attention(q,k,v,attn_mask=attn_mask.bool(),dropout_p=self.dropout_rate if self.training else 0,is_causal=False)\n",
    "\n",
    "        # we can transpose the output from (B,n,T,h) --> (B,T,n,h)\n",
    "        # since the last two dimensions of the transposed tensor are non-contiguous, we apply \n",
    "        # contiguous() which return a contiguous tensor\n",
    "        out = out.transpose(1,2).contiguous()\n",
    "\n",
    "        # finally we collapse the last two dimensions to get the concatenated output, (B,T,n,h) --> (B,T,n*h) \n",
    "        out = out.view(B,T,self.total_head_size)\n",
    "\n",
    "        # now we project the concatenated output so that it has the same dimensions as the multihead attention layer input\n",
    "        # (we need to add it with the input because of the residual connection, so need to be same size) \n",
    "        out = self.proj(out) # (B,T,C) \n",
    "\n",
    "        # apply dropout\n",
    "        out = self.output_dropout(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "# a simple mlp \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "        # we add extra computations by growing out the feed-forward hidden size by a factor of 4\n",
    "        # we also add an extra linear layer at the end to project the residual back to same dimensions as input\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 4*embedding_dim),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*embedding_dim, embedding_dim), \n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "    \n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "# transformer encoder block with residual connection and layer norm\n",
    "# Note: the original transformer uses post layer norms, here we use pre layer norms, i.e. layer norm is applied at the input\n",
    "# instead of the output, this typically leads to better results in terms of training convergence speed and gradient scaling \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, head_size, num_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(block_size, embedding_dim, head_size, num_heads, dropout_rate) # multi-head attention layer \n",
    "        self.ff = FeedForward(embedding_dim, dropout_rate)   # feed-forward layer\n",
    "        self.ln1 = nn.LayerNorm(embedding_dim) # layer norm at input of multi-head attention\n",
    "        self.ln2 = nn.LayerNorm(embedding_dim) # layer norm at input of feed-forward\n",
    "\n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x, attn_mask):\n",
    "        # residual connection between input and multi-head attention output (also note that we're doing a pre-layer norm, i.e. layer norm at the input of the multi-head attention)\n",
    "        x = x + self.sa(self.ln1(x), attn_mask)\n",
    "        # residual connection between multi-head attention output and feed-forward output (also note that we're doing a pre-layer norm, i.e. layer norm at the input of the feed-forward)\n",
    "        x = x + self.ff(self.ln2(x)) \n",
    "        return x\n",
    "    \n",
    "\n",
    "# BERT model with multiple transformer blocks \n",
    "class BERTModel(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, embedding_dim, head_size, num_heads, num_blocks, pad_token_id, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size        # block_size is just the input sequence length\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_size = head_size\n",
    "        self.hum_heads = num_heads\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        '''\n",
    "        Define model parameters\n",
    "        '''\n",
    "        # token embedding layer \n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_token_id) # shape: (vocab_size,C)\n",
    "        # position embedding layer\n",
    "        self.pos_embedding = nn.Embedding(block_size, embedding_dim) # shape: (T,C)\n",
    "        # segment embedding layer (disabled for now)\n",
    "        #self.segment_embedding = nn.Embedding(2, embedding_dim)\n",
    "\n",
    "        # stack of transformer blocks\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(block_size, embedding_dim, head_size, num_heads, dropout_rate) for _ in range(num_blocks)])\n",
    "\n",
    "        # pooling transformation of CLS token (for downstream tasks requiring full sentence hidden representation)\n",
    "        #self.pooling_linear = nn.Linear(embedding_dim, embedding_dim) # shape: (C,C)\n",
    "        #self.pooling_activation_fn = nn.Tanh()\n",
    "\n",
    "        # output layer\n",
    "        self.ln = nn.LayerNorm(embedding_dim)\n",
    "        self.output_linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "\n",
    "        # store position indices inside a buffer for fast access when computing position embeddings\n",
    "        position_idx = torch.arange(block_size, device=device).unsqueeze(0)\n",
    "        self.register_buffer('position_idx', position_idx)\n",
    "\n",
    "\n",
    "        # forward pass takes in a batch of input token sequences idx of shape (B,T) and corresponding targets of shape (B,T)\n",
    "    def forward(self, idx, attn_mask, segment_idx=None):\n",
    "        B, T = idx.shape\n",
    "        # get token embeddings\n",
    "        token_embeds = self.token_embedding(idx) # (B,T,C)\n",
    "        # add positional encoding\n",
    "        pos_embeds = self.pos_embedding(self.position_idx[:,:T]) # (T,C) \n",
    "        \n",
    "        # add sentence segment embedding (disabled for now)\n",
    "        # segment_embeds = self.segment_embedding(segment_idx) # segment_idx is an integer tensor of shape (B,T) and has 0's at positions corresponding to \n",
    "        \n",
    "        # the first sentence and 1's at positions corresponding to the second sentence \n",
    "        x = token_embeds + pos_embeds # (B,T,C)\n",
    "        # pass through transformer blocks to get encoding\n",
    "        for block in self.blocks:\n",
    "            x = block(x, attn_mask) # (B,T,C)\n",
    "    \n",
    "        # get CLS token encoding and apply pooling transform\n",
    "        #cls_encoding = x[:,0] # (B,C)\n",
    "        #pooled_cls_encoding = self.pooling_activation_fn(self.pooling_linear(cls_encoding)) # (B,C)\n",
    "\n",
    "        # apply final layers norm\n",
    "        x = self.ln(x)\n",
    "\n",
    "        # compute output logits\n",
    "        logits = self.output_linear(self.dropout(x))\n",
    "\n",
    "        return logits \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use the WordPiece Tokenizer with the vocabulary used for pre-training the original BERT (https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# load the books corpus dataset from file\n",
    "dataset = Dataset.from_file('book_corpus_dataset/archive/train/dataset.arrow')\n",
    "\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "# check how many sentences are in the dataset\n",
    "print(len(dataset))\n",
    "\n",
    "# show some sentences from the dataset\n",
    "print(dataset[:10]['text'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For simplicity, we will work with a small subset of the full corpus (the first 10M sentences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.05016326904297"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# get chunk of 10M sentences\n",
    "chunk = dataset[:10000000]['text']\n",
    "\n",
    "# size in Mb\n",
    "sys.getsizeof(chunk)/1024**2\n",
    "\n",
    "# save it to a txt file\n",
    "with open('bookcorpus_small.txt', 'w') as output:\n",
    "    for sent in chunk:\n",
    "        output.write(f\"{sent}\\n\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in reduced corpus: 1000000\n"
     ]
    }
   ],
   "source": [
    "def read_first_n_lines(n):\n",
    "    with open(\"bookcorpus_small.txt\", 'r') as file:\n",
    "        return list(islice(file, n))\n",
    "    \n",
    "# read in 1000000 sentences from the txt file\n",
    "text = read_first_n_lines(1000000)\n",
    "text = [line.strip() for line in text]\n",
    "print(f\"Number of sentences in reduced corpus: {len(text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 529.01 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordPiece Tokenizer Algorithm Implementation (https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretokenize the corpus into words and get unigram counts, we will only use the first sentence as an example\n",
    "word_freqs = defaultdict(int)\n",
    "for s in text:\n",
    "    words = s.split()\n",
    "    for word in words:\n",
    "        word_freqs[word] += 1\n",
    "    break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'the': 2,\n",
       "             'half-ling': 1,\n",
       "             'book': 1,\n",
       "             'one': 1,\n",
       "             'in': 1,\n",
       "             'fall': 1,\n",
       "             'of': 1,\n",
       "             'igneeria': 1,\n",
       "             'series': 1,\n",
       "             'kaylee': 2,\n",
       "             'soderburg': 2,\n",
       "             'copyright': 1,\n",
       "             '2013': 1,\n",
       "             'all': 1,\n",
       "             'rights': 1,\n",
       "             'reserved': 1,\n",
       "             '.': 1})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we initialize the WordPiece vocabulary. This contains all the unique first letters of every word in the corpus and all other letters that appear in words prefixed by '##'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['##-', '##0', '##1', '##3', '##a', '##b', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##y', '.', '2', '[CLS]', '[MASK]', '[PAD]', '[SEP]', '[UNK]', 'a', 'b', 'c', 'f', 'h', 'i', 'k', 'o', 'r', 's', 't']\n"
     ]
    }
   ],
   "source": [
    "# initialize WordPiece vocabulary\n",
    "vocab = []\n",
    "for word in word_freqs.keys():\n",
    "    if word[0] not in vocab:\n",
    "        vocab.append(word[0])\n",
    "    for letter in word[1:]:\n",
    "        prefixed = '##' + letter\n",
    "        if prefixed not in vocab:\n",
    "            vocab.append(prefixed)\n",
    "\n",
    "# now add the special tokens\n",
    "vocab = vocab + [\"[PAD]\", \"[CLS]\", \"[UNK]\", \"[MASK]\", \"[SEP]\"]\n",
    "\n",
    "vocab = sorted(vocab)         \n",
    "print(vocab) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split every unique word in corpus into characters and prefix the characters which are not the first with '##'\\\n",
    "e.g. 'word' --> 'w', '##o', '##r', '##d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': ['t', '##h', '##e'],\n",
       " 'half-ling': ['h', '##a', '##l', '##f', '##-', '##l', '##i', '##n', '##g'],\n",
       " 'book': ['b', '##o', '##o', '##k'],\n",
       " 'one': ['o', '##n', '##e'],\n",
       " 'in': ['i', '##n'],\n",
       " 'fall': ['f', '##a', '##l', '##l'],\n",
       " 'of': ['o', '##f'],\n",
       " 'igneeria': ['i', '##g', '##n', '##e', '##e', '##r', '##i', '##a'],\n",
       " 'series': ['s', '##e', '##r', '##i', '##e', '##s'],\n",
       " 'kaylee': ['k', '##a', '##y', '##l', '##e', '##e'],\n",
       " 'soderburg': ['s', '##o', '##d', '##e', '##r', '##b', '##u', '##r', '##g'],\n",
       " 'copyright': ['c', '##o', '##p', '##y', '##r', '##i', '##g', '##h', '##t'],\n",
       " '2013': ['2', '##0', '##1', '##3'],\n",
       " 'all': ['a', '##l', '##l'],\n",
       " 'rights': ['r', '##i', '##g', '##h', '##t', '##s'],\n",
       " 'reserved': ['r', '##e', '##s', '##e', '##r', '##v', '##e', '##d'],\n",
       " '.': ['.']}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = {word: [c if i==0 else f\"##{c}\" for i,c in enumerate(word)] for word in word_freqs.keys()}\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will iteratively merge pairs of these splits into subword tokens. To select which pair to merge, we compute a score for all observed pairs as follows:\n",
    "\n",
    "$score(c1,c2) = \\frac{freq(c1,c2)}{freq(c1) * freq(c2)}$\n",
    "\n",
    "where c1 and c2 are a pair of splits, freq(c1,c2) is the count of how many times c1,c2 co-occur in the corpus and freq(c) is the count of how many times we observe c. This merger algorithm trherefore prioritizes merging splits which appear together frequently but appear separately more rarely.\n",
    "\n",
    "\n",
    "For example, \n",
    "\n",
    "freq('t', '##h') = 2 (observed in the word 'the':2) and \n",
    "\n",
    "freq('t') = 2 (observed in the word 'the':2)\n",
    "\n",
    "freq('##h') = 4 (observed in words 'the':2, 'copyright':1,'rights':1)  \n",
    "\n",
    "score('t', '##h') = 1/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for computing pair scores\n",
    "def compute_pair_scores(splits):\n",
    "    letter_freqs = defaultdict(int)\n",
    "    pair_freqs = defaultdict(int)\n",
    "\n",
    "    for word,freq in word_freqs.items():            \n",
    "        split = splits[word]\n",
    "        # if word only contains one split\n",
    "        if len(split) == 1:\n",
    "            letter_freqs[split[0]] += freq\n",
    "            continue\n",
    "        \n",
    "        # count up every individual split and adjacent pair of splits \n",
    "        for i in range(len(split)-1):\n",
    "            pair = (split[i], split[i+1])\n",
    "            letter_freqs[split[i]] += freq\n",
    "            pair_freqs[pair] += freq\n",
    "        letter_freqs[split[-1]] += freq\n",
    "\n",
    "    scores = {pair: freq/(letter_freqs[pair[0]]*letter_freqs[pair[1]]) for pair,freq in pair_freqs.items()}\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('t', '##h'): 0.25, ('##h', '##e'): 0.03125, ('h', '##a'): 0.2, ('##a', '##l'): 0.05, ('##l', '##f'): 0.0625, ('##f', '##-'): 0.5, ('##-', '##l'): 0.125, ('##l', '##i'): 0.025, ('##i', '##n'): 0.05, ('##n', '##g'): 0.041666666666666664, ('b', '##o'): 0.2, ('##o', '##o'): 0.04, ('##o', '##k'): 0.2, ('o', '##n'): 0.125, ('##n', '##e'): 0.03125, ('i', '##n'): 0.125, ('f', '##a'): 0.2, ('##l', '##l'): 0.03125, ('o', '##f'): 0.25, ('i', '##g'): 0.08333333333333333, ('##g', '##n'): 0.041666666666666664, ('##e', '##e'): 0.01171875, ('##e', '##r'): 0.0390625, ('##r', '##i'): 0.075, ('##i', '##a'): 0.04, ('s', '##e'): 0.020833333333333332, ('##i', '##e'): 0.0125, ('##e', '##s'): 0.041666666666666664, ('k', '##a'): 0.2, ('##a', '##y'): 0.13333333333333333, ('##y', '##l'): 0.08333333333333333, ('##l', '##e'): 0.015625, ('s', '##o'): 0.13333333333333333, ('##o', '##d'): 0.13333333333333333, ('##d', '##e'): 0.041666666666666664, ('##r', '##b'): 0.125, ('##b', '##u'): 0.5, ('##u', '##r'): 0.125, ('##r', '##g'): 0.041666666666666664, ('c', '##o'): 0.2, ('##o', '##p'): 0.2, ('##p', '##y'): 0.3333333333333333, ('##y', '##r'): 0.041666666666666664, ('##i', '##g'): 0.06666666666666667, ('##g', '##h'): 0.08333333333333333, ('##h', '##t'): 0.25, ('2', '##0'): 1.0, ('##0', '##1'): 1.0, ('##1', '##3'): 1.0, ('a', '##l'): 0.125, ('r', '##i'): 0.1, ('##t', '##s'): 0.16666666666666666, ('r', '##e'): 0.03125, ('##s', '##e'): 0.020833333333333332, ('##r', '##v'): 0.125, ('##v', '##e'): 0.0625, ('##e', '##d'): 0.020833333333333332}\n",
      "Pair with largest score:  ('2', '##0')\n"
     ]
    }
   ],
   "source": [
    "pair_scores = compute_pair_scores(splits)\n",
    "print(pair_scores)\n",
    "\n",
    "# get pair with largest score (ties broken by picking first occurance of largest value)\n",
    "max_score_pair = max(pair_scores, key = lambda x: pair_scores[x])\n",
    "print(\"Pair with largest score: \", max_score_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we merge these two splits into a new subword token and add the subword to our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['##-', '##0', '##1', '##3', '##a', '##b', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##y', '.', '2', '[CLS]', '[MASK]', '[PAD]', '[SEP]', '[UNK]', 'a', 'b', 'c', 'f', 'h', 'i', 'k', 'o', 'r', 's', 't', '20']\n"
     ]
    }
   ],
   "source": [
    "subword = max_score_pair[0] + max_score_pair[1].lstrip('#')\n",
    "vocab.append(subword)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(c1, c2, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) > 1:\n",
    "            i = 0\n",
    "            while i < len(split)-1:\n",
    "                if split[i] == c1 and split[i+1] == c2:\n",
    "                    merged = c1 + c2.lstrip('#')\n",
    "                    split = split[:i] + [merged] + split[i+2:]\n",
    "                else:\n",
    "                    i += 1\n",
    "                splits[word] = split\n",
    "\n",
    "    return splits                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': ['t', '##h', '##e'],\n",
       " 'half-ling': ['h', '##a', '##l', '##f', '##-', '##l', '##i', '##n', '##g'],\n",
       " 'book': ['b', '##o', '##o', '##k'],\n",
       " 'one': ['o', '##n', '##e'],\n",
       " 'in': ['i', '##n'],\n",
       " 'fall': ['f', '##a', '##l', '##l'],\n",
       " 'of': ['o', '##f'],\n",
       " 'igneeria': ['i', '##g', '##n', '##e', '##e', '##r', '##i', '##a'],\n",
       " 'series': ['s', '##e', '##r', '##i', '##e', '##s'],\n",
       " 'kaylee': ['k', '##a', '##y', '##l', '##e', '##e'],\n",
       " 'soderburg': ['s', '##o', '##d', '##e', '##r', '##b', '##u', '##r', '##g'],\n",
       " 'copyright': ['c', '##o', '##p', '##y', '##r', '##i', '##g', '##h', '##t'],\n",
       " '2013': ['20', '##1', '##3'],\n",
       " 'all': ['a', '##l', '##l'],\n",
       " 'rights': ['r', '##i', '##g', '##h', '##t', '##s'],\n",
       " 'reserved': ['r', '##e', '##s', '##e', '##r', '##v', '##e', '##d'],\n",
       " '.': ['.']}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = merge_pair(max_score_pair[0], max_score_pair[1], splits)\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can keep merging iteratively until we have reahed some maximum vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 50\n",
    "\n",
    "while len(vocab) < max_vocab_size:\n",
    "    # compute all pair scores\n",
    "    pair_scores = compute_pair_scores(splits)\n",
    "    # get pair with largest score (ties broken by picking first occurance of largest value)\n",
    "    max_score_pair = max(pair_scores, key = lambda x: pair_scores[x])\n",
    "    # add new subword to vacabulary\n",
    "    subword = max_score_pair[0] + max_score_pair[1].lstrip('#')\n",
    "    vocab.append(subword)\n",
    "    # update splits \n",
    "    splits = merge_pair(*max_score_pair, splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['##-', '##0', '##1', '##3', '##a', '##b', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##y', '.', '2', '[CLS]', '[MASK]', '[PAD]', '[SEP]', '[UNK]', 'a', 'b', 'c', 'f', 'h', 'i', 'k', 'o', 'r', 's', 't', '20', '201', '2013', '##f-', 'of', '##bu', '##py', 'th', '##ht']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we tokenize a given word into the learned subwords. To tokenize a word, we find the longest matching subword starting from the first character and we split the word. Then we repeat the process from the second half. If we reach the fiunal character and haven't found a matching subword from the vocabulary, then we declare the entire word as '[UNK]' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_word(word):\n",
    "    tokens = []\n",
    "    while len(word) > 0:\n",
    "        i = len(word)    \n",
    "        # find longest mactching subword subword\n",
    "        while i > 0 and word[:i] not in vocab:\n",
    "            i -= 1\n",
    "        if i == 0:\n",
    "            # no match found\n",
    "            return [\"[UNK]\"]\n",
    "        # found longest subword\n",
    "        tokens.append(word[:i])\n",
    "        word = word[i:]\n",
    "        # add prefix\n",
    "        if len(word) > 0:\n",
    "            word = f\"##{word}\"\n",
    "    return tokens            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f', '##a', '##bu', '##l', '##o', '##u', '##s']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_word(\"fabulous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All together, the WordPiece vocab generator can be implemented as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPieceTokenizer():\n",
    "    def __init__(self):\n",
    "        self.vocab = []\n",
    "        self.word2int = {}\n",
    "        self.int2word = {}\n",
    "        # special tokens\n",
    "        self.pad_token = \"[PAD]\"\n",
    "        self.mask_token = \"[MASK]\"\n",
    "        self.unk_token = \"[UNK]\"\n",
    "        self.cls_token = \"[CLS]\"\n",
    "        self.sep_token = \"[SEP]\"\n",
    "        \n",
    "        self.invalid_chars = ('*', '~', '_', '^', '`')\n",
    "        self.max_vocab_size = None\n",
    "        \n",
    "\n",
    "    def mask_token_id(self):\n",
    "        return self.word2int[self.mask_token]\n",
    "\n",
    "    def pad_token_id(self):\n",
    "        return self.word2int[self.pad_token]\n",
    "\n",
    "    def cls_token_id(self):\n",
    "        return self.word2int[self.cls_token]\n",
    "\n",
    "    def unk_token_id(self):\n",
    "        return self.word2int[self.unk_token]\n",
    "\n",
    "    def sep_token_id(self):\n",
    "        return self.word2int[self.sep_token]\n",
    "\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    # removes all control characters and invalid characters, replaces multiple adjacent whitespace with single whitespace\n",
    "    def clean_sentence(self, s):\n",
    "        s = \"\".join(ch for ch in s if unicodedata.category(ch)[0] != 'C' and ch not in self.invalid_chars) \n",
    "        s = \" \".join(s.split())\n",
    "        # remove all non-letter characters\n",
    "        #s = \"\".join(ch for ch in s if unicodedata.category(ch)[0]=='L' or unicodedata.category(ch)=='Zs') \n",
    "        return s\n",
    "\n",
    "    # generates wordpiece vocabulary of subwords from a given corpus\n",
    "    # the input corpus is a list of sentences\n",
    "    def generate_vocab(self, corpus, max_vocab_size):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        # pretokenize the corpus into words and get unigram counts, we will only use the first sentence as an example\n",
    "        word_freqs = defaultdict(int)\n",
    "        for s in corpus:\n",
    "            s = self.clean_sentence(s)\n",
    "            words = s.split()\n",
    "            for word in words:\n",
    "                word_freqs[word] += 1\n",
    "        \n",
    "        # initialize WordPiece vocabulary\n",
    "        for word in word_freqs.keys():\n",
    "            if word[0] not in self.vocab:\n",
    "                self.vocab.append(word[0])\n",
    "            for letter in word[1:]:\n",
    "                prefixed = '##' + letter\n",
    "                if prefixed not in self.vocab:\n",
    "                    self.vocab.append(prefixed)\n",
    "\n",
    "        # now add special tokens\n",
    "        self.vocab = self.vocab + [self.pad_token, self.cls_token, self.unk_token, self.mask_token, self.sep_token]\n",
    "\n",
    "        # generate splits\n",
    "        splits = {word: [c if i==0 else f\"##{c}\" for i,c in enumerate(word)] for word in word_freqs.keys()}\n",
    "        \n",
    "        # function for computing pair scores\n",
    "        def compute_pair_scores(splits):\n",
    "            letter_freqs = defaultdict(int)\n",
    "            pair_freqs = defaultdict(int)\n",
    "\n",
    "            for word,freq in word_freqs.items():            \n",
    "                split = splits[word]\n",
    "                # if word only contains one split\n",
    "                if len(split) == 1:\n",
    "                    letter_freqs[split[0]] += freq\n",
    "                    continue\n",
    "                \n",
    "                # count up every individual split and adjacent pair of splits \n",
    "                for i in range(len(split)-1):\n",
    "                    pair = (split[i], split[i+1])\n",
    "                    letter_freqs[split[i]] += freq\n",
    "                    pair_freqs[pair] += freq\n",
    "                letter_freqs[split[-1]] += freq\n",
    "\n",
    "            scores = {pair: freq/(letter_freqs[pair[0]]*letter_freqs[pair[1]]) for pair,freq in pair_freqs.items()}\n",
    "            return scores\n",
    "        \n",
    "        # function for merging a pair of splits    \n",
    "        def merge_pair(c1, c2, splits):\n",
    "            for word in word_freqs:\n",
    "                split = splits[word]\n",
    "                if len(split) > 1:\n",
    "                    i = 0\n",
    "                    while i < len(split)-1:\n",
    "                        if split[i] == c1 and split[i+1] == c2:\n",
    "                            merged = c1 + c2.lstrip('#')\n",
    "                            split = split[:i] + [merged] + split[i+2:]\n",
    "                        else:\n",
    "                            i += 1\n",
    "                        splits[word] = split\n",
    "\n",
    "            return splits    \n",
    "        \n",
    "        # generate the subword vocabulary\n",
    "        pbar = tqdm(total=max_vocab_size, desc=\"Building vocab. Current vocab_size --> \")\n",
    "        pbar.update(len(self.vocab))\n",
    "\n",
    "        while len(self.vocab) < max_vocab_size:\n",
    "            # compute all pair scores\n",
    "            pair_scores = compute_pair_scores(splits)\n",
    "            # get pairs with largest score \n",
    "            max_score = max(pair_scores.values())\n",
    "            max_score_pairs = [pair for pair, score in pair_scores.items() if score== max_score]\n",
    "            # randomly break ties\n",
    "            max_score_pair = random.choice(max_score_pairs)\n",
    "            # add new subword to vacabulary\n",
    "            subword = max_score_pair[0] + max_score_pair[1].lstrip('#')\n",
    "            self.vocab.append(subword)\n",
    "            # update splits \n",
    "            splits = merge_pair(*max_score_pair, splits)\n",
    "            pbar.update(1)\n",
    "            \n",
    "        self.vocab = sorted(set(self.vocab))\n",
    "        self.word2int = {word:i for i,word in enumerate(self.vocab)}\n",
    "        self.int2word = {i:word for i,word in enumerate(self.vocab)}\n",
    "\n",
    "\n",
    "    def encode_sentence(self, s):\n",
    "        # first clean the sentence\n",
    "        s = self.clean_sentence(s)\n",
    "        # tokenize the sentence into subword sequence\n",
    "        subword_tokens = self.tokenize_sentence(s)\n",
    "        # convert to token indices\n",
    "        indices = [self.word2int[t] for t in subword_tokens]\n",
    "        return indices\n",
    "\n",
    "    # encode sentence into subword token indices\n",
    "    def encode(self, sentences):\n",
    "        encoded_sentences = []\n",
    "        pbar = tqdm(total=len(sentences), desc=\"Encoding sequences.\")\n",
    "        pbar.update(len(self.vocab))\n",
    "        for s in sentences:\n",
    "            indices = self.encode_sentence(s)\n",
    "            encoded_sentences.append(indices)\n",
    "            pbar.update(1)\n",
    "        return encoded_sentences\n",
    "\n",
    "    # decode subword token index sequences back to sentences\n",
    "    def decode(self, idx):\n",
    "        sentences = []\n",
    "        pbar = tqdm(total=len(idx), desc=\"Decoding sequences.\")\n",
    "        for indices in idx:\n",
    "            # first cnvert indices to subword tokens\n",
    "            subwords = [self.int2word[ix] for ix in indices]\n",
    "\n",
    "            # merge subwords\n",
    "            i = 0\n",
    "            while i < len(subwords)-1:\n",
    "                a = subwords[i]\n",
    "                b = subwords[i+1]\n",
    "                if len(b) == 1:\n",
    "                    i += 1  \n",
    "                    continue\n",
    "                if b[:2]==\"##\":\n",
    "                    subwords = subwords[:i] + [a+b.lstrip('#')] + subwords[i+2:]\n",
    "                \n",
    "                else:       \n",
    "                    i += 1    \n",
    "\n",
    "            s = \" \".join(subwords)\n",
    "            sentences.append(s)\n",
    "            pbar.update(1)        \n",
    "        return sentences\n",
    "\n",
    "\n",
    "    def tokenize_sentence(self, sent):\n",
    "        tokens = []\n",
    "        # split the sentence into words \n",
    "        # make sure to convert all characters to lower case because our vocabulary does not contain\n",
    "        # upper case letters\n",
    "        words = sent.lower().split()\n",
    "        # tokenize each word\n",
    "        for word in words:\n",
    "            tokens = tokens + self.tokenize_word(word)\n",
    "        return tokens\n",
    "\n",
    "    def tokenize_word(self, word):\n",
    "        tokens = []\n",
    "        while len(word) > 0:\n",
    "            i = len(word)    \n",
    "            # find longest mactching subword subword\n",
    "            while i > 0 and word[:i] not in self.vocab:\n",
    "                i -= 1\n",
    "            if i == 0:\n",
    "                # no match found\n",
    "                return [self.unk_token]\n",
    "            # found longest subword\n",
    "            tokens.append(word[:i])\n",
    "            word = word[i:]\n",
    "            # add prefix\n",
    "            if len(word) > 0:\n",
    "                word = f\"##{word}\"\n",
    "        return tokens          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate tokenizer object\n",
    "tokenizer = WordPieceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building vocab. Current vocab_size --> : 100%|██████████| 256/256 [00:32<00:00,  7.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '#', \"##'\", '##+', '##++', '##,', '##-', '##.', '##/', '##/+', '##0', '##0:42', '##0:46', '##0:48', '##1', '##12:3', '##12:30', '##1:47', '##1:48', '##2', '##2:3', '##2:4', '##2:45', '##2:47', '##3', '##3:41', '##4', '##5', '##5:46', '##6', '##7', '##8', '##9', '##:', '##:1', '##:15', '##:19', '##:3', '##:4', '##:41', '##:42', '##:43', '##:45', '##:46', '##:47', '##:48', '##=', '##\\\\', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##j', '##k', '##l', '##m', '##n', '##o', '##p', '##q', '##r', '##s', '##t', '##u', '##v', '##w', '##x', '##y', '##z', '$', '%', '&', \"'\", '(', ')', '+', ',', '-', '.', '/', '//', '0', '1', '10:42', '10:48', '11:47', '11:48', '12:3', '12:30', '12:30:', '12:30:0', '12:30:00', '12:30:1', '12:30:18', '12:30:2', '12:30:20', '12:31', '12:32', '12:34', '12:35', '12:36', '12:39', '12:4', '12:40', '12:43', '12:45', '12:46', '12:47', '12:49', '15:46', '19', '196', '1963', '1967', '1968', '1969', '199', '1998', '1999', '1:4', '1:48', '2', '20:46', '22:3', '22:32', '22:35', '23:41', '2:47', '2:48', '3', '3:1', '3:13', '3:14', '3:15', '3:16', '3:4', '3:40', '3:47', '4', '4:15', '4:19', '4:3', '4:30', '4:31', '4:35', '4:36', '4:38', '4:39', '4:4', '4:40', '4:41', '4:42', '4:44', '4:45', '4:47', '5', '5:3', '5:4', '5:40', '5:41', '5:43', '5:45', '5:46', '5:49', '6', '6:', '6:3', '6:4', '6:40', '6:43', '6:45', '6:47', '6:49', '7', '7:', '7:3', '7:30', '7:31', '7:32', '7:34', '7:35', '7:37', '7:38', '7:4', '7:40', '7:41', '7:45', '7:47', '8', '8:', '8:3', '8:4', '8:41', '8:45', '8:46', '8:47', '8:48', '9', '9:', '9:3', '9:30', '9:35', '9:4', '9:42', '9:45', '9:47', ':', ';', '<', '=', '=9', '>', '?', '@', '[', '[CLS]', '[MASK]', '[PAD]', '[SEP]', '[UNK]', '\\\\', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# generate vocabulary\n",
    "tokenizer.generate_vocab(text, max_vocab_size=256)\n",
    "print(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you try squeezing out information from one of these pests .\n",
      "['you', 't', '##r', '##y', 'squ', '##e', '##e', '##zing', 'out', 'i', '##n', '##f', '##o', '##r', '##m', '##a', '##t', '##i', '##o', '##n', 'f', '##r', '##o', '##m', 'on', '##e', 'of', 'th', '##e', '##s', '##e', 'p', '##e', '##s', '##t', '##s', '[UNK]']\n",
      "[[8140, 7289, 2349, 3238, 7140, 735, 735, 3312, 6507, 5317, 1740, 736, 1931, 2349, 1648, 0, 2832, 1008, 1931, 1740, 4876, 2349, 1931, 1648, 6426, 735, 6359, 7319, 735, 2662, 735, 6557, 735, 2662, 2832, 2662, 3341]]\n",
      "['you try squeezing out information from one of these pests [UNK]']\n"
     ]
    }
   ],
   "source": [
    "# test encoding and decoding a sentence\n",
    "s = text[712]\n",
    "print(s)\n",
    "print(tokenizer.tokenize_sentence(s))\n",
    "encoded = tokenizer.encode([s])\n",
    "print(encoded)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets encode the dataset into integer token sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_encoded = tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "'''\n",
    "# save encoded dataset to file\n",
    "with open('dataset_encoded', 'wb') as file:\n",
    "    pickle.dump(dataset_encoded, file)   \n",
    "'''\n",
    "\n",
    "'''\n",
    "with open('dataset_encoded', 'rb') as file:\n",
    "    dataset_encoded_2 = pickle.load(file)\n",
    "'''  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a copy of the trained tokenizer to file\n",
    "'''\n",
    "with open('WordPiece_tokenizer.pkl', 'wb') as file:\n",
    "    pickle.dump(tokenizer, file)\n",
    "'''\n",
    "\n",
    "# load tokenizer object from file\n",
    "'''\n",
    "with open('WordPiece_tokenizer.pkl', 'rb') as file:\n",
    "    tokenizer = pickle.load(file)  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch Dataset API for prepping data for Masked Language Model Task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, corpus, tokenizer, block_size, mlm_prob=0.15):\n",
    "        self.corpus = corpus          # encoded sentences\n",
    "        self.tokenizer = tokenizer    # wordpiece tokenizer\n",
    "        self.block_size = block_size  # truncation/max length of sentences\n",
    "        self.corpus_len = len(corpus) # size of corpus\n",
    "        self.mlm_prob = mlm_prob\n",
    "        self.vocab_size = tokenizer.vocab_size()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.corpus_len\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get the sentence \n",
    "        s = self.get_corpus_sentence(idx)\n",
    "        # truncate to block_size-1\n",
    "        s = s[:self.block_size-1] \n",
    "        s_len = len(s)\n",
    "        \n",
    "        # replace tokens randomly\n",
    "        s, label = self.replace_tokens(s)\n",
    "        # append the CLS token at the beginning of sentence\n",
    "        s = [self.tokenizer.cls_token_id()] + s\n",
    "        # apply padding\n",
    "        pad_len = max(0,self.block_size-s_len-1)\n",
    "        s = s + [self.tokenizer.pad_token_id()]*pad_len\n",
    "        label = [-100] + label + [-100]*pad_len\n",
    "        # create attention mask which has 0's at positions of pad tokens and 1's everywhere else \n",
    "        attention_mask = [1]*(self.block_size-pad_len) + [0]*pad_len       \n",
    "\n",
    "        # convert to torch tensors\n",
    "        s = torch.tensor(s)\n",
    "        label = torch.tensor(label)\n",
    "        attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "        # Note: Unlike the original BERT, we are not returning a pair of sentences, so we\n",
    "        # don't need to return segment labels or next_sentence label \n",
    "        return {\"masked_input\" : s, \"label\" : label, \"attention_mask\" : attention_mask}\n",
    "\n",
    "\n",
    "    # randomly replace tokens with mlm_prob probability\n",
    "    def replace_tokens(self, s):\n",
    "        # the labels for a masked token is the original token index and -100 for non-masked tokens\n",
    "        label = [-100] * len(s)\n",
    "        for i,t in enumerate(s):\n",
    "            p = random.random()    \n",
    "            if p < self.mlm_prob:\n",
    "                p = p/self.mlm_prob\n",
    "                # replace with masked token with 80% probability\n",
    "                if p < 0.8:\n",
    "                    s[i] = self.tokenizer.mask_token_id()        \n",
    "\n",
    "                elif p < 0.9:\n",
    "                    # replace with random token with 10% probability \n",
    "                    s[i] = random.randrange(self.vocab_size)\n",
    "                \n",
    "                # Note: for all three cases, i.e. token getting replaced by mask token, token getting replaced\n",
    "                # by another random token or token not getting replaced, we want to predict the actual word as our target label \n",
    "                label[i] = t\n",
    "\n",
    "        return s, label\n",
    "\n",
    "\n",
    "    def get_corpus_sentence(self, idx):\n",
    "        return self.corpus[idx]        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoader for generating batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of batches: 15625\n"
     ]
    }
   ],
   "source": [
    "block_size = 128\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = BERTDataset(dataset_encoded, tokenizer, block_size=block_size)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4)  # set pin_memory for faster pre-fetching \n",
    "print(f\"Total number of batches: {len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'masked_input': tensor([[3337, 7319, 7078,  ..., 3339, 3339, 3339],\n",
       "         [3337, 6883, 1931,  ..., 3339, 3339, 3339],\n",
       "         [3337, 7990, 6949,  ..., 3339, 3339, 3339],\n",
       "         ...,\n",
       "         [3337, 7990, 1008,  ..., 3339, 3339, 3339],\n",
       "         [3337, 6507, 2662,  ..., 3339, 3339, 3339],\n",
       "         [3337, 5317, 3338,  ..., 3339, 3339, 3339]]),\n",
       " 'label': tensor([[-100, -100,  735,  ..., -100, -100, -100],\n",
       "         [-100, -100, -100,  ..., -100, -100, -100],\n",
       "         [-100, -100, -100,  ..., -100, -100, -100],\n",
       "         ...,\n",
       "         [-100, -100, 1008,  ..., -100, -100, -100],\n",
       "         [-100, -100, -100,  ..., -100, -100, -100],\n",
       "         [-100, -100, 6883,  ..., -100, -100, -100]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load an example batch and show the contents\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "sample_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n"
     ]
    }
   ],
   "source": [
    "print(sample_batch['masked_input'].shape)\n",
    "print(sample_batch['label'].shape)\n",
    "print(sample_batch['attention_mask'].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's train our BERT model on the MLM task. Note that were using fixed sized input sequences, denoted by block_size. Ideally we would want block_size to be as large as possible because a larger context window leads to the model learning better language representations from the MLM task. However, there are obvious computational constraints which prevent us from making block_size too big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_checkpoint(model, optimizer, epoch=None, loss=None):\n",
    "    # Save the model and optimizer state_dict\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "\n",
    "    # Save the checkpoint to a file\n",
    "    torch.save(checkpoint, 'BERT_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 20.536064 M\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "block_size = 128\n",
    "embedding_dim = 384\n",
    "head_size = embedding_dim\n",
    "num_heads = 12\n",
    "num_blocks = 8\n",
    "dropout_rate = 0.2\n",
    "max_iters = 1\n",
    "learning_rate = 5e-5\n",
    "\n",
    "model = BERTModel(vocab_size=tokenizer.vocab_size(), block_size=block_size, embedding_dim=embedding_dim, head_size=head_size, num_heads=num_heads, num_blocks=num_blocks, pad_token_id=tokenizer.pad_token_id(), dropout_rate=dropout_rate)\n",
    "# move model to device\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "\n",
    "num_params = sum(p.numel() for p in m.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch Loss: 0.900, Moving avg. Loss: 0.890: 100%|██████████| 15625/15625 [49:18<00:00,  5.28it/s] \n",
      "Epoch 2, Batch Loss: 0.743, Moving avg. Loss: 0.797: 100%|██████████| 15625/15625 [49:19<00:00,  5.28it/s]\n",
      "Epoch 3, Batch Loss: 0.762, Moving avg. Loss: 0.818: 100%|██████████| 15625/15625 [49:25<00:00,  5.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "train_loss = None\n",
    "val_loss = None\n",
    "\n",
    "#smoothed_loss = 0.0\n",
    "for epoch in range(3):\n",
    "    pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "    for batch in pbar:\n",
    "        # sample a batch of trainin data\n",
    "        xb, yb, attn_mask = batch['masked_input'], batch['label'], batch['attention_mask'] \n",
    "        # move batches to gpu\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        attn_mask = attn_mask.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        logits = m(xb, attn_mask)\n",
    "        # compute loss\n",
    "        B,T,vocab_size = logits.shape\n",
    "        # reshape the logits and targets such that batch of input sequences are flattened into a single big input sequence\n",
    "        # i.e. (B,T) --> (B*T)\n",
    "        logits = logits.view(B*T,vocab_size) # reshaped to (B*T,vocab_size)\n",
    "        yb = yb.view(B*T) # reshaped to (B*T)\n",
    "        # compute cross entropy loss (i.e. average negative log likelihood)\n",
    "        loss = F.cross_entropy(logits, yb, ignore_index=-100)\n",
    "\n",
    "        # exponential moving average loss\n",
    "        smoothed_loss = 0.9 * smoothed_loss + 0.1 * loss\n",
    "\n",
    "        # reset parameter gradients\n",
    "        optimizer.zero_grad(set_to_none=True) \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_description(f\"Epoch {epoch + 1}, Batch Loss: {loss:.3f}, Moving avg. Loss: {smoothed_loss:.3f}\")   \n",
    "    \n",
    "    # save checkpoint \n",
    "    save_model_checkpoint(m, optimizer)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Load the checkpoint from the file\n",
    "checkpoint = torch.load('BERT_checkpoint.pth')\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = BERTModel(vocab_size=tokenizer.vocab_size(), block_size=block_size, embedding_dim=embedding_dim, head_size=head_size, num_heads=num_heads, num_blocks=num_blocks, pad_token_id=tokenizer.pad_token_id(), dropout_rate=dropout_rate)\n",
    "# move model to device\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "\n",
    "# Load the model and optimizer state_dict from the checkpoint\n",
    "m.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "m.train()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at some test predictions made by our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first create test dataset (since first 1M lines are used for training, grab lines from after that point)\n",
    "with open(\"bookcorpus_small.txt\", 'r') as f:\n",
    "    # get 2000 lines\n",
    "    test_lines = list(islice(f, 2000000, 2002000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the sentences\n",
    "for i, s in enumerate(test_lines):\n",
    "    s = test_lines[i].strip()\n",
    "    test_lines[i] = \"\".join(ch for ch in s if unicodedata.category(ch)[0]=='L' or unicodedata.category(ch)=='Zs') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'while his opponent took a far more casual approach , playing out each move with consideration , ease and precision , dancing his way through the brawl , maneuvering with absolute elegance .\\n'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = test_lines[0]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wh',\n",
       " '##i',\n",
       " '##l',\n",
       " '##e',\n",
       " 'h',\n",
       " '##i',\n",
       " '##s',\n",
       " 'oppon',\n",
       " '##e',\n",
       " '##n',\n",
       " '##t',\n",
       " 'to',\n",
       " '##o',\n",
       " '##k',\n",
       " 'a',\n",
       " 'f',\n",
       " '##a',\n",
       " '##r',\n",
       " 'm',\n",
       " '##o',\n",
       " '##r',\n",
       " '##e',\n",
       " 'c',\n",
       " '##a',\n",
       " '##s',\n",
       " '##u',\n",
       " '##a',\n",
       " '##l',\n",
       " 'app',\n",
       " '##r',\n",
       " '##o',\n",
       " '##a',\n",
       " '##ch',\n",
       " '[UNK]',\n",
       " 'p',\n",
       " '##l',\n",
       " '##a',\n",
       " '##y',\n",
       " '##ing',\n",
       " 'out',\n",
       " 'e',\n",
       " '##a',\n",
       " '##ch',\n",
       " 'm',\n",
       " '##o',\n",
       " '##v',\n",
       " '##e',\n",
       " 'with',\n",
       " 'consid',\n",
       " '##e',\n",
       " '##r',\n",
       " '##a',\n",
       " '##t',\n",
       " '##i',\n",
       " '##o',\n",
       " '##n',\n",
       " '[UNK]',\n",
       " 'e',\n",
       " '##a',\n",
       " '##s',\n",
       " '##e',\n",
       " 'a',\n",
       " '##n',\n",
       " '##d',\n",
       " 'p',\n",
       " '##r',\n",
       " '##e',\n",
       " '##c',\n",
       " '##i',\n",
       " '##s',\n",
       " '##i',\n",
       " '##o',\n",
       " '##n',\n",
       " '[UNK]',\n",
       " 'd',\n",
       " '##a',\n",
       " '##n',\n",
       " '##c',\n",
       " '##ing',\n",
       " 'h',\n",
       " '##i',\n",
       " '##s',\n",
       " 'w',\n",
       " '##a',\n",
       " '##y',\n",
       " 'through',\n",
       " 'th',\n",
       " '##e',\n",
       " 'b',\n",
       " '##r',\n",
       " '##a',\n",
       " '##w',\n",
       " '##l',\n",
       " '[UNK]',\n",
       " 'm',\n",
       " '##a',\n",
       " '##n',\n",
       " '##e',\n",
       " '##u',\n",
       " '##v',\n",
       " '##e',\n",
       " '##r',\n",
       " '##ing',\n",
       " 'with',\n",
       " 'ab',\n",
       " '##s',\n",
       " '##o',\n",
       " '##l',\n",
       " '##u',\n",
       " '##t',\n",
       " '##e',\n",
       " 'e',\n",
       " '##l',\n",
       " '##e',\n",
       " '##g',\n",
       " '##a',\n",
       " '##n',\n",
       " '##c',\n",
       " '##e',\n",
       " '[UNK]']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subword_tokens = tokenizer.tokenize_sentence(s)\n",
    "subword_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
