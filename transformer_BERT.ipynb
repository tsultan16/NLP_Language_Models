{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# use GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A simple/minimal implementation of the BERT model (https://arxiv.org/pdf/1810.04805v2.pdf). \n",
    "\n",
    "The transformer block and multihead attention layer implementations are based on the Andrej Karpathy GPT youtube tutorial. In this case, we use a transformer encoder block which uses bi-directional context, differing from the transformer decoder in GPT which is unidirectional (achieved via causal masking of attention weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, total_head_size, num_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        assert total_head_size % num_heads == 0, \"head_size needs to be integer multiple of num_heads\"\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.total_head_size = total_head_size \n",
    "        self.head_size = total_head_size // num_heads \n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # define parameters\n",
    "        self.key = nn.Linear(embedding_dim, self.total_head_size, bias=False)\n",
    "        self.query = nn.Linear(embedding_dim, self.total_head_size, bias=False)\n",
    "        self.value = nn.Linear(embedding_dim, self.total_head_size, bias=False)\n",
    "        self.attn_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # non-parameter tensor of lower triangular ones\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        # we also need to apply a linear projection to make the output residual the same dimension as the input\n",
    "        self.proj = nn.Linear(total_head_size, embedding_dim) \n",
    "        self.output_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "\n",
    "    # define forward pass, input shape: (B,T,C) where B=batch size, T=block_size, C=embedding_dim\n",
    "    # the attn_mask is a mask that can be used for masking out the attention weights for padding tokens \n",
    "    def forward(self, x, attn_mask):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B,T,H) where H is the total_head_size\n",
    "        q = self.query(x) # (B,T,H)\n",
    "        v = self.value(x) # (B,T,H)\n",
    "\n",
    "        # reshape (B,T,H) --> (B,T,n,h), where n=num_heads and h=head_size and H=n*h\n",
    "        k = k.view(B,T,self.num_heads,self.head_size) \n",
    "        q = q.view(B,T,self.num_heads,self.head_size) \n",
    "        v = v.view(B,T,self.num_heads,self.head_size) \n",
    "\n",
    "        # now we transpose so that the num_heads is the second dimension followed by T,h\n",
    "        # this allows us to batch matrix mutliply for all heads simulataneously to compute their attention weights\n",
    "        # (B,T,n,h) --> (B,n,T,h) \n",
    "        k = k.transpose(1,2) \n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "\n",
    "        # compute attention scores manually (slower)\n",
    "        '''\n",
    "        W = q @ k.transpose(-2,-1)  / math.sqrt(self.head_size) # (B,n,T,T)\n",
    "        W = W.masked_fill(attn_mask == 0, float('-inf')) \n",
    "        W = F.softmax(W, dim=-1)\n",
    "        # apply dropout to attention weights\n",
    "        W = self.attn_dropout(W)\n",
    "        out = W @ v # (B,n,T,h)\n",
    "        '''\n",
    "\n",
    "        # use pytorch built-in function for faster computation of attention scores (set the 'is_causal' parameter for applying causal masking)\n",
    "        out = F.scaled_dot_product_attention(q,k,v,attn_mask=attn_mask,dropout_p=self.dropout_rate if self.training else 0,is_causal=False)\n",
    "\n",
    "        # we can transpose the output from (B,n,T,h) --> (B,T,n,h)\n",
    "        # since the last two dimensions of the transposed tensor are non-contiguous, we apply \n",
    "        # contiguous() which return a contiguous tensor\n",
    "        out = out.transpose(1,2).contiguous()\n",
    "\n",
    "        # finally we collapse the last two dimensions to get the concatenated output, (B,T,n,h) --> (B,T,n*h) \n",
    "        out = out.view(B,T,self.total_head_size)\n",
    "\n",
    "        # now we project the concatenated output so that it has the same dimensions as the multihead attention layer input\n",
    "        # (we need to add it with the input because of the residual connection, so need to be same size) \n",
    "        out = self.proj(out) # (B,T,C) \n",
    "\n",
    "        # apply dropout\n",
    "        out = self.output_dropout(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "# a simple mlp \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "        # we add extra computations by growing out the feed-forward hidden size by a factor of 4\n",
    "        # we also add an extra linear layer at the end to project the residual back to same dimensions as input\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 4*embedding_dim),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*embedding_dim, embedding_dim), \n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "    \n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "# transformer encoder block with residual connection and layer norm\n",
    "# Note: the original transformer uses post layer norms, here we use pre layer norms, i.e. layer norm is applied at the input\n",
    "# instead of the output, this typically leads to better results in terms of training convergence speed and gradient scaling \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, head_size, num_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(block_size, embedding_dim, head_size, num_heads, dropout_rate) # multi-head attention layer \n",
    "        self.ff = FeedForward(embedding_dim, dropout_rate)   # feed-forward layer\n",
    "        self.ln1 = nn.LayerNorm(embedding_dim) # layer norm at input of multi-head attention\n",
    "        self.ln2 = nn.LayerNorm(embedding_dim) # layer norm at input of feed-forward\n",
    "\n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x, attn_mask):\n",
    "        # residual connection between input and multi-head attention output (also note that we're doing a pre-layer norm, i.e. layer norm at the input of the multi-head attention)\n",
    "        x = x + self.sa(self.ln1(x), attn_mask)\n",
    "        # residual connection between multi-head attention output and feed-forward output (also note that we're doing a pre-layer norm, i.e. layer norm at the input of the feed-forward)\n",
    "        x = x + self.ff(self.ln2(x)) \n",
    "        return x\n",
    "    \n",
    "\n",
    "# BERT model with multiple transformer blocks \n",
    "class BERTModel(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, embedding_dim, head_size, num_heads, num_blocks, pad_token_id, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size        # block_size is just the input sequence length\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_size = head_size\n",
    "        self.hum_heads = num_heads\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        '''\n",
    "        Define model parameters\n",
    "        '''\n",
    "        # token embedding layer \n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_token_id) # shape: (vocab_size,C)\n",
    "        # position embedding layer\n",
    "        self.pos_embedding = nn.Embedding(block_size, embedding_dim) # shape: (T,C)\n",
    "        # segment embedding layer (disabled for now)\n",
    "        #self.segment_embedding = nn.Embedding(2, embedding_dim)\n",
    "\n",
    "        # stack of transformer blocks\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(block_size, embedding_dim, head_size, num_heads, dropout_rate) for _ in range(num_blocks)])\n",
    "\n",
    "        # pooling transformation of CLS token\n",
    "        self.pooling_linear = nn.Linear(embedding_dim, embedding_dim) # shape: (C,C)\n",
    "        self.pooling_activation_fn = nn.Tanh()\n",
    "\n",
    "        # store position indices inside a buffer for fast access when computing position embeddings\n",
    "        self.position_idx = torch.arange(block_size, device=device).unsqueeze(0)\n",
    "        self.register_buffer('position_idx', self.position_idx)\n",
    "\n",
    "\n",
    "        # forward pass takes in a batch of input token sequences idx of shape (B,T) and corresponding targets of shape (B,T)\n",
    "    def forward(self, idx, attn_mask, targets=None, segment_idx=None):\n",
    "        B, T = idx.shape\n",
    "        # get token embeddings\n",
    "        token_embeds = self.token_embedding(idx) # (B,T,C)\n",
    "        # add positional encoding\n",
    "        pos_embeds = self.pos_embedding(self.position_idx[:,:T]) # (T,C) \n",
    "        \n",
    "        # add sentence segment embedding (disabled for now)\n",
    "        # segment_embeds = self.segment_embedding(segment_idx) # segment_idx is an integer tensor of shape (B,T) and has 0's at positions corresponding to \n",
    "        \n",
    "        # the first sentence and 1's at positions corresponding to the second sentence \n",
    "        x = token_embeds + pos_embeds # (B,T,C)\n",
    "        # pass through transformer blocks to get encoding\n",
    "        for block in self.blocks:\n",
    "            x = block(x, attn_mask) # (B,T,C)\n",
    "    \n",
    "        # get CLS token encoding and apply pooling transform\n",
    "        cls_encoding = x[:,0] # (B,C)\n",
    "        pooled_cls_encoding = self.pooling_activation_fn(self.pooling_linear(cls_encoding)) # (B,C)\n",
    "\n",
    "        return x, pooled_cls_encoding \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will now implement the WordPiece Tokenizer (https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, lets define the special tokens\n",
    "pad_token = \"[PAD]\"\n",
    "unk_token = \"[UNK]\"\n",
    "cls_token = \"[CLS]\"\n",
    "mask_token = \"[MASK]\"\n",
    "sep_token = \"[SEP]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
