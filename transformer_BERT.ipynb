{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import psutil\n",
    "import sys\n",
    "from itertools import islice\n",
    "import string\n",
    "import unicodedata\n",
    "import random \n",
    "random.seed(1234)\n",
    "\n",
    "# use GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A simple/minimal implementation of the BERT model (https://arxiv.org/pdf/1810.04805v2.pdf). \n",
    "\n",
    "The transformer block and multihead attention layer implementations are based on the Andrej Karpathy GPT youtube tutorial. In this case, we use a transformer encoder block which uses bi-directional context, differing from the transformer decoder in GPT which is unidirectional (achieved via causal masking of attention weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, total_head_size, num_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        assert total_head_size % num_heads == 0, \"head_size needs to be integer multiple of num_heads\"\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.total_head_size = total_head_size \n",
    "        self.head_size = total_head_size // num_heads \n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # define parameters\n",
    "        self.key = nn.Linear(embedding_dim, self.total_head_size, bias=False)\n",
    "        self.query = nn.Linear(embedding_dim, self.total_head_size, bias=False)\n",
    "        self.value = nn.Linear(embedding_dim, self.total_head_size, bias=False)\n",
    "        self.attn_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # we also need to apply a linear projection to make the output residual the same dimension as the input\n",
    "        self.proj = nn.Linear(total_head_size, embedding_dim) \n",
    "        self.output_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "\n",
    "    # define forward pass, input shape: (B,T,C) where B=batch size, T=block_size, C=embedding_dim\n",
    "    # the attn_mask is a mask that can be used for masking out the attention weights for padding tokens \n",
    "    def forward(self, x, attn_mask):\n",
    "        B, T, C = x.shape\n",
    "        #print(f\"B = {B}, T={T}, C={C}\")\n",
    "        k = self.key(x) # (B,T,H) where H is the total_head_size\n",
    "        q = self.query(x) # (B,T,H)\n",
    "        v = self.value(x) # (B,T,H)\n",
    "\n",
    "        # reshape (B,T,H) --> (B,T,n,h), where n=num_heads and h=head_size and H=n*h\n",
    "        k = k.view(B,T,self.num_heads,self.head_size) \n",
    "        q = q.view(B,T,self.num_heads,self.head_size) \n",
    "        v = v.view(B,T,self.num_heads,self.head_size) \n",
    "\n",
    "        # now we transpose so that the num_heads is the second dimension followed by T,h\n",
    "        # this allows us to batch matrix mutliply for all heads simulataneously to compute their attention weights\n",
    "        # (B,T,n,h) --> (B,n,T,h) \n",
    "        k = k.transpose(1,2) \n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "\n",
    "        # compute attention scores manually (slower)\n",
    "        W = q @ k.transpose(-2,-1)  / math.sqrt(self.head_size) # (B,n,T,T)\n",
    "        attn_mask = attn_mask.unsqueeze(1).unsqueeze(1)        \n",
    "        #print(f\"W shape= {W.shape}, attn_mask shape = {attn_mask.shape}\")\n",
    "        W = W.masked_fill(attn_mask == 0, float('-inf')) \n",
    "        W = F.softmax(W, dim=-1)\n",
    "        # apply dropout to attention weights\n",
    "        W = self.attn_dropout(W)\n",
    "        out = W @ v # (B,n,T,h)\n",
    "        \n",
    "\n",
    "        # use pytorch built-in function for faster computation of attention scores (set the 'is_causal' parameter for applying causal masking)\n",
    "        #out = F.scaled_dot_product_attention(q,k,v,attn_mask=attn_mask.bool(),dropout_p=self.dropout_rate if self.training else 0,is_causal=False)\n",
    "\n",
    "        # we can transpose the output from (B,n,T,h) --> (B,T,n,h)\n",
    "        # since the last two dimensions of the transposed tensor are non-contiguous, we apply \n",
    "        # contiguous() which return a contiguous tensor\n",
    "        out = out.transpose(1,2).contiguous()\n",
    "\n",
    "        # finally we collapse the last two dimensions to get the concatenated output, (B,T,n,h) --> (B,T,n*h) \n",
    "        out = out.view(B,T,self.total_head_size)\n",
    "\n",
    "        # now we project the concatenated output so that it has the same dimensions as the multihead attention layer input\n",
    "        # (we need to add it with the input because of the residual connection, so need to be same size) \n",
    "        out = self.proj(out) # (B,T,C) \n",
    "\n",
    "        # apply dropout\n",
    "        out = self.output_dropout(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "# a simple mlp \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "        # we add extra computations by growing out the feed-forward hidden size by a factor of 4\n",
    "        # we also add an extra linear layer at the end to project the residual back to same dimensions as input\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 4*embedding_dim),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*embedding_dim, embedding_dim), \n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "    \n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "# transformer encoder block with residual connection and layer norm\n",
    "# Note: the original transformer uses post layer norms, here we use pre layer norms, i.e. layer norm is applied at the input\n",
    "# instead of the output, this typically leads to better results in terms of training convergence speed and gradient scaling \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, head_size, num_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(block_size, embedding_dim, head_size, num_heads, dropout_rate) # multi-head attention layer \n",
    "        self.ff = FeedForward(embedding_dim, dropout_rate)   # feed-forward layer\n",
    "        self.ln1 = nn.LayerNorm(embedding_dim) # layer norm at input of multi-head attention\n",
    "        self.ln2 = nn.LayerNorm(embedding_dim) # layer norm at input of feed-forward\n",
    "\n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x, attn_mask):\n",
    "        # residual connection between input and multi-head attention output (also note that we're doing a pre-layer norm, i.e. layer norm at the input of the multi-head attention)\n",
    "        x = x + self.sa(self.ln1(x), attn_mask)\n",
    "        # residual connection between multi-head attention output and feed-forward output (also note that we're doing a pre-layer norm, i.e. layer norm at the input of the feed-forward)\n",
    "        x = x + self.ff(self.ln2(x)) \n",
    "        return x\n",
    "    \n",
    "\n",
    "# BERT model with multiple transformer blocks \n",
    "class BERTModel(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, embedding_dim, head_size, num_heads, num_blocks, pad_token_id, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size        # block_size is just the input sequence length\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_size = head_size\n",
    "        self.hum_heads = num_heads\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        '''\n",
    "        Define model parameters\n",
    "        '''\n",
    "        # token embedding layer \n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_token_id) # shape: (vocab_size,C)\n",
    "        # position embedding layer\n",
    "        self.pos_embedding = nn.Embedding(block_size, embedding_dim) # shape: (T,C)\n",
    "        # segment embedding layer (disabled for now)\n",
    "        #self.segment_embedding = nn.Embedding(2, embedding_dim)\n",
    "\n",
    "        # stack of transformer blocks\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(block_size, embedding_dim, head_size, num_heads, dropout_rate) for _ in range(num_blocks)])\n",
    "\n",
    "        # pooling transformation of CLS token (for downstream tasks requiring full sentence hidden representation)\n",
    "        #self.pooling_linear = nn.Linear(embedding_dim, embedding_dim) # shape: (C,C)\n",
    "        #self.pooling_activation_fn = nn.Tanh()\n",
    "\n",
    "        # output layer\n",
    "        self.ln = nn.LayerNorm(embedding_dim)\n",
    "        self.output_linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "\n",
    "        # store position indices inside a buffer for fast access when computing position embeddings\n",
    "        position_idx = torch.arange(block_size, device=device).unsqueeze(0)\n",
    "        self.register_buffer('position_idx', position_idx)\n",
    "\n",
    "\n",
    "        # forward pass takes in a batch of input token sequences idx of shape (B,T) and corresponding targets of shape (B,T)\n",
    "    def forward(self, idx, attn_mask, targets=None, segment_idx=None):\n",
    "        B, T = idx.shape\n",
    "        # get token embeddings\n",
    "        token_embeds = self.token_embedding(idx) # (B,T,C)\n",
    "        # add positional encoding\n",
    "        pos_embeds = self.pos_embedding(self.position_idx[:,:T]) # (T,C) \n",
    "        \n",
    "        # add sentence segment embedding (disabled for now)\n",
    "        # segment_embeds = self.segment_embedding(segment_idx) # segment_idx is an integer tensor of shape (B,T) and has 0's at positions corresponding to \n",
    "        \n",
    "        # the first sentence and 1's at positions corresponding to the second sentence \n",
    "        x = token_embeds + pos_embeds # (B,T,C)\n",
    "        # pass through transformer blocks to get encoding\n",
    "        for block in self.blocks:\n",
    "            x = block(x, attn_mask) # (B,T,C)\n",
    "    \n",
    "        # get CLS token encoding and apply pooling transform\n",
    "        #cls_encoding = x[:,0] # (B,C)\n",
    "        #pooled_cls_encoding = self.pooling_activation_fn(self.pooling_linear(cls_encoding)) # (B,C)\n",
    "\n",
    "        # apply final layers norm\n",
    "        x = self.ln(x)\n",
    "\n",
    "        # compute output logits\n",
    "        logits = self.output_linear(self.dropout(x))\n",
    "\n",
    "        return logits \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use the WordPiece Tokenizer with the vocabulary used for pre-training the original BERT (https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# load the books corpus dataset from file\n",
    "dataset = Dataset.from_file('book_corpus_dataset/archive/train/dataset.arrow')\n",
    "\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "# check how many sentences are in the dataset\n",
    "print(len(dataset))\n",
    "\n",
    "# show some sentences from the dataset\n",
    "print(dataset[:10]['text'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For simplicity, we will work with a small subset of the full corpus (the first 10M sentences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.05016326904297"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# get chunk of 10M sentences\n",
    "chunk = dataset[:10000000]['text']\n",
    "\n",
    "# size in Mb\n",
    "sys.getsizeof(chunk)/1024**2\n",
    "\n",
    "# save it to a txt file\n",
    "with open('bookcorpus_small.txt', 'w') as output:\n",
    "    for sent in chunk:\n",
    "        output.write(f\"{sent}\\n\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_first_n_lines(n):\n",
    "    with open(\"bookcorpus_small.txt\", 'r') as file:\n",
    "        return list(islice(file, n))\n",
    "    \n",
    "# read in 100000 sentences from the txt file\n",
    "text = read_first_n_lines(100000)\n",
    "text = [line.strip() for line in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in reduced corpus: 100000\n",
      "RAM used: 475.09 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of sentences in reduced corpus: {len(text)}\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordPiece Tokenizer Algorithm Implementation (https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# pretokenize the corpus into words and get unigram counts, we will only use the first sentence as an example\n",
    "word_freqs = defaultdict(int)\n",
    "for s in text:\n",
    "    words = s.split()\n",
    "    for word in words:\n",
    "        word_freqs[word] += 1\n",
    "    break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'the': 2,\n",
       "             'half-ling': 1,\n",
       "             'book': 1,\n",
       "             'one': 1,\n",
       "             'in': 1,\n",
       "             'fall': 1,\n",
       "             'of': 1,\n",
       "             'igneeria': 1,\n",
       "             'series': 1,\n",
       "             'kaylee': 2,\n",
       "             'soderburg': 2,\n",
       "             'copyright': 1,\n",
       "             '2013': 1,\n",
       "             'all': 1,\n",
       "             'rights': 1,\n",
       "             'reserved': 1,\n",
       "             '.': 1})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we initialize the WordPiece vocabulary. This contains all the unique first letters of every word in the corpus and all other letters that appear in words prefixed by '##'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['##-', '##0', '##1', '##3', '##a', '##b', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##y', '.', '2', '[CLS]', '[MASK]', '[PAD]', '[SEP]', '[UNK]', 'a', 'b', 'c', 'f', 'h', 'i', 'k', 'o', 'r', 's', 't']\n"
     ]
    }
   ],
   "source": [
    "# initialize WordPiece vocabulary\n",
    "vocab = []\n",
    "for word in word_freqs.keys():\n",
    "    if word[0] not in vocab:\n",
    "        vocab.append(word[0])\n",
    "    for letter in word[1:]:\n",
    "        prefixed = '##' + letter\n",
    "        if prefixed not in vocab:\n",
    "            vocab.append(prefixed)\n",
    "\n",
    "# now add the special tokens\n",
    "vocab = vocab + [\"[PAD]\", \"[CLS]\", \"[UNK]\", \"[MASK]\", \"[SEP]\"]\n",
    "\n",
    "vocab = sorted(vocab)         \n",
    "print(vocab) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split every unique word in corpus into characters and prefix the characters which are not the first with '##'\\\n",
    "e.g. 'word' --> 'w', '##o', '##r', '##d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': ['t', '##h', '##e'],\n",
       " 'half-ling': ['h', '##a', '##l', '##f', '##-', '##l', '##i', '##n', '##g'],\n",
       " 'book': ['b', '##o', '##o', '##k'],\n",
       " 'one': ['o', '##n', '##e'],\n",
       " 'in': ['i', '##n'],\n",
       " 'fall': ['f', '##a', '##l', '##l'],\n",
       " 'of': ['o', '##f'],\n",
       " 'igneeria': ['i', '##g', '##n', '##e', '##e', '##r', '##i', '##a'],\n",
       " 'series': ['s', '##e', '##r', '##i', '##e', '##s'],\n",
       " 'kaylee': ['k', '##a', '##y', '##l', '##e', '##e'],\n",
       " 'soderburg': ['s', '##o', '##d', '##e', '##r', '##b', '##u', '##r', '##g'],\n",
       " 'copyright': ['c', '##o', '##p', '##y', '##r', '##i', '##g', '##h', '##t'],\n",
       " '2013': ['2', '##0', '##1', '##3'],\n",
       " 'all': ['a', '##l', '##l'],\n",
       " 'rights': ['r', '##i', '##g', '##h', '##t', '##s'],\n",
       " 'reserved': ['r', '##e', '##s', '##e', '##r', '##v', '##e', '##d'],\n",
       " '.': ['.']}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = {word: [c if i==0 else f\"##{c}\" for i,c in enumerate(word)] for word in word_freqs.keys()}\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will iteratively merge pairs of these splits into subword tokens. To select which pair to merge, we compute a score for all observed pairs as follows:\n",
    "\n",
    "$score(c1,c2) = \\frac{freq(c1,c2)}{freq(c1) * freq(c2)}$\n",
    "\n",
    "where c1 and c2 are a pair of splits, freq(c1,c2) is the count of how many times we observe c1,c2 adjacent in the corpus and freq(c) is the count of how many times we observe c. This merger algorithm trherefore prioritizes merging splits which appear together frequently but appear separately more rarely.\n",
    "\n",
    "\n",
    "For example, \n",
    "\n",
    "freq('t', '##h') = 2 (observed in the word 'the':2) and \n",
    "\n",
    "freq('t') = 2 (observed in the word 'the':2)\n",
    "\n",
    "freq('##h') = 4 (observed in words 'the':2, 'copyright':1,'rights':1)  \n",
    "\n",
    "score('t', '##h') = 1/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for computing pair scores\n",
    "def compute_pair_scores(splits):\n",
    "    letter_freqs = defaultdict(int)\n",
    "    pair_freqs = defaultdict(int)\n",
    "\n",
    "    for word,freq in word_freqs.items():            \n",
    "        split = splits[word]\n",
    "        # if word only contains one split\n",
    "        if len(split) == 1:\n",
    "            letter_freqs[split[0]] += freq\n",
    "            continue\n",
    "        \n",
    "        # count up every individual split and adjacent pair of splits \n",
    "        for i in range(len(split)-1):\n",
    "            pair = (split[i], split[i+1])\n",
    "            letter_freqs[split[i]] += freq\n",
    "            pair_freqs[pair] += freq\n",
    "        letter_freqs[split[-1]] += freq\n",
    "\n",
    "    scores = {pair: freq/(letter_freqs[pair[0]]*letter_freqs[pair[1]]) for pair,freq in pair_freqs.items()}\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('t', '##h'): 0.25, ('##h', '##e'): 0.03125, ('h', '##a'): 0.2, ('##a', '##l'): 0.05, ('##l', '##f'): 0.0625, ('##f', '##-'): 0.5, ('##-', '##l'): 0.125, ('##l', '##i'): 0.025, ('##i', '##n'): 0.05, ('##n', '##g'): 0.041666666666666664, ('b', '##o'): 0.2, ('##o', '##o'): 0.04, ('##o', '##k'): 0.2, ('o', '##n'): 0.125, ('##n', '##e'): 0.03125, ('i', '##n'): 0.125, ('f', '##a'): 0.2, ('##l', '##l'): 0.03125, ('o', '##f'): 0.25, ('i', '##g'): 0.08333333333333333, ('##g', '##n'): 0.041666666666666664, ('##e', '##e'): 0.01171875, ('##e', '##r'): 0.0390625, ('##r', '##i'): 0.075, ('##i', '##a'): 0.04, ('s', '##e'): 0.020833333333333332, ('##i', '##e'): 0.0125, ('##e', '##s'): 0.041666666666666664, ('k', '##a'): 0.2, ('##a', '##y'): 0.13333333333333333, ('##y', '##l'): 0.08333333333333333, ('##l', '##e'): 0.015625, ('s', '##o'): 0.13333333333333333, ('##o', '##d'): 0.13333333333333333, ('##d', '##e'): 0.041666666666666664, ('##r', '##b'): 0.125, ('##b', '##u'): 0.5, ('##u', '##r'): 0.125, ('##r', '##g'): 0.041666666666666664, ('c', '##o'): 0.2, ('##o', '##p'): 0.2, ('##p', '##y'): 0.3333333333333333, ('##y', '##r'): 0.041666666666666664, ('##i', '##g'): 0.06666666666666667, ('##g', '##h'): 0.08333333333333333, ('##h', '##t'): 0.25, ('2', '##0'): 1.0, ('##0', '##1'): 1.0, ('##1', '##3'): 1.0, ('a', '##l'): 0.125, ('r', '##i'): 0.1, ('##t', '##s'): 0.16666666666666666, ('r', '##e'): 0.03125, ('##s', '##e'): 0.020833333333333332, ('##r', '##v'): 0.125, ('##v', '##e'): 0.0625, ('##e', '##d'): 0.020833333333333332}\n",
      "Pair with largest score:  ('2', '##0')\n"
     ]
    }
   ],
   "source": [
    "pair_scores = compute_pair_scores(splits)\n",
    "print(pair_scores)\n",
    "\n",
    "# get pair with largest score (ties broken by picking first occurance of largest value)\n",
    "max_score_pair = max(pair_scores, key = lambda x: pair_scores[x])\n",
    "print(\"Pair with largest score: \", max_score_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we merge these two splits into a new subword token and add the subword to our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['##-', '##0', '##1', '##3', '##a', '##b', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##y', '.', '2', '[CLS]', '[MASK]', '[PAD]', '[SEP]', '[UNK]', 'a', 'b', 'c', 'f', 'h', 'i', 'k', 'o', 'r', 's', 't', '20']\n"
     ]
    }
   ],
   "source": [
    "subword = max_score_pair[0] + max_score_pair[1].lstrip('#')\n",
    "vocab.append(subword)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(c1, c2, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) > 1:\n",
    "            i = 0\n",
    "            while i < len(split)-1:\n",
    "                if split[i] == c1 and split[i+1] == c2:\n",
    "                    merged = c1 + c2.lstrip('#')\n",
    "                    split = split[:i] + [merged] + split[i+2:]\n",
    "                else:\n",
    "                    i += 1\n",
    "                splits[word] = split\n",
    "\n",
    "    return splits                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': ['t', '##h', '##e'],\n",
       " 'half-ling': ['h', '##a', '##l', '##f', '##-', '##l', '##i', '##n', '##g'],\n",
       " 'book': ['b', '##o', '##o', '##k'],\n",
       " 'one': ['o', '##n', '##e'],\n",
       " 'in': ['i', '##n'],\n",
       " 'fall': ['f', '##a', '##l', '##l'],\n",
       " 'of': ['o', '##f'],\n",
       " 'igneeria': ['i', '##g', '##n', '##e', '##e', '##r', '##i', '##a'],\n",
       " 'series': ['s', '##e', '##r', '##i', '##e', '##s'],\n",
       " 'kaylee': ['k', '##a', '##y', '##l', '##e', '##e'],\n",
       " 'soderburg': ['s', '##o', '##d', '##e', '##r', '##b', '##u', '##r', '##g'],\n",
       " 'copyright': ['c', '##o', '##p', '##y', '##r', '##i', '##g', '##h', '##t'],\n",
       " '2013': ['20', '##1', '##3'],\n",
       " 'all': ['a', '##l', '##l'],\n",
       " 'rights': ['r', '##i', '##g', '##h', '##t', '##s'],\n",
       " 'reserved': ['r', '##e', '##s', '##e', '##r', '##v', '##e', '##d'],\n",
       " '.': ['.']}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = merge_pair(max_score_pair[0], max_score_pair[1], splits)\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can keep merging iteratively until we have reahed some maximum vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 50\n",
    "\n",
    "while len(vocab) < max_vocab_size:\n",
    "    # compute all pair scores\n",
    "    pair_scores = compute_pair_scores(splits)\n",
    "    # get pair with largest score (ties broken by picking first occurance of largest value)\n",
    "    max_score_pair = max(pair_scores, key = lambda x: pair_scores[x])\n",
    "    # add new subword to vacabulary\n",
    "    subword = max_score_pair[0] + max_score_pair[1].lstrip('#')\n",
    "    vocab.append(subword)\n",
    "    # update splits \n",
    "    splits = merge_pair(*max_score_pair, splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['##-', '##0', '##1', '##3', '##a', '##b', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##y', '.', '2', '[CLS]', '[MASK]', '[PAD]', '[SEP]', '[UNK]', 'a', 'b', 'c', 'f', 'h', 'i', 'k', 'o', 'r', 's', 't', '20', '201', '2013', '##f-', 'of', '##bu', '##py', 'th', '##ht']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we tokenize a given word into the learned subwords. To tokenize a word, we find the longest matching subword starting from the first character and we split the word. Then we repeat the process from the second half. If we reach the fiunal character and haven't found a matching subword from the vocabulary, then we declare the entire word as '[UNK]' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_word(word):\n",
    "    tokens = []\n",
    "    while len(word) > 0:\n",
    "        i = len(word)    \n",
    "        # find longest mactching subword subword\n",
    "        while i > 0 and word[:i] not in vocab:\n",
    "            i -= 1\n",
    "        if i == 0:\n",
    "            # no match found\n",
    "            return [\"[UNK]\"]\n",
    "        # found longest subword\n",
    "        tokens.append(word[:i])\n",
    "        word = word[i:]\n",
    "        # add prefix\n",
    "        if len(word) > 0:\n",
    "            word = f\"##{word}\"\n",
    "    return tokens            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f', '##a', '##bu', '##l', '##o', '##u', '##s']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_word(\"fabulous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All together, the WordPiece vocab generator can be implemented as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPieceTokenizer():\n",
    "    def __init__(self):\n",
    "        self.vocab = []\n",
    "        self.word2int = {}\n",
    "        self.int2word = {}\n",
    "        # special tokens\n",
    "        self.pad_token = \"[PAD]\"\n",
    "        self.mask_token = \"[MASK]\"\n",
    "        self.unk_token = \"[UNK]\"\n",
    "        self.cls_token = \"[CLS]\"\n",
    "        self.sep_token = \"[SEP]\"\n",
    "\n",
    "\n",
    "    def mask_token_id(self):\n",
    "        return self.word2int[self.mask_token]\n",
    "\n",
    "    def pad_token_id(self):\n",
    "        return self.word2int[self.pad_token]\n",
    "\n",
    "    def cls_token_id(self):\n",
    "        return self.word2int[self.cls_token]\n",
    "\n",
    "    def unk_token_id(self):\n",
    "        return self.word2int[self.unk_token]\n",
    "\n",
    "    def sep_token_id(self):\n",
    "        return self.word2int[self.sep_token]\n",
    "\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "\n",
    "    # generates wordpiece vocabulary of subwords from a given corpus\n",
    "    # the input corpus is a list of sentences\n",
    "    def generate_vocab(self, corpus, max_vocab_size):\n",
    "    \n",
    "        # pretokenize the corpus into words and get unigram counts, we will only use the first sentence as an example\n",
    "        word_freqs = defaultdict(int)\n",
    "        for s in corpus:\n",
    "\n",
    "            # first clean the sentence, i.e. remove control characters and punctuation\n",
    "            #s = \"\".join(ch for ch in s if unicodedata.category(ch)[0] not in ('C', 'P')) \n",
    "\n",
    "            # remove all non-letter characters\n",
    "            s = \"\".join(ch for ch in s if unicodedata.category(ch)[0] == 'L') \n",
    "\n",
    "            words = s.split()\n",
    "            for word in words:\n",
    "                word_freqs[word] += 1\n",
    "        \n",
    "        # initialize WordPiece vocabulary\n",
    "        for word in word_freqs.keys():\n",
    "            if word[0] not in self.vocab:\n",
    "                self.vocab.append(word[0])\n",
    "            for letter in word[1:]:\n",
    "                prefixed = '##' + letter\n",
    "                if prefixed not in self.vocab:\n",
    "                    self.vocab.append(prefixed)\n",
    "\n",
    "        # now add special tokens\n",
    "        self.vocab = self.vocab + [self.pad_token, self.cls_token, self.unk_token, self.mask_token, self.sep_token]\n",
    "\n",
    "        # generate splits\n",
    "        splits = {word: [c if i==0 else f\"##{c}\" for i,c in enumerate(word)] for word in word_freqs.keys()}\n",
    "        \n",
    "        # function for computing pair scores\n",
    "        def compute_pair_scores(splits):\n",
    "            letter_freqs = defaultdict(int)\n",
    "            pair_freqs = defaultdict(int)\n",
    "\n",
    "            for word,freq in word_freqs.items():            \n",
    "                split = splits[word]\n",
    "                # if word only contains one split\n",
    "                if len(split) == 1:\n",
    "                    letter_freqs[split[0]] += freq\n",
    "                    continue\n",
    "                \n",
    "                # count up every individual split and adjacent pair of splits \n",
    "                for i in range(len(split)-1):\n",
    "                    pair = (split[i], split[i+1])\n",
    "                    letter_freqs[split[i]] += freq\n",
    "                    pair_freqs[pair] += freq\n",
    "                letter_freqs[split[-1]] += freq\n",
    "\n",
    "            scores = {pair: freq/(letter_freqs[pair[0]]*letter_freqs[pair[1]]) for pair,freq in pair_freqs.items()}\n",
    "            return scores\n",
    "        \n",
    "        # function for merging a pair of splits    \n",
    "        def merge_pair(c1, c2, splits):\n",
    "            for word in word_freqs:\n",
    "                split = splits[word]\n",
    "                if len(split) > 1:\n",
    "                    i = 0\n",
    "                    while i < len(split)-1:\n",
    "                        if split[i] == c1 and split[i+1] == c2:\n",
    "                            merged = c1 + c2.lstrip('#')\n",
    "                            split = split[:i] + [merged] + split[i+2:]\n",
    "                        else:\n",
    "                            i += 1\n",
    "                        splits[word] = split\n",
    "\n",
    "            return splits    \n",
    "        \n",
    "        # generate the subword vocabulary\n",
    "        while len(self.vocab) < max_vocab_size:\n",
    "            # compute all pair scores\n",
    "            pair_scores = compute_pair_scores(splits)\n",
    "            # get pairs with largest score \n",
    "            max_score = max(pair_scores.values())\n",
    "            max_score_pairs = [pair for pair, score in pair_scores.items() if score== max_score]\n",
    "            # randomly break ties\n",
    "            max_score_pair = random.choice(max_score_pairs)\n",
    "            # add new subword to vacabulary\n",
    "            subword = max_score_pair[0] + max_score_pair[1].lstrip('#')\n",
    "            self.vocab.append(subword)\n",
    "            # update splits \n",
    "            splits = merge_pair(*max_score_pair, splits)\n",
    "            \n",
    "            if max_vocab_size % len(self.vocab) == 4:\n",
    "                print(f\"Building vocab.. current vocab length = {len(self.vocab)}\")\n",
    "            \n",
    "        self.vocab = sorted(set(self.vocab))\n",
    "        self.word2int = {word:i for i,word in enumerate(self.vocab)}\n",
    "        self.int2word = {i:word for i,word in enumerate(self.vocab)}\n",
    "\n",
    "\n",
    "    # encode sentence into subword token indices\n",
    "    def encode(self, sentences):\n",
    "        encoded_sentences = []\n",
    "        for s in sentences:\n",
    "            # first tokenize the sentence into subword sequence\n",
    "            subword_tokens = self.tokenize_sentence(s)\n",
    "            # convert to token indices\n",
    "            indices = [self.word2int[t] for t in subword_tokens]\n",
    "            encoded_sentences.append(indices)\n",
    "        return encoded_sentences\n",
    "\n",
    "    # decode subword token index sequences back to sentences\n",
    "    def decode(self, idx):\n",
    "        sentences = []\n",
    "        for indices in idx:\n",
    "            # first cnvert indices to subword tokens\n",
    "            subwords = [self.int2word[ix] for ix in indices]\n",
    "\n",
    "            # merge subwords\n",
    "            i = 0\n",
    "            while i < len(subwords)-1:\n",
    "                a = subwords[i]\n",
    "                b = subwords[i+1]\n",
    "                if len(b) == 1:\n",
    "                    i += 1  \n",
    "                    continue\n",
    "                if b[:2]==\"##\":\n",
    "                    subwords = subwords[:i] + [a+b.lstrip('#')] + subwords[i+2:]\n",
    "                else:\n",
    "                    i += 1    \n",
    "\n",
    "            s = \" \".join(subwords)\n",
    "            sentences.append(s)\n",
    "        \n",
    "        return sentences\n",
    "\n",
    "\n",
    "    def tokenize_sentence(self, sent):\n",
    "        tokens = []\n",
    "        # split the sentence into words \n",
    "        # make sure to convert all characters to lower case because our vocabulary does not contain\n",
    "        # upper case letters\n",
    "        words = sent.lower().split()\n",
    "        # tokenize each word\n",
    "        for word in words:\n",
    "            tokens = tokens + self.tokenize_word(word)\n",
    "        return tokens\n",
    "\n",
    "    def tokenize_word(self, word):\n",
    "        tokens = []\n",
    "        while len(word) > 0:\n",
    "            i = len(word)    \n",
    "            # find longest mactching subword subword\n",
    "            while i > 0 and word[:i] not in self.vocab:\n",
    "                i -= 1\n",
    "            if i == 0:\n",
    "                # no match found\n",
    "                return [self.unk_token]\n",
    "            # found longest subword\n",
    "            tokens.append(word[:i])\n",
    "            word = word[i:]\n",
    "            # add prefix\n",
    "            if len(word) > 0:\n",
    "                word = f\"##{word}\"\n",
    "        return tokens          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate tokenizer object\n",
    "tokenizer = WordPieceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocab: current vocab lengt = 60\n",
      "Building vocab: current vocab lengt = 68\n",
      "Building vocab: current vocab lengt = 85\n",
      "Building vocab: current vocab lengt = 102\n",
      "Building vocab: current vocab lengt = 170\n",
      "Building vocab: current vocab lengt = 204\n",
      "Building vocab: current vocab lengt = 255\n",
      "Building vocab: current vocab lengt = 340\n"
     ]
    }
   ],
   "source": [
    "# generate vocabulary\n",
    "tokenizer.generate_vocab(text, max_vocab_size=1024)\n",
    "\n",
    "print(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we sat at the table and emily titled the page : warnings .\n",
      "[[55], [36], [], [51], [32], [52], [], [32], [52], [], [52], [40], [36], [], [52], [32], [33], [44], [36], [], [32], [46], [35], [], [36], [45], [41], [44], [57], [], [52], [41], [52], [44], [36], [35], [], [52], [40], [36], [], [48], [32], [39], [36], [], [31], [], [55], [32], [50], [46], [41], [46], [39], [51], [], [31]]\n",
      "['w', 'e', '', 's', 'a', 't', '', 'a', 't', '', 't', 'h', 'e', '', 't', 'a', 'b', 'l', 'e', '', 'a', 'n', 'd', '', 'e', 'm', 'i', 'l', 'y', '', 't', 'i', 't', 'l', 'e', 'd', '', 't', 'h', 'e', '', 'p', 'a', 'g', 'e', '', '[UNK]', '', 'w', 'a', 'r', 'n', 'i', 'n', 'g', 's', '', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "# test encoding and decoding a sentence\n",
    "s = text[500]\n",
    "print(s)\n",
    "encoded = tokenizer.encode(s)\n",
    "print(encoded)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(decoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets encode the dataset into integer token sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_encoded = tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch Dataset API for prepping data for Masked Language Model Task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, corpus, tokenizer, block_size, mlm_prob=0.15):\n",
    "        self.corpus = corpus          # encoded sentences\n",
    "        self.tokenizer = tokenizer    # wordpiece tokenizer\n",
    "        self.block_size = block_size  # truncation/max length of sentences\n",
    "        self.corpus_len = len(corpus) # size of corpus\n",
    "        self.mlm_prob = mlm_prob\n",
    "        self.vocab_size = tokenizer.vocab_size()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.corpus_len\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get the sentence \n",
    "        s = self.get_corpus_sentence(idx)\n",
    "        # truncate to block_size-1\n",
    "        s = s[:self.block_size] \n",
    "        s_len = len(s)\n",
    "        \n",
    "        # replace tokens randomly\n",
    "        s, label = self.replace_tokens(s)\n",
    "        # append the CLS token at the beginning of sentence\n",
    "        s = [self.tokenizer.cls_token_id()] + s\n",
    "        # apply padding\n",
    "        pad_len = max(0,self.block_size-s_len-1)\n",
    "        s = s + [self.tokenizer.pad_token_id()]*pad_len\n",
    "        label = [-100] + label + [-100]*pad_len\n",
    "\n",
    "        # convert to torch tensors\n",
    "        s = torch.tensor(s)\n",
    "        label = torch.tensor(label)\n",
    "\n",
    "        # Note: Unlike the original BERT, we are not returning a pair of sentences, so we\n",
    "        # don't need to return segment labels or next_sentence label \n",
    "        return {\"masked_input\" : s, \"label\" : label}\n",
    "\n",
    "\n",
    "    # randomly replace tokens with mlm_prob probability\n",
    "    def replace_tokens(self, s):\n",
    "        # the labels for a masked token is the original token index and -100 for non-masked tokens\n",
    "        label = [-100] * len(s)\n",
    "        for i,t in enumerate(s):\n",
    "            p = random.random()    \n",
    "            if p < self.mlm_prob:\n",
    "                p = p/self.mlm_prob\n",
    "                # replace with masked token with 80% probability\n",
    "                if p < 0.8:\n",
    "                    s[i] = self.tokenizer.mask_token_id()        \n",
    "\n",
    "                elif p < 0.9:\n",
    "                    # replace with random token with 10% probability \n",
    "                    s[i] = random.randrange(self.vocab_size)\n",
    "                label[i] = t\n",
    "\n",
    "        return s, label\n",
    "\n",
    "\n",
    "    def get_corpus_sentence(self, idx):\n",
    "        return self.corpus[idx]        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoader for generating batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of batches: 25000\n"
     ]
    }
   ],
   "source": [
    "block_size = 128\n",
    "batch_size = 4\n",
    "\n",
    "train_dataset = BERTDataset(dataset_encoded, tokenizer, block_size=block_size)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4)  # set pin_memory for faster pre-fetching \n",
    "print(f\"Total number of batches: {len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'masked_input': tensor([[ 89, 117,  34,  31,   7,  31,  90,  44,  31, 116,  46,  35,  38,  38,\n",
       "          103,  35,  45,  34,  31,  44,  39,  31,  40, 120,  34,  41, 116,  27,\n",
       "           35,  38, 112,  47,  46, 106,  40, 117,  34,  31,  35,  44,  99,  41,\n",
       "           27,  46,  45,  90,  44,  90,  39,  63,  90,  31, 105,  27,  44,  28,\n",
       "           41,  47,  44,  90,  27,  29,  34, 101,  90,  51,  68,  91,  91,  91,\n",
       "           91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,\n",
       "           91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,\n",
       "           91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,\n",
       "           91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,\n",
       "           91,  91],\n",
       "         [ 89, 106,  46, 120,  90,  45, 115,  31,  27,  90,  38,  51,  98,  99,\n",
       "           31,  27,  47,  90,  35,  32,  47,  38, 101,  90,  90,  68,  91,  91,\n",
       "           91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,\n",
       "           91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,\n",
       "           91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,\n",
       "           91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,\n",
       "           91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,\n",
       "           91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,\n",
       "           91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,\n",
       "           91,  91],\n",
       "         [ 89, 111,  41,  49,  90,  34,  31, 120,  27,  45,  99,  90,  47,  28,\n",
       "           90,  31,  44,  35,  40,  33,  90,  99,  47,  46, 117,  34,  31,  90,\n",
       "           90,  90,  41,  44,  51,  90,  32, 102,  48,  90,  44,  51,  90,  34,\n",
       "           35,  40,  33, 120,  27,  45, 117,  27,  37,  35,  40,  33, 112,  48,\n",
       "           31,  44,  81, 117,  34,  90, 117,  34,  35,  90,  33,  45, 109,  52,\n",
       "           27,  44,  41, 116,  27,  35,  30,  68,  91,  91,  91,  91,  91,  91,\n",
       "           91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,\n",
       "           91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,\n",
       "           91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,\n",
       "           91,  91],\n",
       "         [ 89, 107,  47,  45,  46, 117,  31,  90,  38, 110,  31, 122,  90,  47,\n",
       "          105,  27,  48,  31, 117,  34,  31, 116,  47,  42,  31,  44, 100,  34,\n",
       "           35,  42,   0,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,\n",
       "           91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,\n",
       "           91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,\n",
       "           91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,\n",
       "           91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,\n",
       "           91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,\n",
       "           91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,\n",
       "           91,  91]]),\n",
       " 'label': tensor([[-100, -100, -100, -100,   44, -100,   98, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100,  103, -100,   41,\n",
       "          -100,  117,   34, -100, -100, -100, -100, -100, -100, -100, -100,  102,\n",
       "          -100, -100, -100, -100,   27, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100],\n",
       "         [-100, -100, -100, -100,   27, -100, -100, -100, -100,   38, -100, -100,\n",
       "          -100, -100, -100, -100, -100,   46, -100, -100, -100, -100, -100,   27,\n",
       "            51, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100],\n",
       "         [-100, -100, -100, -100,  116, -100, -100, -100, -100, -100, -100,   38,\n",
       "          -100, -100,   28, -100, -100, -100, -100,   33,   66, -100, -100, -100,\n",
       "          -100, -100, -100,  110,   31,   39, -100, -100, -100,  112, -100, -100,\n",
       "          -100,   31, -100, -100,   46, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100,   31, -100, -100, -100,   40, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100],\n",
       "         [-100, -100, -100, -100, -100, -100, -100,   38, -100, -100, -100, -100,\n",
       "            41, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100]])}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load an example batch and show the contents\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "sample_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 128])\n",
      "torch.Size([4, 128])\n"
     ]
    }
   ],
   "source": [
    "print(sample_batch['masked_input'].shape)\n",
    "print(sample_batch['label'].shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
