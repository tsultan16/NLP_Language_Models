{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Language Model (based on Adrej Karpathy's nanoGPT tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1234)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the txt entire file into a single string\n",
    "with open('tinyshakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of dataset in characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total length of dataset in characters: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character vocabulary:  ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# get vocabulary of characters\n",
    "vocab = sorted(set(list(text)))\n",
    "print(\"character vocabulary: \", vocab)\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text\n",
    "ctoi = {vocab[i]:i for i in range(vocab_size)}\n",
    "itoc = {i:vocab[i] for i in range(vocab_size)}\n",
    "encode = lambda s: [ctoi[c] for c in s]  # converts a string to integer token sequence\n",
    "decode = lambda s: [itoc[ix] for ix in s]  # converts an integer token sequence to string of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42, 2]\n",
      "['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!']\n"
     ]
    }
   ],
   "source": [
    "print(encode('Hello world!'))\n",
    "print(decode(encode('Hello world!')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "# tokenize the dataset into integer sequence, convert to torch tensor of type int64\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long) \n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-validation splits (90-10)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split the data into chuncks of size block_size. For each chunk, we create (input,target) pairs for next character prediction, where the input is a context window containing all characters preceding the target character. Note that the context sizes range from 1 up to block size, i.e. there will be block_size number of (input,target) pairs per chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: tensor([18]) --> target: 47\n",
      "Context: tensor([18, 47]) --> target: 56\n",
      "Context: tensor([18, 47, 56]) --> target: 57\n",
      "Context: tensor([18, 47, 56, 57]) --> target: 58\n",
      "Context: tensor([18, 47, 56, 57, 58]) --> target: 1\n",
      "Context: tensor([18, 47, 56, 57, 58,  1]) --> target: 15\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15]) --> target: 47\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15, 47]) --> target: 58\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "\n",
    "# example showing the first chunk and all possible (input,target) pairs we can get from it\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"Context: {context} --> target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create a batch generator which creates a batch of randomly selected blocks/chunks from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input batch: \n",
      "torch.Size([4, 8])\n",
      "tensor([[17, 17, 26,  1, 17, 24, 21, 38],\n",
      "        [14, 17, 24, 24, 13, 10,  0, 28],\n",
      "        [63,  1, 47, 52,  1, 56, 43, 55],\n",
      "        [56, 59, 57, 58,  1, 59, 54, 53]], device='cuda:0')\n",
      "target batch: \n",
      "torch.Size([4, 8])\n",
      "tensor([[17, 26,  1, 17, 24, 21, 38, 13],\n",
      "        [17, 24, 24, 13, 10,  0, 28, 50],\n",
      "        [ 1, 47, 52,  1, 56, 43, 55, 59],\n",
      "        [59, 57, 58,  1, 59, 54, 53, 52]], device='cuda:0')\n",
      "\n",
      "A batch of 4 blocks:\n",
      "\n",
      "Block 0:\n",
      "Context: [17] --> target: 17\n",
      "Context: [17, 17] --> target: 26\n",
      "Context: [17, 17, 26] --> target: 1\n",
      "Context: [17, 17, 26, 1] --> target: 17\n",
      "Context: [17, 17, 26, 1, 17] --> target: 24\n",
      "Context: [17, 17, 26, 1, 17, 24] --> target: 21\n",
      "Context: [17, 17, 26, 1, 17, 24, 21] --> target: 38\n",
      "Context: [17, 17, 26, 1, 17, 24, 21, 38] --> target: 13\n",
      "\n",
      "\n",
      "Block 1:\n",
      "Context: [14] --> target: 17\n",
      "Context: [14, 17] --> target: 24\n",
      "Context: [14, 17, 24] --> target: 24\n",
      "Context: [14, 17, 24, 24] --> target: 13\n",
      "Context: [14, 17, 24, 24, 13] --> target: 10\n",
      "Context: [14, 17, 24, 24, 13, 10] --> target: 0\n",
      "Context: [14, 17, 24, 24, 13, 10, 0] --> target: 28\n",
      "Context: [14, 17, 24, 24, 13, 10, 0, 28] --> target: 50\n",
      "\n",
      "\n",
      "Block 2:\n",
      "Context: [63] --> target: 1\n",
      "Context: [63, 1] --> target: 47\n",
      "Context: [63, 1, 47] --> target: 52\n",
      "Context: [63, 1, 47, 52] --> target: 1\n",
      "Context: [63, 1, 47, 52, 1] --> target: 56\n",
      "Context: [63, 1, 47, 52, 1, 56] --> target: 43\n",
      "Context: [63, 1, 47, 52, 1, 56, 43] --> target: 55\n",
      "Context: [63, 1, 47, 52, 1, 56, 43, 55] --> target: 59\n",
      "\n",
      "\n",
      "Block 3:\n",
      "Context: [56] --> target: 59\n",
      "Context: [56, 59] --> target: 57\n",
      "Context: [56, 59, 57] --> target: 58\n",
      "Context: [56, 59, 57, 58] --> target: 1\n",
      "Context: [56, 59, 57, 58, 1] --> target: 59\n",
      "Context: [56, 59, 57, 58, 1, 59] --> target: 54\n",
      "Context: [56, 59, 57, 58, 1, 59, 54] --> target: 53\n",
      "Context: [56, 59, 57, 58, 1, 59, 54, 53] --> target: 52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1223)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "# data loader (generates a bvatch of randomly selected blocks)\n",
    "def get_batch(split='train'):\n",
    "    data = train_data if split=='train' else val_data\n",
    "\n",
    "    # sample positions from which to grab blocks\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])      \n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) # move tensors to gpu \n",
    "    return x,y \n",
    "\n",
    "xbatch, ybatch = get_batch('train')\n",
    "print(\"input batch: \")\n",
    "print(xbatch.shape)\n",
    "print(xbatch)\n",
    "print(\"target batch: \")\n",
    "print(ybatch.shape)     \n",
    "print(ybatch)     \n",
    "print(\"\")\n",
    "\n",
    "# context target pairs\n",
    "print(f\"A batch of {batch_size} blocks:\")\n",
    "for b in range(batch_size): # batch dimension\n",
    "    print(f\"\\nBlock {b}:\")\n",
    "    for t in range(block_size):  # time dimension\n",
    "        context = xbatch[b,:t+1]\n",
    "        target = ybatch[b,t]\n",
    "        print(f\"Context: {context.tolist()} --> target: {target}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's create a pytorch-ified Bi-gram language model (will serve as a baseline for comparing the transformer model later on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "batch_size = 64\n",
    "block_size = 8\n",
    "max_iters = 6000\n",
    "learning_rate = 1e-2\n",
    "eval_interval = 300\n",
    "eval_iters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        Define model parameters\n",
    "        '''\n",
    "        # lookup table for finding logits for the next token (i.e. log of counts for all possible next token given input token)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # shape: (C,C)\n",
    "\n",
    "\n",
    "    # forward pass takes in a batch of input token sequences of shape (B,T) and corresponding targets of shape (B,T)\n",
    "    def forward(self, idx, targets=None):\n",
    "        # get logits for every input token\n",
    "        logits = self.token_embedding_table(idx) # shape: (B,T,C)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B,T,C = logits.shape\n",
    "            # reshape the logits and targets such that batch of input sequences are flattened into a single big input sequence\n",
    "            # i.e. (B,T) --> (B*T)\n",
    "            logits = logits.view(B*T,C) # reshaped to (B*T,C)\n",
    "            targets = targets.view(B*T) # reshaped to (B*T)\n",
    "            # compute cross entropy loss (i.e. average negative log likelihood)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    # generates new sequences continuing from a given batch of context tokens\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # batch of contexts, idx has shape (B,T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get predictions\n",
    "            logits, _ = self(idx) # shape: (B,T,C)\n",
    "            # for each context sequence (in the batch), compute the probability of the next token using the logits of the last token in the context sequence\n",
    "            logits = logits[:,-1,:] # shape: (B,C)\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            # sample from the probability distribution to get next token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # shape: (B,1)\n",
    "            # append to the current context\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # shape: (B,T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.5195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Generated sequence:\n",
      " \n",
      "CPryCqOOqL 'xTaAvrDiqnGa$PxT\n",
      "?k3Jy$vjOpus.ca$KsY?u?s$-qwc: !!rzuniPGJ'qvs3VMkxHUWY qeqmefwjYG,LLZ\n",
      "xM\n"
     ]
    }
   ],
   "source": [
    "# create a bigram language model and test it on the example batch\n",
    "model = BigramLanguageModel(vocab_size=vocab_size)\n",
    "# move model to device\n",
    "m = model.to(device)\n",
    "logits, loss = m(xbatch, ybatch)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# generate a single sequences using the model with start token 0\n",
    "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_seq = m.generate(idx, max_new_tokens=100)[0].tolist()\n",
    "# Decode integer tokens into characters\n",
    "generated_seq = decode(generated_seq)\n",
    "print(\"\\nGenerated sequence:\\n\",\"\".join(generated_seq))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated sequence looks like gibberish, because model is untrained. We now train the model using a graident based optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating training and validation losses averaged over lots of batches\n",
    "@torch.no_grad() # disable gradient tracking\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval() # swicth to inference mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split) \n",
    "            _, loss = m(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean() \n",
    "    model.train() # switch back to training mode\n",
    "    return out       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, training loss: 4.615415573120117, validation loss: 4.633676052093506\n",
      "epoch: 300, training loss: 2.7299535274505615, validation loss: 2.7564690113067627\n",
      "epoch: 600, training loss: 2.519435167312622, validation loss: 2.5423777103424072\n",
      "epoch: 900, training loss: 2.488534927368164, validation loss: 2.5016705989837646\n",
      "epoch: 1200, training loss: 2.4763269424438477, validation loss: 2.4994447231292725\n",
      "epoch: 1500, training loss: 2.4710662364959717, validation loss: 2.4984192848205566\n",
      "epoch: 1800, training loss: 2.466684103012085, validation loss: 2.4925339221954346\n",
      "epoch: 2100, training loss: 2.467583656311035, validation loss: 2.4871132373809814\n",
      "epoch: 2400, training loss: 2.4666929244995117, validation loss: 2.4932312965393066\n",
      "epoch: 2700, training loss: 2.459779739379883, validation loss: 2.485511064529419\n",
      "epoch: 3000, training loss: 2.454580783843994, validation loss: 2.482884168624878\n",
      "epoch: 3300, training loss: 2.4595024585723877, validation loss: 2.4840993881225586\n",
      "epoch: 3600, training loss: 2.458650588989258, validation loss: 2.491469144821167\n",
      "epoch: 3900, training loss: 2.460474967956543, validation loss: 2.4808499813079834\n",
      "epoch: 4200, training loss: 2.468102216720581, validation loss: 2.486747980117798\n",
      "epoch: 4500, training loss: 2.457486867904663, validation loss: 2.485264778137207\n",
      "epoch: 4800, training loss: 2.458542823791504, validation loss: 2.4905202388763428\n",
      "epoch: 5100, training loss: 2.4552981853485107, validation loss: 2.4863624572753906\n",
      "epoch: 5400, training loss: 2.4497551918029785, validation loss: 2.488884210586548\n",
      "epoch: 5700, training loss: 2.4539077281951904, validation loss: 2.4895074367523193\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_iters):\n",
    "    # sample a batch of trainin data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    _, loss = m(xb, yb)\n",
    "    # reset parameter gradients\n",
    "    optimizer.zero_grad(set_to_none=True) \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % eval_interval == 0:\n",
    "        losses = estimate_loss(m)\n",
    "        print(f\"epoch: {epoch}, training loss: {losses['train'].item()}, validation loss: {losses['val'].item()}\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try generating some text using the trained bigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sequence:\n",
      " \n",
      "TI owind,\n",
      "\n",
      "Y:\n",
      "A:\n",
      "F awind fllaverat heroshilds CEdd n!\n",
      "ISarext me s merd; w klishe t in\n",
      "Thes\n",
      "TE k s t thaifor than OLo, s th t ng, ffeey\n",
      "AS:\n",
      "Foeers ordld ann d itou makenche thir in. pr.\n",
      "Hot h po cceplcy aue as YCE t! the\n",
      "Th--\n",
      "\n",
      "\n",
      "Thinongamarearr thes ci.\n",
      "fekesmy tot eritond e;\n",
      "Whyoke aknthin; y w,-h; \n"
     ]
    }
   ],
   "source": [
    "# generate a single sequences using the model with start token 0\n",
    "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_seq = m.generate(idx, max_new_tokens=300)[0].tolist()\n",
    "# Decode integer tokens into characters\n",
    "generated_seq = decode(generated_seq)\n",
    "print(\"\\nGenerated sequence:\\n\",\"\".join(generated_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better! It has similar syntactic structure as the training text and even has some correct words. The quality is still very bad because the context window is too small, only the previous character is used to predict the next character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-attention basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider a batch of 4 sequences, with 8 token embeddings in each sequence, with embedding dims=2\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for each sequence, we will create context windows randing from size 1 up to T. The context of size t is then computed as the average of the embeddings of all tokens up to position t. This simple averaging gives us a \"bag of words\" context which has no awareness of relative positions of the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # embedding vectors for all tokens in the context window of size t --> shape: (t,C)  \n",
    "        # compute bag of words context of size t for the bth sequence\n",
    "        xbow[b,t] = xprev.mean(dim=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more efficient way of computing these context vectors is using matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.]])\n",
      "\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 2.],\n",
      "        [2., 2.]])\n",
      "\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "tensor([[1.0000, 1.0000],\n",
      "        [0.5000, 1.0000],\n",
      "        [0.6667, 0.6667]])\n"
     ]
    }
   ],
   "source": [
    "# consider a single sequence of 3 tokens with embedding dims of 3 --> shape: (3,3)\n",
    "x = torch.tensor([[1,1], [0,1], [1,0]], dtype=torch.float32) # each row is an embedding vector\n",
    "print(x)\n",
    "print(\"\")\n",
    "# then to get the context of shape: (3,2), in which the the t-th row is the sum of the first t rows in x\n",
    "# we can simply multiply a lower triangular matrix in which all elements above the diagonal are zero and the rest are 1s\n",
    "W = torch.tril(torch.ones(3,3))\n",
    "print(W)   \n",
    "xbow = W @ x\n",
    "print(xbow)\n",
    "print(\"\")\n",
    "\n",
    "# however, we don't want the sum of embedding vectors, but the mean so instead do the following\n",
    "W = torch.tril(torch.ones(3,3))\n",
    "W = W / W.sum(dim=1, keepdims=True)\n",
    "print(W)\n",
    "xbow = W @ x\n",
    "print(xbow)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So each row in the weights matrix W gives us the weights for summing up the embedding vectors of all the token. For the ith row, all weights after the ith column are zero,\n",
    "which means the weighted sum for the ith context only includes tokens up to and including the ith position (in our example, we used uniform weights). So effectively, we're masking out all \"future\" tokens so that the context only depends on current and past tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then for a batch of sequences, we can do the following\n",
    "x = torch.randn(B,T,C)\n",
    "W = torch.tril(torch.ones(T,T))\n",
    "W = W / W.sum(dim=1, keepdims=True)\n",
    "xbow = W @ x # batch matrix multiplication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "# another way to generate the lower triangular matrix needed to compute the mean context vectors is as follows\n",
    "A = torch.tril(torch.ones(T,T))\n",
    "W = torch.zeros((T,T))\n",
    "W = W.masked_fill(A == 0, float('-inf')) # masked fill replaces every element in A which equals 0 with -infinity \n",
    "print(W)\n",
    "# then by taking the softmax, we get the desired matrix \n",
    "W = F.softmax(W, dim=-1)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In a self-attention head, these attention weights are not uniform, but are instead computed using the (key, query) vectors of each token in the sequence. Then the output of the attention-head is the weighted sum of value vectors of the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "un-normalized attention weights for first sequence in batch:\n",
      "\n",
      "tensor([[ 0.0545, -0.0593, -0.4230, -0.2394, -0.2138, -0.0163,  0.3961, -0.2642],\n",
      "        [-0.0224,  0.0344,  0.2661,  0.1355,  0.1128,  0.0098, -0.2368,  0.1565],\n",
      "        [-0.0832,  0.1832,  1.4977,  0.7088,  0.5580,  0.0534, -1.2894,  0.8469],\n",
      "        [-0.1025,  0.1488,  1.1382,  0.5882,  0.4951,  0.0421, -1.0202,  0.6751],\n",
      "        [-0.1214,  0.1572,  1.1731,  0.6261,  0.5387,  0.0440, -1.0676,  0.7085],\n",
      "        [-0.0049,  0.0084,  0.0666,  0.0330,  0.0270,  0.0024, -0.0585,  0.0386],\n",
      "        [ 0.1230, -0.2082, -1.6385, -0.8157, -0.6681, -0.0596,  1.4433, -0.9521],\n",
      "        [-0.0872,  0.1431,  1.1201,  0.5616,  0.4624,  0.0409, -0.9899,  0.6534]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "unnormalized masked attention weights:\n",
      "\n",
      "tensor([[ 0.0545,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.0224,  0.0344,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.0832,  0.1832,  1.4977,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.1025,  0.1488,  1.1382,  0.5882,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.1214,  0.1572,  1.1731,  0.6261,  0.5387,    -inf,    -inf,    -inf],\n",
      "        [-0.0049,  0.0084,  0.0666,  0.0330,  0.0270,  0.0024,    -inf,    -inf],\n",
      "        [ 0.1230, -0.2082, -1.6385, -0.8157, -0.6681, -0.0596,  1.4433,    -inf],\n",
      "        [-0.0872,  0.1431,  1.1201,  0.5616,  0.4624,  0.0409, -0.9899,  0.6534]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "Normalized masked attention weights:\n",
      "\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4858, 0.5142, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1396, 0.1822, 0.6783, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1292, 0.1661, 0.4468, 0.2578, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0998, 0.1319, 0.3643, 0.2108, 0.1932, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1622, 0.1644, 0.1742, 0.1685, 0.1674, 0.1634, 0.0000, 0.0000],\n",
      "        [0.1368, 0.0982, 0.0235, 0.0535, 0.0620, 0.1139, 0.5121, 0.0000],\n",
      "        [0.0776, 0.0977, 0.2595, 0.1484, 0.1344, 0.0882, 0.0315, 0.1627]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "head_size = 16 # this is the dimensions of the key and query vectors\n",
    "\n",
    "# the key and query vector are ontained by a linear transform of the embeddings obtained by multiplying with (C, head_size) matrices of learnable weights\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# then given a batch of token sequence embeddings\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B,T,C) # (B,T,C)\n",
    "\n",
    "# we can compute the query, key and value vectors for each token as follows\n",
    "k = key(x) # (B,T,h) where h=16 is the head_size\n",
    "q = query(x) # (B,T,h)\n",
    "v = value(x) # (B,T,h)\n",
    "\n",
    "# then for a sequence of tokens, the (i,j)th attention weight is assigned to be the dot product of the query vector of ith token\n",
    "# with key vector of jth token, so for the entuire batch we have the following\n",
    "W = q @ k.transpose(-2,-1)  # we've transposed the key matrix: (B,T,h) --> (B,h,T), the shape of the matrix multiplication result is: (B,T,h) @ (B,h,T) = (B,T,T)\n",
    "\n",
    "# we also scale the unnormalized weights to have variance of roughly 1\n",
    "W = W * head_size**(-0.5)\n",
    "\n",
    "\n",
    "print(\"\\nun-normalized attention weights for first sequence in batch:\\n\")\n",
    "print(W[0]) \n",
    "\n",
    "# then we apply the \"temporal\" masking so that the attention weights of future tokens is zero and also normalize so that weights sum to one\n",
    "A = torch.tril(torch.ones(T,T))\n",
    "W = W.masked_fill(A == 0, float('-inf')) # masked fill replaces every element in A which equals 0 with -infinity \n",
    "print(\"\\nunnormalized masked attention weights:\\n\")\n",
    "print(W[0]) \n",
    "W = F.softmax(W, dim=-1)\n",
    "print(\"\\nNormalized masked attention weights:\\n\")\n",
    "print(W[0])\n",
    "\n",
    "# the output of the self-attention head is then the sums of the token embeddings weighted by the attention weights\n",
    "out = W @ v # (B,T,T) @ (B,T,h) = (B,T,h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now using the idea of self-attention, we will design a better language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we create a single self-attention head module\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_size = head_size\n",
    "\n",
    "        # define parameters\n",
    "        self.key = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "\n",
    "        # non-parameter tensor of lower triangular ones\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "   \n",
    "    # define forward pass, input shape: (B,T,C) where B=batch size, T=block_size, C=embedding_dim\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B,T,h) where h is the head_size\n",
    "        q = self.query(x) # (B,T,h)\n",
    "        v = self.value(x) # (B,T,h)\n",
    "        W = q @ k.transpose(-2,-1)  * self.head_size**(-0.5) # (B,T,T)\n",
    "        W = W.masked_fill(self.tril[:T, :T] == 0, float('-inf')) \n",
    "        W = F.softmax(W, dim=-1)\n",
    "        out = W @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "class ImprovedLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, embedding_dim, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_size = head_size\n",
    "\n",
    "        '''\n",
    "        Define model parameters\n",
    "        '''\n",
    "        # token embedding layer \n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim) # shape: (vocab_size,C)\n",
    "        # position embedding layer\n",
    "        self.pos_embedding = nn.Embedding(block_size, embedding_dim) # shape: (T,C)\n",
    "\n",
    "        # self-attention layer\n",
    "        self.sa_head = Head(block_size, embedding_dim, head_size) # shape: (T,C,h)\n",
    "        # output layer logits\n",
    "        self.lm_head = nn.Linear(head_size, vocab_size) # shape: (h,vocab_size)\n",
    "\n",
    "\n",
    "        # forward pass takes in a batch of input token sequences of shape (B,T) and corresponding targets of shape (B,T)\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T =idx.shape\n",
    "        # get token embeddings\n",
    "        token_embeds = self.token_embedding(idx) # (B,T,C)\n",
    "        # add positional encoding\n",
    "        pos_embeds = self.pos_embedding(torch.arange(T, device=device)) # (T,C) \n",
    "        x = token_embeds + pos_embeds # (B,T,C)\n",
    "        # apply self-attention\n",
    "        x = self.sa_head(x) # (B,T,h)\n",
    "        # compute output logits\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B,T,vocab_size = logits.shape\n",
    "            # reshape the logits and targets such that batch of input sequences are flattened into a single big input sequence\n",
    "            # i.e. (B,T) --> (B*T)\n",
    "            logits = logits.view(B*T,vocab_size) # reshaped to (B*T,vocab_size)\n",
    "            targets = targets.view(B*T) # reshaped to (B*T)\n",
    "            # compute cross entropy loss (i.e. average negative log likelihood)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "\n",
    "    # generates new sequences continuing from a given batch of context tokens\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # batch of contexts, idx has shape (B,T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # since we're using positional encoding, we need to crop idx if input sequence length exceeds block size (keep last block_size tokens)\n",
    "            idx_crop = idx[:,-block_size:] \n",
    "            # get predictions\n",
    "            logits, _ = self(idx_crop) # shape: (B,T,C)\n",
    "            # for each context sequence (in the batch), compute the probability of the next token using the logits of the last token in the context sequence\n",
    "            logits = logits[:,-1,:] # shape: (B,C)\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            # sample from the probability distribution to get next token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # shape: (B,1)\n",
    "            # append to the current context\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # shape: (B,T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train this improved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "block_size = 8\n",
    "embedding_dim = 32\n",
    "head_size = 32\n",
    "max_iters = 5000\n",
    "learning_rate = 1e-3\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "\n",
    "model = ImprovedLanguageModel(vocab_size=vocab_size, block_size=block_size, embedding_dim=embedding_dim, head_size=head_size)\n",
    "# move model to device\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, training loss: 2.394552230834961, validation loss: 2.399357318878174\n",
      "epoch: 500, training loss: 2.379080295562744, validation loss: 2.4083101749420166\n",
      "epoch: 1000, training loss: 2.3789455890655518, validation loss: 2.3955671787261963\n",
      "epoch: 1500, training loss: 2.3779523372650146, validation loss: 2.398219585418701\n",
      "epoch: 2000, training loss: 2.379781723022461, validation loss: 2.384589433670044\n",
      "epoch: 2500, training loss: 2.3580307960510254, validation loss: 2.386016607284546\n",
      "epoch: 3000, training loss: 2.371300458908081, validation loss: 2.394611120223999\n",
      "epoch: 3500, training loss: 2.366600275039673, validation loss: 2.3787920475006104\n",
      "epoch: 4000, training loss: 2.3598947525024414, validation loss: 2.3750083446502686\n",
      "epoch: 4500, training loss: 2.3642451763153076, validation loss: 2.3858792781829834\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(max_iters):\n",
    "    # sample a batch of trainin data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    _, loss = m(xb, yb)\n",
    "    # reset parameter gradients\n",
    "    optimizer.zero_grad(set_to_none=True) \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % eval_interval == 0:\n",
    "        losses = estimate_loss(m)\n",
    "        print(f\"epoch: {epoch}, training loss: {losses['train'].item()}, validation loss: {losses['val'].item()}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sequence:\n",
      " \n",
      "Fe dis spe shan thess sco win not wigrs,\n",
      "Fithangts In pisis; pluson lpleon ighake y ealf y!\n",
      "Whatierathimy fus gatwou le hay sthis,\n",
      "Ato' omod, a it wiss out wheave louce ake I st I syok yow oris terwathy ut dat\n",
      "FR ES:\n",
      "The dares\n",
      "O RID:\n",
      "Thhe!\n",
      "ANIDUSCHe seden schapreche!\n",
      "Yome, ow IO:\n",
      "we ds\n",
      "Wiseifof arse\n"
     ]
    }
   ],
   "source": [
    "# generate a single sequences using the model with start token 0\n",
    "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_seq = m.generate(idx, max_new_tokens=300)[0].tolist()\n",
    "# Decode integer tokens into characters\n",
    "generated_seq = decode(generated_seq)\n",
    "print(\"\\nGenerated sequence:\\n\",\"\".join(generated_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the improved model acheives a slightly lower loss and generate slightly better sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To acheive even better performance, we can use multiple attention heads in parallel and concatenate their outputs. This is called \"multi-head attention\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, head_size, num_heads):\n",
    "        super().__init__()\n",
    "        assert head_size % num_heads == 0, \"head_size needs to be integer multiple of num_heads\"\n",
    "        self.heads = nn.ModuleList([Head(block_size, embedding_dim, head_size//num_heads) for _ in range(num_heads)])\n",
    "\n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improved language model with multi-head self attention\n",
    "class ImprovedLanguageModelMultiHead(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, embedding_dim, head_size, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_size = head_size\n",
    "        self.hum_heads = num_heads\n",
    "\n",
    "        '''\n",
    "        Define model parameters\n",
    "        '''\n",
    "        # token embedding layer \n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim) # shape: (vocab_size,C)\n",
    "        # position embedding layer\n",
    "        self.pos_embedding = nn.Embedding(block_size, embedding_dim) # shape: (T,C)\n",
    "\n",
    "        # self-attention layer\n",
    "        self.sa_heads = MultiHeadAttention(block_size, embedding_dim, head_size, num_heads) # shape: (T,C,h)\n",
    "        # output layer logits\n",
    "        self.lm_head = nn.Linear(head_size, vocab_size) # shape: (h,vocab_size)\n",
    "\n",
    "\n",
    "        # forward pass takes in a batch of input token sequences of shape (B,T) and corresponding targets of shape (B,T)\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T =idx.shape\n",
    "        # get token embeddings\n",
    "        token_embeds = self.token_embedding(idx) # (B,T,C)\n",
    "        # add positional encoding\n",
    "        pos_embeds = self.pos_embedding(torch.arange(T, device=device)) # (T,C) \n",
    "        x = token_embeds + pos_embeds # (B,T,C)\n",
    "        # apply self-attention\n",
    "        x = self.sa_heads(x) # (B,T,h)\n",
    "        # compute output logits\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B,T,vocab_size = logits.shape\n",
    "            # reshape the logits and targets such that batch of input sequences are flattened into a single big input sequence\n",
    "            # i.e. (B,T) --> (B*T)\n",
    "            logits = logits.view(B*T,vocab_size) # reshaped to (B*T,vocab_size)\n",
    "            targets = targets.view(B*T) # reshaped to (B*T)\n",
    "            # compute cross entropy loss (i.e. average negative log likelihood)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "\n",
    "    # generates new sequences continuing from a given batch of context tokens\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # batch of contexts, idx has shape (B,T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # since we're using positional encoding, we need to crop idx if input sequence length exceeds block size (keep last block_size tokens)\n",
    "            idx_crop = idx[:,-block_size:] \n",
    "            # get predictions\n",
    "            logits, _ = self(idx_crop) # shape: (B,T,C)\n",
    "            # for each context sequence (in the batch), compute the probability of the next token using the logits of the last token in the context sequence\n",
    "            logits = logits[:,-1,:] # shape: (B,C)\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            # sample from the probability distribution to get next token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # shape: (B,1)\n",
    "            # append to the current context\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # shape: (B,T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "block_size = 8\n",
    "embedding_dim = 32\n",
    "head_size = 32\n",
    "num_heads = 4\n",
    "max_iters = 5000\n",
    "learning_rate = 1e-3\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "\n",
    "model = ImprovedLanguageModelMultiHead(vocab_size=vocab_size, block_size=block_size, embedding_dim=embedding_dim, head_size=head_size, num_heads=num_heads)\n",
    "# move model to device\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, training loss: 4.176211357116699, validation loss: 4.173470973968506\n",
      "epoch: 500, training loss: 2.6718404293060303, validation loss: 2.6823761463165283\n",
      "epoch: 1000, training loss: 2.520519495010376, validation loss: 2.509432077407837\n",
      "epoch: 1500, training loss: 2.4418094158172607, validation loss: 2.4534318447113037\n",
      "epoch: 2000, training loss: 2.3901007175445557, validation loss: 2.402662992477417\n",
      "epoch: 2500, training loss: 2.3557698726654053, validation loss: 2.3734166622161865\n",
      "epoch: 3000, training loss: 2.3254342079162598, validation loss: 2.3540947437286377\n",
      "epoch: 3500, training loss: 2.3081467151641846, validation loss: 2.324794054031372\n",
      "epoch: 4000, training loss: 2.288052797317505, validation loss: 2.3057618141174316\n",
      "epoch: 4500, training loss: 2.2802255153656006, validation loss: 2.296482801437378\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(max_iters):\n",
    "    # sample a batch of trainin data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    _, loss = m(xb, yb)\n",
    "    # reset parameter gradients\n",
    "    optimizer.zero_grad(set_to_none=True) \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % eval_interval == 0:\n",
    "        losses = estimate_loss(m)\n",
    "        print(f\"epoch: {epoch}, training loss: {losses['train'].item()}, validation loss: {losses['val'].item()}\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sequence:\n",
      " \n",
      "Bure,\n",
      "I gaisck yres thas wo his a omeptilosce;\n",
      "Ne sor sear:\n",
      "Ther my nerch hat thougm; obelalivedcorusowno an we, sey don aesndsof nod, Sot hofnucourand py thawke','tber.\n",
      "\n",
      "QUEREIOn woweabe\n",
      "Wour ind may gan pat am tis fors.\n",
      "\n",
      "KICANUS:\n",
      "RTume wen.\n",
      "YOMjumy athe to dime fan thoul sold noule shives\n",
      "Wo's Go sar thoust hat of kerifarn,\n",
      "ame don tharteat te hor,\n",
      "Mcam, both' thed aven hat detway by; ad, se hais wow I isnd to to dos ne;\n",
      "yourdent hese by wert blo's thend of you to wir, a tou's buds us this cereryou sarcald, ut ET:\n",
      "Sang ande;\n",
      "Nem Paceim\n",
      "And sopor picomee ath cher nownIm'D ETHETH!\n",
      "Tha mernoo nobst sole I inse may\n",
      "Cour\n",
      "Milot a magish esilor,-e loon hit Cy:\n",
      "Sat le, of ing,\n",
      "Eugant bos!\n",
      "\n",
      "Theld me theat dos's yousthearis\n",
      "Tond of,-\n",
      "The as ot noon, id domavend preght,\n",
      "Per, ye-fis men, tit moncit \n"
     ]
    }
   ],
   "source": [
    "# generate a single sequences using the model with start token 0\n",
    "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_seq = m.generate(idx, max_new_tokens=800)[0].tolist()\n",
    "# Decode integer tokens into characters\n",
    "generated_seq = decode(generated_seq)\n",
    "print(\"\\nGenerated sequence:\\n\",\"\".join(generated_seq))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instead of calculating the output logits directly from the attention head output, we can put a feed forward (multi-layer perceptron) layer in between the attention head and output layer. This allows us to pack in extra computations and extract more meaningful representations from the attention output. This brings us to the Transformer (Decoder) Block. Each transformer block consists of a multihead self-attemtion layer followed by a feed-forward layer. Transformer blocks are also designed to be stacked up (similar to stacked CNNs and stacked RNNs) and therefore also incorporate residual conections and layer normalization to ensure that the gradients can backpropagate without any difficulty as the stack of transformer blocks become deeper.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple mlp \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, head_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(head_size, head_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# a transformer block consisting of a multihead attention-layer followed by a feed-forward layer\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, head_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(block_size, embedding_dim, head_size, num_heads)\n",
    "        self.ff = FeedForward(head_size, num_heads)\n",
    "        \n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        x = self.sa(x)\n",
    "        x = self.ff(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language model with multiple transfop4mer blocks\n",
    "class ImprovedLanguageModelTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, embedding_dim, head_size, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_size = head_size\n",
    "        self.hum_heads = num_heads\n",
    "\n",
    "        '''\n",
    "        Define model parameters\n",
    "        '''\n",
    "        # token embedding layer \n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim) # shape: (vocab_size,C)\n",
    "        # position embedding layer\n",
    "        self.pos_embedding = nn.Embedding(block_size, embedding_dim) # shape: (T,C)\n",
    "\n",
    "        # stack of 3 transformer blocks\n",
    "        self.blocks = nn.Sequential(\n",
    "            TransformerBlock(block_size, embedding_dim, head_size, num_heads),\n",
    "            TransformerBlock(block_size, embedding_dim, head_size, num_heads),\n",
    "            TransformerBlock(block_size, embedding_dim, head_size, num_heads),\n",
    "        )\n",
    "\n",
    "        # output layer logits\n",
    "        self.lm_head = nn.Linear(head_size, vocab_size) # shape: (h,vocab_size)\n",
    "\n",
    "\n",
    "        # forward pass takes in a batch of input token sequences of shape (B,T) and corresponding targets of shape (B,T)\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T =idx.shape\n",
    "        # get token embeddings\n",
    "        token_embeds = self.token_embedding(idx) # (B,T,C)\n",
    "        # add positional encoding\n",
    "        pos_embeds = self.pos_embedding(torch.arange(T, device=device)) # (T,C) \n",
    "        x = token_embeds + pos_embeds # (B,T,C)\n",
    "        # pass through transformer blocks\n",
    "        x = self.blocks(x) # (B,T,h)\n",
    "        # compute output logits\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B,T,vocab_size = logits.shape\n",
    "            # reshape the logits and targets such that batch of input sequences are flattened into a single big input sequence\n",
    "            # i.e. (B,T) --> (B*T)\n",
    "            logits = logits.view(B*T,vocab_size) # reshaped to (B*T,vocab_size)\n",
    "            targets = targets.view(B*T) # reshaped to (B*T)\n",
    "            # compute cross entropy loss (i.e. average negative log likelihood)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "\n",
    "    # generates new sequences continuing from a given batch of context tokens\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # batch of contexts, idx has shape (B,T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # since we're using positional encoding, we need to crop idx if input sequence length exceeds block size (keep last block_size tokens)\n",
    "            idx_crop = idx[:,-block_size:] \n",
    "            # get predictions\n",
    "            logits, _ = self(idx_crop) # shape: (B,T,C)\n",
    "            # for each context sequence (in the batch), compute the probability of the next token using the logits of the last token in the context sequence\n",
    "            logits = logits[:,-1,:] # shape: (B,C)\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            # sample from the probability distribution to get next token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # shape: (B,1)\n",
    "            # append to the current context\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # shape: (B,T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "block_size = 8\n",
    "embedding_dim = 32\n",
    "head_size = 32\n",
    "num_heads = 4\n",
    "max_iters = 5000\n",
    "learning_rate = 1e-3\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "\n",
    "model = ImprovedLanguageModelTransformer(vocab_size=vocab_size, block_size=block_size, embedding_dim=embedding_dim, head_size=head_size, num_heads=num_heads)\n",
    "# move model to device\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, training loss: 4.1873459815979, validation loss: 4.184993743896484\n",
      "epoch: 500, training loss: 3.0473477840423584, validation loss: 3.0452191829681396\n",
      "epoch: 1000, training loss: 2.646038770675659, validation loss: 2.638247489929199\n",
      "epoch: 1500, training loss: 2.5332300662994385, validation loss: 2.51843523979187\n",
      "epoch: 2000, training loss: 2.46379017829895, validation loss: 2.4540207386016846\n",
      "epoch: 2500, training loss: 2.4229698181152344, validation loss: 2.4220693111419678\n",
      "epoch: 3000, training loss: 2.373483657836914, validation loss: 2.3759841918945312\n",
      "epoch: 3500, training loss: 2.3541784286499023, validation loss: 2.3471546173095703\n",
      "epoch: 4000, training loss: 2.3195488452911377, validation loss: 2.320119619369507\n",
      "epoch: 4500, training loss: 2.293874740600586, validation loss: 2.301326274871826\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(max_iters):\n",
    "    # sample a batch of trainin data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    _, loss = m(xb, yb)\n",
    "    # reset parameter gradients\n",
    "    optimizer.zero_grad(set_to_none=True) \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % eval_interval == 0:\n",
    "        losses = estimate_loss(m)\n",
    "        print(f\"epoch: {epoch}, training loss: {losses['train'].item()}, validation loss: {losses['val'].item()}\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
