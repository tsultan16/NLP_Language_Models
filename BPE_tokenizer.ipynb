{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization: Byte Pair Encoding\n",
    "\n",
    "Based on Andrej Karpathy youtube tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Tokenization into bytes`: We can use UTF-8 encoding to convert our string of Unicode characters into a sequence of bytes. Then we could define each byte as a separate `token`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original string: cafÃ©, UTF-8 encoding: b'caf\\xc3\\xa9', size of encoding: 5 bytes\n",
      "c -> b'c' --> [99], num bytes: 1\n",
      "a -> b'a' --> [97], num bytes: 1\n",
      "f -> b'f' --> [102], num bytes: 1\n",
      "Ã© -> b'\\xc3\\xa9' --> [195, 169], num bytes: 2\n",
      "\n",
      " UTF-8 encoding of 'cafÃ©' converted to a list of integers: [99, 97, 102, 195, 169]\n"
     ]
    }
   ],
   "source": [
    "# sample string of Unicode characters\n",
    "s = 'cafÃ©'\n",
    "# convert to bytes using UTF-8 encoding\n",
    "b = s.encode('utf8')\n",
    "print(f\"Original string: {s}, UTF-8 encoding: {b}, size of encoding: {len(b)} bytes\")\n",
    "# show each character and it's utf-8 byte representation\n",
    "for c in s:\n",
    "    print(f\"{c} -> {c.encode('utf8')} --> {list(c.encode('utf8'))}, num bytes: {len(c.encode('utf8'))}\")\n",
    "\n",
    "# convert each of the 5 bytes in the utf-8 encoding of the sample string to its corresponding integer value (0-255)\n",
    "byte_values = list(b)\n",
    "print(f\"\\n UTF-8 encoding of '{s}' converted to a list of integers: {byte_values}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that utf-8 encoding is variable length, the encoding for a character can range from 1 to 4 bytes. The first 3 chacracters `c`, `a` and `f` are each represented by a single byte, while the accented character `Ã©` is represented by 2 bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious even 30 years after Unicodeâ€™s inception. \n",
      "length of text: 532 characters \n",
      "UTF-8 encoded bytes (each byte converted to an integer): [239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46] \n",
      "length of encoding: 615 bytes\n"
     ]
    }
   ],
   "source": [
    "# longer sample text (taken from https://www.reedbeta.com/blog/programmers-intro-to-unicode/)\n",
    "text = \"ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious even 30 years after Unicodeâ€™s inception.\"\n",
    "\n",
    "# encode text into utf-8 byte sequence\n",
    "tokens = text.encode('utf-8') # byte stream\n",
    "# convert bytes to integers\n",
    "tokens = list(tokens) # integer tokens\n",
    "\n",
    "print(f\"Original text: {text} \\nlength of text: {len(text)} characters \\nUTF-8 encoded bytes (each byte converted to an integer): {tokens} \\nlength of encoding: {len(tokens)} bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this simple tokenization scheme, since each byte is represented as an integer value in the range 0-255, we effectively have a vocabulary size of 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(range(256)) # 0-255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement the `Byte-Pair Encoding` algorithm to obtain a new vocabulary which is created by iteratively merging the most frequency co-occuring tokens into a single new token. \n",
    "\n",
    "First, let's implement a function for finding the most commonly occuring pair of adjacent tokens and a function for merging the pair and augmenting the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_pair(tokens):\n",
    "    \"\"\"\n",
    "    Given a list of integers, return the most common pair of integers\n",
    "    \"\"\"\n",
    "    pair_count = defaultdict(int)\n",
    "    for pair in zip(tokens, tokens[1:]):\n",
    "        pair_count[pair] = pair_count[pair] + 1\n",
    "    \n",
    "    # get the most common pair\n",
    "    pair = max(pair_count, key=pair_count.get)    \n",
    "\n",
    "    return pair\n",
    "    \n",
    "\n",
    "def merge_pair(tokens, pair, vocab):\n",
    "    \"\"\"\n",
    "    Given a list of integers and a pair of integers, merge the pair into a single integer\n",
    "    \"\"\"\n",
    "\n",
    "    # create a new token that represents the merged pair\n",
    "    new_token = len(vocab)\n",
    "    vocab.append(new_token)\n",
    "    # replace all occurances of the pair in the list of tokens with the new token\n",
    "    updated_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == pair:\n",
    "            updated_tokens.append(new_token)\n",
    "            i += 2\n",
    "        else:\n",
    "            updated_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "    return updated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common pair of integers in the text is: (101, 32), which corresponds to the characters: 'e' and ' '\n",
      "The new token list after merging the most common pair is: [239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 256, 118, 101, 114, 121, 32, 110, 97, 109, 256, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 256, 105, 110, 116, 111, 32, 116, 104, 256, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 256, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 256, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 256, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 256, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 256, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 256, 99, 97, 110, 32, 98, 256, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 256, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 256, 85, 110, 105, 99, 111, 100, 256, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 256, 109, 111, 114, 256, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 256, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 256, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 256, 119, 104, 111, 108, 256, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "Length of new token list: 595\n"
     ]
    }
   ],
   "source": [
    "# find most common pair\n",
    "p = most_common_pair(tokens)\n",
    "print(f\"The most common pair of integers in the text is: {p}, which corresponds to the characters: '{chr(p[0])}' and '{chr(p[1])}'\")\n",
    "\n",
    "# merge the most common pair into a single new token\n",
    "tokens = merge_pair(tokens, p, vocab)\n",
    "print(f\"The new token list after merging the most common pair is: {tokens}\")\n",
    "print(f\"Length of new token list: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine the find most common pair and pair merger into a single operation for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_most_common_pair(tokens, vocab, verbose=False):\n",
    "    \"\"\"\n",
    "    Given a list of integers, find the most common pair and merge it into a single integer\n",
    "    \"\"\"\n",
    "    pair = most_common_pair(tokens)\n",
    "    updated_tokens = merge_pair(tokens, pair, vocab)\n",
    "    if verbose:\n",
    "        print(f\"Merged pair {pair} into new token {vocab[-1]}\")\n",
    "        print(f\"New token list: {updated_tokens}\")\n",
    "        print(f\"Length of new token list: {len(updated_tokens)}\")\n",
    "    return updated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged pair (97, 110) into new token 262\n",
      "New token list: [239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 258, 133, 164, 258, 133, 157, 258, 133, 152, 258, 133, 146, 258, 133, 158, 258, 133, 147, 258, 133, 148, 259, 189, 32, 258, 135, 186, 259, 140, 258, 135, 179, 259, 140, 258, 135, 174, 259, 140, 258, 135, 168, 259, 140, 258, 135, 180, 259, 140, 258, 135, 169, 259, 140, 258, 135, 170, 33, 32, 258, 152, 132, 32, 84, 104, 256, 118, 101, 114, 121, 32, 110, 97, 109, 256, 115, 116, 114, 105, 107, 101, 261, 102, 101, 97, 114, 32, 262, 100, 32, 97, 119, 256, 260, 116, 111, 32, 116, 104, 256, 104, 101, 97, 114, 116, 261, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 261, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 256, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 256, 111, 117, 103, 104, 116, 32, 116, 111, 32, 259, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 259, 157, 32, 260, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 256, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 262, 115, 259, 148, 108, 105, 107, 256, 117, 115, 260, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 256, 115, 116, 114, 260, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 256, 99, 262, 32, 98, 256, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 262, 100, 32, 100, 105, 118, 260, 103, 32, 260, 116, 111, 32, 116, 104, 256, 116, 104, 111, 117, 115, 262, 100, 45, 112, 97, 103, 256, 85, 110, 105, 99, 111, 100, 256, 83, 116, 262, 100, 97, 114, 100, 32, 112, 108, 117, 261, 105, 116, 261, 100, 111, 122, 101, 110, 261, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 262, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 262, 100, 32, 110, 111, 116, 101, 261, 99, 262, 32, 98, 256, 109, 111, 114, 256, 116, 104, 262, 32, 97, 32, 108, 105, 116, 116, 108, 256, 260, 116, 105, 109, 105, 100, 97, 116, 260, 103, 46, 32, 73, 32, 100, 111, 110, 259, 153, 116, 32, 98, 108, 97, 109, 256, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 261, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 260, 100, 260, 103, 32, 116, 104, 256, 119, 104, 111, 108, 256, 116, 104, 260, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 261, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 261, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 259, 153, 261, 260, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "Length of new token list: 535\n"
     ]
    }
   ],
   "source": [
    "tokens = merge_most_common_pair(tokens, vocab, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
